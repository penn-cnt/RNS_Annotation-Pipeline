{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('tools')\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import pytorch_lightning.callbacks as pl_callbacks\n",
    "\n",
    "import data_utility\n",
    "import times\n",
    "import segmentation\n",
    "import preprocess\n",
    "import autoencoder\n",
    "import visualizer\n",
    "import kaggle_data_utility"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data_dir = \"data/competition_data/clips\"\n",
    "cache_dir = 'data/competition_data/clips/cache'\n",
    "submission_dir = 'data/competition_data/clips/submission'\n",
    "\n",
    "kaggle_data_utility.makedirs(submission_dir)\n",
    "\n",
    "cached_data_loader = kaggle_data_utility.CachedDataLoader(cache_dir)\n",
    "\n",
    "ts = kaggle_data_utility.time.get_millis()\n",
    "\n",
    "targets = [\n",
    "    'Dog_1',\n",
    "    'Dog_2',\n",
    "    'Dog_3',\n",
    "    'Dog_4',\n",
    "]\n",
    "\n",
    "# targets = [\n",
    "#     'Patient_1',\n",
    "#     'Patient_2',\n",
    "#     'Patient_3',\n",
    "#     'Patient_4',\n",
    "#     'Patient_5',\n",
    "#     'Patient_6',\n",
    "#     'Patient_7',\n",
    "#     'Patient_8'\n",
    "# ]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "(0s)\n",
      "X (178, 16, 400) y (178,) latencies (178,)\n",
      "Loading data\n",
      "(0s)\n",
      "X (172, 16, 400) y (172,) latencies (172,)\n",
      "Loading data\n",
      "(0s)\n",
      "X (480, 16, 400) y (480,) latencies (480,)\n",
      "Loading data\n",
      "(0s)\n",
      "X (257, 16, 400) y (257,) latencies (257,)\n",
      "Loading data\n",
      "(0s)\n",
      "X (418, 16, 400) y (418,)\n",
      "Loading data\n",
      "(0s)\n",
      "X (1148, 16, 400) y (1148,)\n",
      "Loading data\n",
      "(2s)\n",
      "X (4760, 16, 400) y (4760,)\n",
      "Loading data\n",
      "(1s)\n",
      "X (2790, 16, 400) y (2790,)\n",
      "Loading data\n",
      "(1s)\n",
      "X (3181, 16, 400)\n",
      "Loading data\n",
      "(1s)\n",
      "X (2997, 16, 400)\n",
      "Loading data\n",
      "(2s)\n",
      "X (4450, 16, 400)\n",
      "Loading data\n",
      "(1s)\n",
      "X (3013, 16, 400)\n"
     ]
    }
   ],
   "source": [
    "ictal_data_list = [kaggle_data_utility.parse_input_data(data_dir, targets[i], 'ictal', None) for i in\n",
    "                   range(len(targets))]\n",
    "interictal_data_list = [kaggle_data_utility.parse_input_data(data_dir, targets[i], 'interictal', None) for i in\n",
    "                        range(len(targets))]\n",
    "test_data_list = [kaggle_data_utility.parse_input_data(data_dir, targets[i], 'test', None) for i in range(len(targets))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "ictal_data_X = np.empty((0, 16, 400))\n",
    "interictal_data_X = np.empty((0, 16, 400))\n",
    "test_data_X = np.empty((0, 16, 400))\n",
    "for data in ictal_data_list:\n",
    "    ictal_data_X = np.vstack((ictal_data_X, data['X']))\n",
    "for data in interictal_data_list:\n",
    "    interictal_data_X = np.vstack((interictal_data_X, data['X']))\n",
    "for data in test_data_list:\n",
    "    test_data_X = np.vstack((test_data_X, data['X']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class KaggleDataset(Dataset):\n",
    "    def __init__(self, ictal_data_X, interictal_data_X, test_data_X, labeled=True, transform=True):\n",
    "        self.ictal_data_X = ictal_data_X\n",
    "        self.interictal_data_X = interictal_data_X\n",
    "        self.test_data_X = test_data_X\n",
    "        self.ictal_data_y = np.ones(len(self.ictal_data_X))[:, np.newaxis]\n",
    "        self.interictal_data_y = np.zeros(len(self.interictal_data_X))[:, np.newaxis]\n",
    "\n",
    "        self.transform = transform\n",
    "        self.labeled = labeled\n",
    "\n",
    "        data_full = np.vstack((self.ictal_data_X, self.interictal_data_X))\n",
    "        data_full = np.vstack((data_full, self.test_data_X))\n",
    "\n",
    "        self.mean = np.mean(data_full)\n",
    "        self.sd = np.std(data_full)\n",
    "\n",
    "        if labeled:\n",
    "            self.data = np.vstack((self.ictal_data_X, self.interictal_data_X))\n",
    "            self.label = np.vstack((self.ictal_data_y, self.interictal_data_y))\n",
    "        else:\n",
    "            self.data = np.vstack((self.ictal_data_X, self.interictal_data_X))\n",
    "            self.data = np.vstack((self.data, self.test_data_X))\n",
    "            self.label = np.empty(len(self.data))[:, np.newaxis]\n",
    "\n",
    "        self.length = len(self.data)\n",
    "\n",
    "        self.augmentation = T.Compose([\n",
    "\n",
    "            T.Normalize([self.mean, self.mean, self.mean], [self.sd, self.sd, self.sd]),\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((256, 512), interpolation=T.InterpolationMode.NEAREST),\n",
    "            T.RandomApply([T.ColorJitter()], p=0.5),\n",
    "            T.RandomApply([T.GaussianBlur(kernel_size=(3, 3))], p=0.5),\n",
    "            T.RandomInvert(p=0.2),\n",
    "            T.RandomPosterize(4, p=0.2),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "        self.totensor = T.Compose([\n",
    "            T.Normalize([self.mean, self.mean, self.mean], [self.sd, self.sd, self.sd]),\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((256, 512), interpolation=T.InterpolationMode.NEAREST),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "\n",
    "        if self.transform:\n",
    "            channel_index = np.arange(data.shape[0])\n",
    "            np.random.shuffle(channel_index)\n",
    "            data = data[channel_index]\n",
    "            data = torch.from_numpy(data).clone()\n",
    "            data = data.repeat(3, 1, 1)\n",
    "            data = self.augmentation(data)\n",
    "\n",
    "        else:\n",
    "            data = torch.from_numpy(data).clone()\n",
    "            data = data.repeat(3, 1, 1)\n",
    "            data = self.totensor(data)\n",
    "\n",
    "        return data, torch.from_numpy(label).to(dtype=torch.long), index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "labeled_dataset = KaggleDataset(ictal_data_X, interictal_data_X, test_data_X, labeled=False, transform=True)\n",
    "train_set_size = int(labeled_dataset.length * 0.8)\n",
    "valid_set_size = labeled_dataset.length - train_set_size\n",
    "train_set, test_set = torch.utils.data.random_split(labeled_dataset, [train_set_size, valid_set_size])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "from lightly.data import LightlyDataset, SwaVCollateFunction\n",
    "from lightly.loss import SwaVLoss\n",
    "from lightly.loss.memory_bank import MemoryBankModule\n",
    "from lightly.models.modules import SwaVProjectionHead, SwaVPrototypes\n",
    "\n",
    "\n",
    "class SwaV(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = SwaVProjectionHead(512, 512, 128)\n",
    "        self.prototypes = SwaVPrototypes(128, 512, 5)\n",
    "\n",
    "        self.start_queue_at_epoch = 30\n",
    "        self.queues = nn.ModuleList([MemoryBankModule(size=512) for _ in range(2)])\n",
    "\n",
    "    def forward(self, high_resolution, low_resolution, epoch):\n",
    "        self.prototypes.normalize()\n",
    "\n",
    "        high_resolution_features = [self._subforward(x) for x in high_resolution]\n",
    "        low_resolution_features = [self._subforward(x) for x in low_resolution]\n",
    "\n",
    "        high_resolution_prototypes = [\n",
    "            self.prototypes(x, epoch) for x in high_resolution_features\n",
    "        ]\n",
    "        low_resolution_prototypes = [\n",
    "            self.prototypes(x, epoch) for x in low_resolution_features\n",
    "        ]\n",
    "        queue_prototypes = self._get_queue_prototypes(high_resolution_features, epoch)\n",
    "\n",
    "        return high_resolution_prototypes, low_resolution_prototypes, queue_prototypes\n",
    "\n",
    "    def _subforward(self, input):\n",
    "        features = self.backbone(input).flatten(start_dim=1)\n",
    "        features = self.projection_head(features)\n",
    "        features = nn.functional.normalize(features, dim=1, p=2)\n",
    "        return features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_queue_prototypes(self, high_resolution_features, epoch):\n",
    "        if len(high_resolution_features) != len(self.queues):\n",
    "            raise ValueError(\n",
    "                f\"The number of queues ({len(self.queues)}) should be equal to the number of high \"\n",
    "                f\"resolution inputs ({len(high_resolution_features)}). Set `n_queues` accordingly.\"\n",
    "            )\n",
    "\n",
    "        # Get the queue features\n",
    "        queue_features = []\n",
    "        for i in range(len(self.queues)):\n",
    "            _, features = self.queues[i](high_resolution_features[i], update=True)\n",
    "            # Queue features are in (num_ftrs X queue_length) shape, while the high res\n",
    "            # features are in (batch_size X num_ftrs). Swap the axes for interoperability.\n",
    "            features = torch.permute(features, (1, 0))\n",
    "            queue_features.append(features)\n",
    "\n",
    "        # If loss calculation with queue prototypes starts at a later epoch,\n",
    "        # just queue the features and return None instead of queue prototypes.\n",
    "        if self.start_queue_at_epoch > 0 and epoch < self.start_queue_at_epoch:\n",
    "            return None\n",
    "\n",
    "        # Assign prototypes\n",
    "        queue_prototypes = [self.prototypes(x, epoch) for x in queue_features]\n",
    "        return queue_prototypes\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import torch\n",
    "def sigmoid_focal_loss(\n",
    "    inputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    alpha: float = 0.25,\n",
    "    gamma: float = 2,\n",
    "    reduction: str = \"none\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "\n",
    "    Args:\n",
    "        inputs (Tensor): A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets (Tensor): A float tensor with the same shape as inputs. Stores the binary\n",
    "                classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "        alpha (float): Weighting factor in range (0,1) to balance\n",
    "                positive vs negative examples or -1 for ignore. Default: ``0.25``.\n",
    "        gamma (float): Exponent of the modulating factor (1 - p_t) to\n",
    "                balance easy vs hard examples. Default: ``2``.\n",
    "        reduction (string): ``'none'`` | ``'mean'`` | ``'sum'``\n",
    "                ``'none'``: No reduction will be applied to the output.\n",
    "                ``'mean'``: The output will be averaged.\n",
    "                ``'sum'``: The output will be summed. Default: ``'none'``.\n",
    "    Returns:\n",
    "        Loss tensor with the reduction option applied.\n",
    "    \"\"\"\n",
    "    # Original implementation from https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py\n",
    "\n",
    "    p = torch.sigmoid(inputs)\n",
    "\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    p_t = p * targets + (1 - p) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "\n",
    "    # Check reduction option and return loss accordingly\n",
    "    if reduction == \"none\":\n",
    "        pass\n",
    "    elif reduction == \"mean\":\n",
    "        loss = loss.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        loss = loss.sum()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid Value for arg 'reduction': '{reduction} \\n Supported reduction modes: 'none', 'mean', 'sum'\"\n",
    "        )\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FullySupervised(pl.LightningModule):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 8)\n",
    "        self.fc4 = nn.Linear(8, 2)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(-1,512)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pred = self.fc4(x)\n",
    "        label = F.one_hot(y).squeeze()\n",
    "        loss = sigmoid_focal_loss(pred.float(),label.float(), alpha = 0.5, gamma = 8,reduction = 'mean')\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(-1,512)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pred = self.fc4(x)\n",
    "        label = F.one_hot(y).squeeze()\n",
    "        loss = sigmoid_focal_loss(pred.float(),label.float(), alpha = 0.25, gamma = 7,reduction = 'mean')\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return pred, label\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(-1,512)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pred = self.fc4(x)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        return pred, y\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "resnet = torchvision.models.resnet34()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "model = FullySupervised(backbone)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "checkpoint_callback = pl_callbacks.ModelCheckpoint(monitor='val_loss',filename='fully_supervised-{epoch:02d}-{val_loss:.5f}',dirpath='checkpoints')\n",
    "csv_logger = pl_loggers.CSVLogger(\"logs\", name=\"logger\")\n",
    "\n",
    "trainer = pl.Trainer( logger=csv_logger, max_epochs=80, callbacks=[checkpoint_callback],accelerator='gpu', devices=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:616: UserWarning: Checkpoint directory C:\\Users\\Patrick Xu\\Desktop\\RNS_Annotation_Project\\checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | backbone | Sequential | 21.3 M\n",
      "1 | fc1      | Linear     | 131 K \n",
      "2 | fc2      | Linear     | 16.4 K\n",
      "3 | fc3      | Linear     | 520   \n",
      "4 | fc4      | Linear     | 18    \n",
      "----------------------------------------\n",
      "21.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.4 M    Total params\n",
      "85.732    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa5bc598f732402e987125a6b4c8c009"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a53f8a3f81b6426b9349ef1631fb7f4a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "labeled_dataset = KaggleDataset(ictal_data_X, interictal_data_X, test_data_X, labeled=True, transform=True)\n",
    "train_set_size = int(labeled_dataset.length * 0.8)\n",
    "valid_set_size = labeled_dataset.length - train_set_size\n",
    "train_set, test_set = torch.utils.data.random_split(labeled_dataset, [train_set_size, valid_set_size],generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    info = list(zip(*batch))\n",
    "    data = info[0]\n",
    "    label = info[1]\n",
    "    return torch.stack(data), torch.stack(label)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at checkpoints/fully_supervised-epoch=54-val_loss=0.00064.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at checkpoints/fully_supervised-epoch=54-val_loss=0.00064.ckpt\n",
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 35it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a68e837a9cf45b1aff3bc0d86a28f1b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labeled_dataset = KaggleDataset(ictal_data_X, interictal_data_X, test_data_X, labeled=True, transform=False)\n",
    "train_set_size = int(labeled_dataset.length * 0.8)\n",
    "valid_set_size = labeled_dataset.length - train_set_size\n",
    "train_set, test_set = torch.utils.data.random_split(labeled_dataset, [train_set_size, valid_set_size],generator=torch.Generator().manual_seed(42))\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")\n",
    "predictions = trainer.predict(model,val_dataloader,ckpt_path=\"checkpoints/fully_supervised-epoch=54-val_loss=0.00064.ckpt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "(10203, 16, 400)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.dataset.data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "output_list = []\n",
    "target_list = []\n",
    "m = nn.Softmax(dim=1)\n",
    "for pred, y in predictions:\n",
    "    out = m(pred)\n",
    "    output_list.append(out)\n",
    "    target_list.append(y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "output = torch.vstack(output_list)\n",
    "target = torch.vstack(target_list)\n",
    "output = torch.argmax(output, dim=1)\n",
    "output = output.detach().cpu().numpy()\n",
    "target= target.squeeze().detach().cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98      1900\n",
      "           1       0.66      0.94      0.77       141\n",
      "\n",
      "    accuracy                           0.96      2041\n",
      "   macro avg       0.83      0.95      0.88      2041\n",
      "weighted avg       0.97      0.96      0.96      2041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "clf_report = sklearn.metrics.classification_report(output, target)\n",
    "\n",
    "print(f\"Classification Report : \\n{clf_report}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch, label \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[43mdataloader\u001B[49m):\n\u001B[0;32m      2\u001B[0m         batch \u001B[38;5;241m=\u001B[39m batch\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      3\u001B[0m         label \u001B[38;5;241m=\u001B[39m label\u001B[38;5;241m.\u001B[39mto(device)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "for batch, label in tqdm(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        label = label.to(device)\n",
    "        label = F.one_hot(label).squeeze()\n",
    "        outputs = model(batch)\n",
    "        print(outputs)\n",
    "        print(label)\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import copy\n",
    "# import torch\n",
    "# import torchvision\n",
    "# from torch import nn\n",
    "#\n",
    "# from lightly.data import DINOCollateFunction, LightlyDataset\n",
    "# from lightly.loss import DINOLoss\n",
    "# from lightly.models.modules import DINOProjectionHead\n",
    "# from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "# from lightly.utils.scheduler import cosine_schedule\n",
    "#\n",
    "#\n",
    "# class DINO(torch.nn.Module):\n",
    "#     def __init__(self, backbone, input_dim):\n",
    "#         super().__init__()\n",
    "#         self.student_backbone = backbone\n",
    "#         self.student_head = DINOProjectionHead(\n",
    "#             input_dim, 512, 64, 2048, freeze_last_layer=1\n",
    "#         )\n",
    "#         self.teacher_backbone = copy.deepcopy(backbone)\n",
    "#         self.teacher_head = DINOProjectionHead(input_dim, 512, 64, 2048)\n",
    "#         deactivate_requires_grad(self.teacher_backbone)\n",
    "#         deactivate_requires_grad(self.teacher_head)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         y = self.student_backbone(x).flatten(start_dim=1)\n",
    "#         z = self.student_head(y)\n",
    "#         return z\n",
    "#\n",
    "#     def forward_teacher(self, x):\n",
    "#         y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "#         z = self.teacher_head(y)\n",
    "#         return z\n",
    "#\n",
    "#\n",
    "# resnet = torchvision.models.resnet18()\n",
    "# backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "# input_dim = 512\n",
    "# # instead of a resnet you can also use a vision transformer backbone as in the\n",
    "# # original paper (you might have to reduce the batch size in this case):\n",
    "# # backbone = torch.hub.load('facebookresearch/dino:main', 'dino_vits16', pretrained=False)\n",
    "# # input_dim = backbone.embed_dim\n",
    "#\n",
    "# model = DINO(backbone, input_dim)\n",
    "#\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n",
    "#\n",
    "# # # we ignore object detection annotations by setting target_transform to return 0\n",
    "# # pascal_voc = torchvision.datasets.VOCDetection(\n",
    "# #     \"datasets/pascal_voc\", download=True, target_transform=lambda t: 0\n",
    "# # )\n",
    "# # dataset = LightlyDataset.from_torch_dataset(pascal_voc)\n",
    "# # # or create a dataset from a folder containing images or videos:\n",
    "# # # dataset = LightlyDataset(\"path/to/folder\")\n",
    "#\n",
    "# collate_fn = DINOCollateFunction(solarization_prob = 0, hf_prob = 0,vf_prob = 0,rr_prob=0,cj_prob=0,random_gray_scale=0)\n",
    "#\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     train_set,\n",
    "#     batch_size=64,\n",
    "#     collate_fn=collate_fn,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     num_workers=1,\n",
    "# )\n",
    "#\n",
    "# criterion = DINOLoss(\n",
    "#     output_dim=2048,\n",
    "#     warmup_teacher_temp_epochs=5,\n",
    "# )\n",
    "# # move loss to correct device because it also contains parameters\n",
    "# criterion = criterion.to(device)\n",
    "#\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#\n",
    "# epochs = 10\n",
    "#\n",
    "# print(\"Starting Training\")\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = 0\n",
    "#     momentum_val = cosine_schedule(epoch, epochs, 0.996, 1)\n",
    "#     for views, _, _ in tqdm(dataloader):\n",
    "#         update_momentum(model.student_backbone, model.teacher_backbone, m=momentum_val)\n",
    "#         update_momentum(model.student_head, model.teacher_head, m=momentum_val)\n",
    "#         views = [view.to(device) for view in views]\n",
    "#         global_views = views[:2]\n",
    "#         teacher_out = [model.forward_teacher(view) for view in global_views]\n",
    "#         student_out = [model.forward(view) for view in views]\n",
    "#         loss = criterion(teacher_out, student_out, epoch=epoch)\n",
    "#         total_loss += loss.detach()\n",
    "#         loss.backward()\n",
    "#         # We only cancel gradients of student head.\n",
    "#         model.student_head.cancel_last_layer_gradients(current_epoch=epoch)\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#\n",
    "#     avg_loss = total_loss / len(dataloader)\n",
    "#     print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "augmentation = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((256, 512), interpolation=T.InterpolationMode.NEAREST),\n",
    "    T.RandomApply([T.ColorJitter()], p=0.5),\n",
    "    T.RandomApply([T.GaussianBlur(kernel_size=(3, 3))], p=0.5),\n",
    "    T.RandomInvert(p=0.2),\n",
    "    T.RandomPosterize(4, p=0.2),\n",
    "])\n",
    "\n",
    "data = ictal_data_X[0]\n",
    "\n",
    "channel_index = np.arange(data.shape[0])\n",
    "np.random.shuffle(channel_index)\n",
    "data = data[channel_index]\n",
    "data = torch.from_numpy(data).clone()\n",
    "data = data.repeat(3, 1, 1)\n",
    "data = augmentation(data)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "channel_index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[channel_index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#\n",
    "# print(\"Starting Training\")\n",
    "# for epoch in range(50):\n",
    "#     total_loss = 0\n",
    "#     i = 0\n",
    "#     for batch, label in tqdm(dataloader):\n",
    "#         batch = batch.to(device)\n",
    "#         # print(type(batch))\n",
    "#         label = label.to(device)\n",
    "#         label = F.one_hot(label).squeeze()\n",
    "#         outputs = model(batch)\n",
    "#         loss = sigmoid_focal_loss(outputs.float(),label.float(), alpha = 0.25, gamma = 7,reduction = 'mean')\n",
    "#         total_loss += loss.detach()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#\n",
    "#     avg_loss = total_loss / len(dataloader)\n",
    "#     torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': avg_loss,\n",
    "#             }, 'ckpt/checkpoint'+str(epoch)+'.pth')\n",
    "#\n",
    "#     print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}