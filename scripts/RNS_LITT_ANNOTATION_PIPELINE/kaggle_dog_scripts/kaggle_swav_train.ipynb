{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-09-21T11:23:44.522737Z",
     "end_time": "2023-09-21T11:23:45.237387Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../tools')\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import pytorch_lightning.callbacks as pl_callbacks\n",
    "\n",
    "import data_utility\n",
    "import times\n",
    "import segmentation\n",
    "import preprocess\n",
    "import autoencoder\n",
    "import visualizer\n",
    "import kaggle_data_utility\n",
    "from kaggle_data_utility import KaggleDataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-21T11:23:45.239389Z",
     "end_time": "2023-09-21T11:23:52.497705Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data_dir = \"../../../user_data/competition_data/clips\"\n",
    "log_folder_root = '../../../user_data/logs/'\n",
    "ckpt_folder_root = '../../../user_data/checkpoints/'\n",
    "\n",
    "targets = [\n",
    "    'Dog_1',\n",
    "    'Dog_2',\n",
    "    'Dog_3',\n",
    "    'Dog_4',\n",
    "]\n",
    "\n",
    "# targets = [\n",
    "#     'Patient_1',\n",
    "#     'Patient_2',\n",
    "#     'Patient_3',\n",
    "#     'Patient_4',\n",
    "#     'Patient_5',\n",
    "#     'Patient_6',\n",
    "#     'Patient_7',\n",
    "#     'Patient_8'\n",
    "# ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-21T11:23:52.497705Z",
     "end_time": "2023-09-21T11:23:53.019179Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "determine_generator = torch.Generator()\n",
    "determine_generator.manual_seed(random_seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    # True ensures the algorithm selected by CUFA is deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # torch.set_deterministic(True)\n",
    "    # False ensures CUDA select the same algorithm each time the application is run\n",
    "    torch.backends.cudnn.benchmark = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-21T11:23:53.025184Z",
     "end_time": "2023-09-21T11:23:53.559671Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "(0s)\n",
      "X (178, 16, 400) y (178,) latencies (178,)\n",
      "Loading data\n",
      "(0s)\n",
      "X (172, 16, 400) y (172,) latencies (172,)\n",
      "Loading data\n",
      "(0s)\n",
      "X (480, 16, 400) y (480,) latencies (480,)\n",
      "Loading data\n",
      "(0s)\n",
      "X (257, 16, 400) y (257,) latencies (257,)\n",
      "Loading data\n",
      "(0s)\n",
      "X (418, 16, 400) y (418,)\n",
      "Loading data\n",
      "(0s)\n",
      "X (1148, 16, 400) y (1148,)\n",
      "Loading data\n",
      "(2s)\n",
      "X (4760, 16, 400) y (4760,)\n",
      "Loading data\n",
      "(1s)\n",
      "X (2790, 16, 400) y (2790,)\n",
      "Loading data\n",
      "(1s)\n",
      "X (3181, 16, 400)\n",
      "Loading data\n",
      "(1s)\n",
      "X (2997, 16, 400)\n",
      "Loading data\n",
      "(2s)\n",
      "X (4450, 16, 400)\n",
      "Loading data\n",
      "(1s)\n",
      "X (3013, 16, 400)\n"
     ]
    }
   ],
   "source": [
    "ictal_data_list = [kaggle_data_utility.parse_input_data(data_dir, targets[i], 'ictal', None) for i in\n",
    "                   range(len(targets))]\n",
    "interictal_data_list = [kaggle_data_utility.parse_input_data(data_dir, targets[i], 'interictal', None) for i in\n",
    "                        range(len(targets))]\n",
    "test_data_list = [kaggle_data_utility.parse_input_data(data_dir, targets[i], 'test', None) for i in range(len(targets))]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-21T11:23:53.561673Z",
     "end_time": "2023-09-21T11:24:05.235288Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "ictal_data_X = np.empty((0, 16, 400))\n",
    "interictal_data_X = np.empty((0, 16, 400))\n",
    "test_data_X = np.empty((0, 16, 400))\n",
    "for data in ictal_data_list:\n",
    "    ictal_data_X = np.vstack((ictal_data_X, data['X']))\n",
    "for data in interictal_data_list:\n",
    "    interictal_data_X = np.vstack((interictal_data_X, data['X']))\n",
    "for data in test_data_list:\n",
    "    test_data_X = np.vstack((test_data_X, data['X']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-21T11:24:05.242295Z",
     "end_time": "2023-09-21T11:24:06.252214Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "unlabeled_dataset = KaggleDataset(ictal_data_X, interictal_data_X, test_data_X, labeled=False, transform=True,astensor = False)\n",
    "train_set_size = int(unlabeled_dataset.length * 0.8)\n",
    "valid_set_size = unlabeled_dataset.length - train_set_size\n",
    "train_set, test_set = torch.utils.data.random_split(unlabeled_dataset, [train_set_size, valid_set_size],generator=determine_generator)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-21T11:24:06.253214Z",
     "end_time": "2023-09-21T11:24:08.300078Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "23844"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_dataset.length"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-21T11:24:08.303078Z",
     "end_time": "2023-09-21T11:24:08.850577Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision\n",
    "# from torch import nn\n",
    "#\n",
    "# from lightly.data import LightlyDataset, SwaVCollateFunction\n",
    "# from lightly.loss import SwaVLoss\n",
    "# from lightly.loss.memory_bank import MemoryBankModule\n",
    "# from lightly.models.modules import SwaVProjectionHead, SwaVPrototypes\n",
    "#\n",
    "#\n",
    "# class SwaV(nn.Module):\n",
    "#     def __init__(self, backbone):\n",
    "#         super().__init__()\n",
    "#         self.backbone = backbone\n",
    "#         self.projection_head = SwaVProjectionHead(512, 512, 128)\n",
    "#         self.prototypes = SwaVPrototypes(128, 512, 5)\n",
    "#\n",
    "#         self.start_queue_at_epoch = 30\n",
    "#         self.queues = nn.ModuleList([MemoryBankModule(size=512) for _ in range(2)])\n",
    "#\n",
    "#     def forward(self, high_resolution, low_resolution, epoch):\n",
    "#         self.prototypes.normalize()\n",
    "#\n",
    "#         high_resolution_features = [self._subforward(x) for x in high_resolution]\n",
    "#         low_resolution_features = [self._subforward(x) for x in low_resolution]\n",
    "#\n",
    "#         high_resolution_prototypes = [\n",
    "#             self.prototypes(x, epoch) for x in high_resolution_features\n",
    "#         ]\n",
    "#         low_resolution_prototypes = [\n",
    "#             self.prototypes(x, epoch) for x in low_resolution_features\n",
    "#         ]\n",
    "#         queue_prototypes = self._get_queue_prototypes(high_resolution_features, epoch)\n",
    "#\n",
    "#         return high_resolution_prototypes, low_resolution_prototypes, queue_prototypes\n",
    "#\n",
    "#     def _subforward(self, input):\n",
    "#         features = self.backbone(input).flatten(start_dim=1)\n",
    "#         features = self.projection_head(features)\n",
    "#         features = nn.functional.normalize(features, dim=1, p=2)\n",
    "#         return features\n",
    "#\n",
    "#     @torch.no_grad()\n",
    "#     def _get_queue_prototypes(self, high_resolution_features, epoch):\n",
    "#         if len(high_resolution_features) != len(self.queues):\n",
    "#             raise ValueError(\n",
    "#                 f\"The number of queues ({len(self.queues)}) should be equal to the number of high \"\n",
    "#                 f\"resolution inputs ({len(high_resolution_features)}). Set `n_queues` accordingly.\"\n",
    "#             )\n",
    "#\n",
    "#         # Get the queue features\n",
    "#         queue_features = []\n",
    "#         for i in range(len(self.queues)):\n",
    "#             _, features = self.queues[i](high_resolution_features[i], update=True)\n",
    "#             # Queue features are in (num_ftrs X queue_length) shape, while the high res\n",
    "#             # features are in (batch_size X num_ftrs). Swap the axes for interoperability.\n",
    "#             features = torch.permute(features, (1, 0))\n",
    "#             queue_features.append(features)\n",
    "#\n",
    "#         # If loss calculation with queue prototypes starts at a later epoch,\n",
    "#         # just queue the features and return None instead of queue prototypes.\n",
    "#         if self.start_queue_at_epoch > 0 and epoch < self.start_queue_at_epoch:\n",
    "#             return None\n",
    "#\n",
    "#         # Assign prototypes\n",
    "#         queue_prototypes = [self.prototypes(x, epoch) for x in queue_features]\n",
    "#         return queue_prototypes\n",
    "#\n",
    "#\n",
    "# resnet = torchvision.models.resnet34()\n",
    "#\n",
    "# backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "# model = SwaV(backbone)\n",
    "#\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n",
    "\n",
    "# collate_fn = SwaVCollateFunction(gaussian_blur = 0, hf_prob = 0,vf_prob = 0,rr_prob=0,cj_prob=0,random_gray_scale=0, normalize={'mean':[0, 0, 0], 'std':[1, 1, 1]})\n",
    "#\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     unlabeled_dataset,\n",
    "#     batch_size=128,\n",
    "#     collate_fn=collate_fn,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "# )\n",
    "#\n",
    "# criterion = SwaVLoss(sinkhorn_epsilon = 0.05)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#\n",
    "# # torch.save({\n",
    "# #             'epoch': -1,\n",
    "# #             'model_state_dict': model.state_dict(),\n",
    "# #             'optimizer_state_dict': optimizer.state_dict(),\n",
    "# #             'loss': -1,\n",
    "# #             }, 'ckpt/checkpoint_round_2'+str(-1)+'.pth')\n",
    "#\n",
    "# print(\"Starting Training\")\n",
    "# for epoch in range(100):\n",
    "#     total_loss = 0\n",
    "#     i = 0\n",
    "#     for batch, _, _ in tqdm(dataloader):\n",
    "#         batch = [x.to(device) for x in batch]\n",
    "#         high_resolution, low_resolution = batch[:2], batch[2:]\n",
    "#         high_resolution, low_resolution, queue = model(\n",
    "#             high_resolution, low_resolution, epoch\n",
    "#         )\n",
    "#         loss = criterion(high_resolution, low_resolution, queue)\n",
    "#         total_loss += loss.detach()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#\n",
    "#     avg_loss = total_loss / len(dataloader)\n",
    "#     torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': avg_loss,\n",
    "#             }, 'kaggle_human_ckpt/checkpoint'+str(epoch)+'.pth')\n",
    "#\n",
    "#     print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-21T11:24:08.853580Z",
     "end_time": "2023-09-21T11:24:09.375511Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from models.SwaV import SwaV\n",
    "from lightly.data import SwaVCollateFunction\n",
    "\n",
    "model = SwaV()\n",
    "\n",
    "# model.load_from_checkpoint()\n",
    "\n",
    "collate_fn = SwaVCollateFunction(gaussian_blur = 0, hf_prob = 0,vf_prob = 0,rr_prob=0,cj_prob=0,random_gray_scale=0, normalize={'mean':[0, 0, 0], 'std':[1, 1, 1]})\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    unlabeled_dataset,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "checkpoint_callback = pl_callbacks.ModelCheckpoint(monitor='swav_loss',filename='kaggle_dog_swav-{epoch:02d}-{swav_loss:.5f}', save_last=True, save_top_k=-1, dirpath=ckpt_folder_root + 'kaggle_dog_swav')\n",
    "csv_logger = pl_loggers.CSVLogger(log_folder_root, name=\"kaggle_dog_swav\")\n",
    "\n",
    "trainer = pl.Trainer(logger=csv_logger, max_epochs=120, callbacks=[checkpoint_callback],accelerator='gpu', devices=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-21T11:24:09.376512Z",
     "end_time": "2023-09-21T11:24:10.352595Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:616: UserWarning: Checkpoint directory C:\\Users\\Patrick Xu\\Desktop\\RNS_Annotation-Pipeline\\user_data\\checkpoints\\kaggle_dog_swav exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type               | Params\n",
      "-------------------------------------------------------\n",
      "0 | backbone        | Sequential         | 21.3 M\n",
      "1 | projection_head | SwaVProjectionHead | 328 K \n",
      "2 | prototypes      | SwaVPrototypes     | 66.0 K\n",
      "3 | queues          | ModuleList         | 0     \n",
      "4 | criterion       | SwaVLoss           | 0     \n",
      "-------------------------------------------------------\n",
      "21.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.7 M    Total params\n",
      "86.718    Total estimated model params size (MB)\n",
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca10c70e23be49f2bcf3808baa52c54d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model=model, train_dataloaders=dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-21T11:20:24.467456Z",
     "end_time": "2023-09-21T11:23:05.235442Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def sigmoid_focal_loss(\n",
    "    inputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    alpha: float = 0.25,\n",
    "    gamma: float = 2,\n",
    "    reduction: str = \"none\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "\n",
    "    Args:\n",
    "        inputs (Tensor): A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets (Tensor): A float tensor with the same shape as inputs. Stores the binary\n",
    "                classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "        alpha (float): Weighting factor in range (0,1) to balance\n",
    "                positive vs negative examples or -1 for ignore. Default: ``0.25``.\n",
    "        gamma (float): Exponent of the modulating factor (1 - p_t) to\n",
    "                balance easy vs hard examples. Default: ``2``.\n",
    "        reduction (string): ``'none'`` | ``'mean'`` | ``'sum'``\n",
    "                ``'none'``: No reduction will be applied to the output.\n",
    "                ``'mean'``: The output will be averaged.\n",
    "                ``'sum'``: The output will be summed. Default: ``'none'``.\n",
    "    Returns:\n",
    "        Loss tensor with the reduction option applied.\n",
    "    \"\"\"\n",
    "    # Original implementation from https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py\n",
    "\n",
    "    p = torch.sigmoid(inputs)\n",
    "\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    p_t = p * targets + (1 - p) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "\n",
    "    # Check reduction option and return loss accordingly\n",
    "    if reduction == \"none\":\n",
    "        pass\n",
    "    elif reduction == \"mean\":\n",
    "        loss = loss.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        loss = loss.sum()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid Value for arg 'reduction': '{reduction} \\n Supported reduction modes: 'none', 'mean', 'sum'\"\n",
    "        )\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-20T13:34:09.395291Z",
     "end_time": "2023-09-20T13:34:09.970814Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# unlabeled_dataset = KaggleDataset(ictal_data_X, interictal_data_X, test_data_X, labeled=False, transform=True)\n",
    "# train_set_size = int(labeled_dataset.length * 0.8)\n",
    "# valid_set_size = labeled_dataset.length - train_set_size\n",
    "# train_set, test_set = torch.utils.data.random_split(labeled_dataset, [train_set_size, valid_set_size])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-20T13:34:09.970814Z",
     "end_time": "2023-09-20T13:34:10.564861Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class SupervisedDownstream(pl.LightningModule):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 8)\n",
    "        self.fc4 = nn.Linear(8, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.alpha = 0.5\n",
    "        self.gamma = 8\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # self.backbone.eval()\n",
    "        x = self.backbone(x)\n",
    "        # with torch.no_grad():\n",
    "        x = x.view(-1,512)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pred = self.fc4(x)\n",
    "        pred = self.softmax(pred)\n",
    "        label = F.one_hot(y).squeeze()\n",
    "        loss = sigmoid_focal_loss(pred.float(),label.float(), alpha = self.alpha, gamma = self.gamma,reduction = 'mean')\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(-1,512)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pred = self.fc4(x)\n",
    "        pred = self.softmax(pred)\n",
    "        label = F.one_hot(y).squeeze()\n",
    "        loss = sigmoid_focal_loss(pred.float(),label.float(), alpha = self.alpha, gamma = self.gamma,reduction = 'mean')\n",
    "        out = torch.argmax(pred, dim=1)\n",
    "        out = out.detach().cpu().numpy()\n",
    "        target = y.squeeze().detach().cpu().numpy()\n",
    "        precision,recall,fscore,support = sklearn.metrics.precision_recall_fscore_support(out, target)\n",
    "        acc=sklearn.metrics.accuracy_score(out, target)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        self.log(\"val_precision\", precision[1])\n",
    "        self.log(\"val_recall\", recall[1])\n",
    "        return pred, label\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        emb = self.backbone(x)\n",
    "        emb = emb.view(-1,512)\n",
    "        x = F.relu(self.fc1(emb))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pred = self.fc4(x)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        return pred, y, emb\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-20T13:34:10.566862Z",
     "end_time": "2023-09-20T13:34:11.150982Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ckpt/checkpoint99.pth'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [15]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m ckpt \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mckpt/checkpoint99.pth\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m resnet \u001B[38;5;241m=\u001B[39m torchvision\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mresnet34()\n\u001B[0;32m      3\u001B[0m backbone \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mlist\u001B[39m(resnet\u001B[38;5;241m.\u001B[39mchildren())[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:699\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[0;32m    696\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    697\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m--> 699\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m    700\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m    701\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m    702\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m    703\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m    704\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:230\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    229\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 230\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    232\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:211\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 211\u001B[0m     \u001B[38;5;28msuper\u001B[39m(_open_file, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'ckpt/checkpoint99.pth'"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(\"ckpt/checkpoint99.pth\")\n",
    "resnet = torchvision.models.resnet34()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "swav = SwaV(backbone)\n",
    "swav.load_state_dict(ckpt['model_state_dict'])\n",
    "model = SupervisedDownstream(swav.backbone)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "checkpoint_callback = pl_callbacks.ModelCheckpoint(monitor='val_loss',filename='swav_pretrained_unfreeze-{epoch:02d}-{val_loss:.5f}',dirpath='checkpoints')\n",
    "csv_logger = pl_loggers.CSVLogger(\"logs\", name=\"logger\")\n",
    "\n",
    "trainer = pl.Trainer( logger=csv_logger, max_epochs=80, callbacks=[checkpoint_callback],accelerator='gpu', devices=1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "labeled_dataset = KaggleDataset(ictal_data_X, interictal_data_X, test_data_X, labeled=True, transform=True)\n",
    "train_set_size = int(labeled_dataset.length * 0.8)\n",
    "valid_set_size = labeled_dataset.length - train_set_size\n",
    "train_set, test_set = torch.utils.data.random_split(labeled_dataset, [train_set_size, valid_set_size],generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    info = list(zip(*batch))\n",
    "    data = info[0]\n",
    "    label = info[1]\n",
    "    return torch.stack(data), torch.stack(label)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labeled_dataset = KaggleDataset(ictal_data_X, interictal_data_X, test_data_X, labeled=True, transform=False)\n",
    "train_set_size = int(labeled_dataset.length * 0.8)\n",
    "valid_set_size = labeled_dataset.length - train_set_size\n",
    "train_set, test_set = torch.utils.data.random_split(labeled_dataset, [train_set_size, valid_set_size],generator=torch.Generator().manual_seed(42))\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")\n",
    "predictions = trainer.predict(model,val_dataloader,ckpt_path='checkpoints/swav_pretrained-epoch=26-val_loss=0.00029_alpha_0.9_gamma_8_sftm.ckpt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_list = []\n",
    "target_list = []\n",
    "m = nn.Softmax(dim=1)\n",
    "for pred, y,_ in predictions:\n",
    "    out = m(pred)\n",
    "    print(out)\n",
    "    output_list.append(out)\n",
    "    target_list.append(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output = torch.vstack(output_list)\n",
    "target = torch.vstack(target_list)\n",
    "inds = np.where(target==1)[0]\n",
    "\n",
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(target,output[:,1],pos_label=1)\n",
    "sklearn.metrics.auc(fpr, tpr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output[:,1][inds]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "RocCurveDisplay.from_predictions(\n",
    "    target,\n",
    "    output[:,1],\n",
    "    color=\"darkorange\",\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"One-vs-Rest ROC curves:\\nVirginica vs (Setosa & Versicolor)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output[:,1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "output = torch.argmax(output, dim=1)\n",
    "output = output.detach().cpu().numpy()\n",
    "target= target.squeeze().detach().cpu().numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "clf_report = sklearn.metrics.classification_report(output, target, digits=6)\n",
    "\n",
    "print(f\"Classification Report : \\n{clf_report}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for batch, label in tqdm(val_dataloader):\n",
    "        batch = batch.to(device)\n",
    "        label = label.to(device)\n",
    "        label = F.one_hot(label).squeeze()\n",
    "        outputs = model(batch)\n",
    "        print(batch)\n",
    "        loss = sigmoid_focal_loss(pred.float(),label.float(), alpha = 0.5, gamma = 8,reduction = 'mean')\n",
    "        print(loss)\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import copy\n",
    "# import torch\n",
    "# import torchvision\n",
    "# from torch import nn\n",
    "#\n",
    "# from lightly.data import DINOCollateFunction, LightlyDataset\n",
    "# from lightly.loss import DINOLoss\n",
    "# from lightly.models.modules import DINOProjectionHead\n",
    "# from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "# from lightly.utils.scheduler import cosine_schedule\n",
    "#\n",
    "#\n",
    "# class DINO(torch.nn.Module):\n",
    "#     def __init__(self, backbone, input_dim):\n",
    "#         super().__init__()\n",
    "#         self.student_backbone = backbone\n",
    "#         self.student_head = DINOProjectionHead(\n",
    "#             input_dim, 512, 64, 2048, freeze_last_layer=1\n",
    "#         )\n",
    "#         self.teacher_backbone = copy.deepcopy(backbone)\n",
    "#         self.teacher_head = DINOProjectionHead(input_dim, 512, 64, 2048)\n",
    "#         deactivate_requires_grad(self.teacher_backbone)\n",
    "#         deactivate_requires_grad(self.teacher_head)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         y = self.student_backbone(x).flatten(start_dim=1)\n",
    "#         z = self.student_head(y)\n",
    "#         return z\n",
    "#\n",
    "#     def forward_teacher(self, x):\n",
    "#         y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "#         z = self.teacher_head(y)\n",
    "#         return z\n",
    "#\n",
    "#\n",
    "# resnet = torchvision.models.resnet18()\n",
    "# backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "# input_dim = 512\n",
    "# # instead of a resnet you can also use a vision transformer backbone as in the\n",
    "# # original paper (you might have to reduce the batch size in this case):\n",
    "# # backbone = torch.hub.load('facebookresearch/dino:main', 'dino_vits16', pretrained=False)\n",
    "# # input_dim = backbone.embed_dim\n",
    "#\n",
    "# model = DINO(backbone, input_dim)\n",
    "#\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n",
    "#\n",
    "# # # we ignore object detection annotations by setting target_transform to return 0\n",
    "# # pascal_voc = torchvision.datasets.VOCDetection(\n",
    "# #     \"datasets/pascal_voc\", download=True, target_transform=lambda t: 0\n",
    "# # )\n",
    "# # dataset = LightlyDataset.from_torch_dataset(pascal_voc)\n",
    "# # # or create a dataset from a folder containing images or videos:\n",
    "# # # dataset = LightlyDataset(\"path/to/folder\")\n",
    "#\n",
    "# collate_fn = DINOCollateFunction(solarization_prob = 0, hf_prob = 0,vf_prob = 0,rr_prob=0,cj_prob=0,random_gray_scale=0)\n",
    "#\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     train_set,\n",
    "#     batch_size=64,\n",
    "#     collate_fn=collate_fn,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     num_workers=1,\n",
    "# )\n",
    "#\n",
    "# criterion = DINOLoss(\n",
    "#     output_dim=2048,\n",
    "#     warmup_teacher_temp_epochs=5,\n",
    "# )\n",
    "# # move loss to correct device because it also contains parameters\n",
    "# criterion = criterion.to(device)\n",
    "#\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#\n",
    "# epochs = 10\n",
    "#\n",
    "# print(\"Starting Training\")\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = 0\n",
    "#     momentum_val = cosine_schedule(epoch, epochs, 0.996, 1)\n",
    "#     for views, _, _ in tqdm(dataloader):\n",
    "#         update_momentum(model.student_backbone, model.teacher_backbone, m=momentum_val)\n",
    "#         update_momentum(model.student_head, model.teacher_head, m=momentum_val)\n",
    "#         views = [view.to(device) for view in views]\n",
    "#         global_views = views[:2]\n",
    "#         teacher_out = [model.forward_teacher(view) for view in global_views]\n",
    "#         student_out = [model.forward(view) for view in views]\n",
    "#         loss = criterion(teacher_out, student_out, epoch=epoch)\n",
    "#         total_loss += loss.detach()\n",
    "#         loss.backward()\n",
    "#         # We only cancel gradients of student head.\n",
    "#         model.student_head.cancel_last_layer_gradients(current_epoch=epoch)\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#\n",
    "#     avg_loss = total_loss / len(dataloader)\n",
    "#     print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "augmentation = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((256, 512), interpolation=T.InterpolationMode.NEAREST),\n",
    "    T.RandomApply([T.ColorJitter()], p=0.5),\n",
    "    T.RandomApply([T.GaussianBlur(kernel_size=(3, 3))], p=0.5),\n",
    "    T.RandomInvert(p=0.2),\n",
    "    T.RandomPosterize(4, p=0.2),\n",
    "])\n",
    "\n",
    "data = ictal_data_X[0]\n",
    "\n",
    "channel_index = np.arange(data.shape[0])\n",
    "np.random.shuffle(channel_index)\n",
    "data = data[channel_index]\n",
    "data = torch.from_numpy(data).clone()\n",
    "data = data.repeat(3, 1, 1)\n",
    "data = augmentation(data)\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "channel_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[channel_index]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#\n",
    "# print(\"Starting Training\")\n",
    "# for epoch in range(50):\n",
    "#     total_loss = 0\n",
    "#     i = 0\n",
    "#     for batch, label in tqdm(dataloader):\n",
    "#         batch = batch.to(device)\n",
    "#         # print(type(batch))\n",
    "#         label = label.to(device)\n",
    "#         label = F.one_hot(label).squeeze()\n",
    "#         outputs = model(batch)\n",
    "#         loss = sigmoid_focal_loss(outputs.float(),label.float(), alpha = 0.25, gamma = 7,reduction = 'mean')\n",
    "#         total_loss += loss.detach()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#\n",
    "#     avg_loss = total_loss / len(dataloader)\n",
    "#     torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': avg_loss,\n",
    "#             }, 'ckpt/checkpoint'+str(epoch)+'.pth')\n",
    "#\n",
    "#     print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
