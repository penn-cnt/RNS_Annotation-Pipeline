{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T08:35:29.226901Z",
     "end_time": "2023-06-22T08:35:30.324033Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T08:35:30.325033Z",
     "end_time": "2023-06-22T08:35:47.979152Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('tools')\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import pytorch_lightning.callbacks as pl_callbacks\n",
    "\n",
    "import data_utility\n",
    "import times\n",
    "import segmentation\n",
    "import preprocess\n",
    "import autoencoder\n",
    "import visualizer\n",
    "import kaggle_data_utility\n",
    "import annotation_utility\n",
    "import interactive_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T08:35:47.971146Z",
     "end_time": "2023-06-22T08:35:48.693012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['HUP047',\n 'HUP084',\n 'HUP096',\n 'HUP109',\n 'HUP121',\n 'HUP129',\n 'HUP131',\n 'HUP137',\n 'HUP147',\n 'HUP153',\n 'HUP156',\n 'HUP159',\n 'HUP182',\n 'HUP197',\n 'HUP199',\n 'HUP205',\n 'RNS026',\n 'RNS029']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_annotations = pd.read_csv('full_updated_anns_annotTbl_cleaned.csv')\n",
    "ids = list(np.unique(raw_annotations[raw_annotations['descriptions'].notnull()]['HUP_ID']))\n",
    "# ids = list(np.unique(raw_annotations['HUP_ID']))\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T08:35:48.690009Z",
     "end_time": "2023-06-22T08:36:17.247876Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:27<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "data_import = data_utility.read_files(path='data/rns_data', path_data='rns_raw_cache', patientIDs=ids,\n",
    "                                      verbose=True)  # Import data with annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T08:36:17.245875Z",
     "end_time": "2023-06-22T08:36:20.879194Z"
    }
   },
   "outputs": [],
   "source": [
    "annotations = annotation_utility.read_annotation(annotation_path='full_updated_anns_annotTbl_cleaned.csv',\n",
    "                                                 data=data_import, n_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T08:36:20.878194Z",
     "end_time": "2023-06-22T08:36:21.512528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 73)\n",
      "(2, 99)\n",
      "(2, 60)\n",
      "(2, 59)\n",
      "(2, 24)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed=42)\n",
    "annot = annotations.annotations\n",
    "annot_nonseizure = annot[annot['Class_Code'] == 0]\n",
    "annot_seizure = annot[annot['Class_Code'] == 1]\n",
    "# patient_list = list(np.unique(annot['Patient_ID']))\n",
    "patient_list = ['RNS026', 'HUP159', 'HUP129', 'HUP096', 'HUP182']\n",
    "clip_dict = {}\n",
    "for p in patient_list:\n",
    "    seizure_start_index = np.array([])\n",
    "    seizure_end_index = np.array([])\n",
    "    nonseizure_start_index = np.array([])\n",
    "    nonseizure_end_index = np.array([])\n",
    "    start_index = annot_seizure[annot_seizure['Patient_ID'] == p]['Episode_Start_Index']\n",
    "    end_index = annot_seizure[annot_seizure['Patient_ID'] == p]['Episode_End_Index']\n",
    "    annot_start_list = annot_seizure[annot_seizure['Patient_ID'] == p]['Annotation_Start_Index']\n",
    "    annot_end_list = annot_seizure[annot_seizure['Patient_ID'] == p]['Annotation_End_Index']\n",
    "    for i, slel in enumerate(zip(annot_start_list, annot_end_list)):\n",
    "        sl = slel[0]\n",
    "        el = slel[1]\n",
    "        annot_array = np.vstack((sl, el))\n",
    "        test = start_index.iloc[i]\n",
    "        seizure_start_index = np.hstack((seizure_start_index, annot_array[0, :]))\n",
    "        seizure_end_index = np.hstack((seizure_end_index, annot_array[1, :]))\n",
    "\n",
    "        nonseizure_start_index = np.hstack((nonseizure_start_index, start_index.iloc[i]))\n",
    "        nonseizure_end_index = np.hstack((nonseizure_end_index, annot_array[0, 0]))\n",
    "\n",
    "        nonseizure_start_index = np.hstack((nonseizure_start_index, annot_array[1, -1]))\n",
    "        nonseizure_end_index = np.hstack((nonseizure_end_index, end_index.iloc[i]))\n",
    "        if annot_array.shape[1] > 1:\n",
    "            test1 = annot_array[0, 1:]\n",
    "            test2 = annot_array[1, :-1]\n",
    "            nonseizure_start_index = np.hstack((nonseizure_start_index, annot_array[0, 1:]))\n",
    "            nonseizure_end_index = np.hstack((nonseizure_end_index, annot_array[1, :-1]))\n",
    "\n",
    "    nonseizure_valid = np.where(nonseizure_end_index - nonseizure_start_index > 500)\n",
    "    seizure_valid = np.where(seizure_end_index - seizure_start_index > 500)\n",
    "\n",
    "    nonseizure_ind_arr = np.vstack(\n",
    "        (nonseizure_start_index[nonseizure_valid], nonseizure_end_index[nonseizure_valid])).astype(int)\n",
    "    start_index = annot_nonseizure[annot_nonseizure['Patient_ID'] == p]['Episode_Start_Index']\n",
    "    end_index = annot_nonseizure[annot_nonseizure['Patient_ID'] == p]['Episode_End_Index']\n",
    "\n",
    "    print(np.vstack((seizure_start_index[seizure_valid], seizure_end_index[seizure_valid])).astype(int).shape)\n",
    "    valid = np.where(end_index - start_index > 500)\n",
    "    nonseizure_ind_arr_eps = np.vstack((start_index.iloc[valid], end_index.iloc[valid])).astype(int)\n",
    "\n",
    "    if len(valid[0]) and len(seizure_valid[0]) > 0:\n",
    "        nonseizure_clip_temp = np.hstack((nonseizure_ind_arr, nonseizure_ind_arr_eps))\n",
    "        seizure_clip_temp = np.vstack((seizure_start_index[seizure_valid], seizure_end_index[seizure_valid])).astype(\n",
    "            int)\n",
    "\n",
    "        nonseizure_clip_label = np.zeros(nonseizure_clip_temp.shape[1]).astype(int)\n",
    "        seizure_clip_label = np.ones(seizure_clip_temp.shape[1]).astype(int)\n",
    "\n",
    "        seizure_clip = np.vstack((seizure_clip_temp, seizure_clip_label))\n",
    "        non_seizure_clip = np.vstack((nonseizure_clip_temp, nonseizure_clip_label))\n",
    "\n",
    "        combined_clip = np.hstack((seizure_clip, non_seizure_clip))\n",
    "\n",
    "        shuffled_index = np.arange(combined_clip.shape[1])\n",
    "        np.random.shuffle(shuffled_index)\n",
    "\n",
    "        clip_dict[p] = combined_clip[:, shuffled_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T08:36:21.508525Z",
     "end_time": "2023-06-22T08:36:27.669747Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "window_len = 1\n",
    "stride = 1\n",
    "concat_n = 4\n",
    "for id in tqdm(clip_dict.keys()):\n",
    "    data_import[id].set_window_parameter(window_length=window_len, window_displacement=stride)\n",
    "    data_import[id].set_concatenation_parameter(concatenate_window_n=concat_n)\n",
    "    window_indices, _ = data_import[id].get_windowed_data(clip_dict[id][0], clip_dict[id][1])\n",
    "    import_label = np.array([])\n",
    "    for i, ind in enumerate(window_indices):\n",
    "        import_label = np.hstack((import_label, np.repeat(clip_dict[id][2][i], len(ind))))\n",
    "    data_import[id].normalize_windowed_data()\n",
    "    _, concatenated_data = data_import[id].get_concatenated_data(data_import[id].windowed_data, arrange='channel_stack')\n",
    "    assert import_label.shape[0] == concatenated_data.shape[0]\n",
    "    np.save('rns_test_cache/' + id + '.npy', {'data': concatenated_data, 'label': import_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T08:36:27.675752Z",
     "end_time": "2023-06-22T08:36:28.226120Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNS_Downstream(Dataset):\n",
    "    def __init__(self, data, label, transform=True, astensor=True):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        print('data loaded')\n",
    "\n",
    "        self.label = self.label[np.newaxis].T\n",
    "\n",
    "        self.length = len(self.data)\n",
    "\n",
    "        print(data.shape)\n",
    "        print(label.shape)\n",
    "\n",
    "        if astensor:\n",
    "            self.augmentation = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "                T.RandomApply([T.ColorJitter()], p=0.5),\n",
    "                T.RandomApply([T.GaussianBlur(kernel_size=(3, 3))], p=0.5),\n",
    "                T.RandomInvert(p=0.2),\n",
    "                T.RandomPosterize(4, p=0.2),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "\n",
    "            self.totensor = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            self.augmentation = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "                T.RandomApply([T.ColorJitter()], p=0.5),\n",
    "                T.RandomApply([T.GaussianBlur(kernel_size=(3, 3))], p=0.5),\n",
    "                T.RandomInvert(p=0.2),\n",
    "                T.RandomPosterize(4, p=0.2),\n",
    "            ])\n",
    "\n",
    "            self.totensor = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "\n",
    "        if self.transform:\n",
    "            concat_len = data.shape[1] / 4\n",
    "            channel_index = np.arange(4)\n",
    "            np.random.shuffle(channel_index)\n",
    "            channel_index = channel_index * concat_len + (concat_len - 1) / 2\n",
    "            channel_index = np.repeat(channel_index, concat_len)\n",
    "            concate_len_1 = (concat_len - 1) / 2\n",
    "            a_repeat = np.arange(-concate_len_1, concate_len_1 + 1)[np.newaxis].T\n",
    "            base_repeat = np.repeat(a_repeat, 4, axis=1).T.flatten()\n",
    "            channel_index = channel_index + base_repeat\n",
    "            data = data[channel_index.astype(int)]\n",
    "            data = torch.from_numpy(data).clone()\n",
    "            data = data.repeat(3, 1, 1)\n",
    "            data = self.augmentation(data)\n",
    "\n",
    "        else:\n",
    "            concat_len = data.shape[1] / 4\n",
    "            channel_index = np.arange(4)\n",
    "            # np.random.shuffle(channel_index)\n",
    "            channel_index = channel_index * concat_len + (concat_len - 1) / 2\n",
    "            channel_index = np.repeat(channel_index, concat_len)\n",
    "            concate_len_1 = (concat_len - 1) / 2\n",
    "            a_repeat = np.arange(-concate_len_1, concate_len_1 + 1)[np.newaxis].T\n",
    "            base_repeat = np.repeat(a_repeat, 4, axis=1).T.flatten()\n",
    "            channel_index = channel_index + base_repeat\n",
    "            data = data[channel_index.astype(int)]\n",
    "            data = torch.from_numpy(data).clone()\n",
    "            data = data.repeat(3, 1, 1)\n",
    "            data = self.totensor(data)\n",
    "\n",
    "        return data, torch.from_numpy(label).to(dtype=torch.long), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T08:36:28.222116Z",
     "end_time": "2023-06-22T08:36:29.244950Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "from lightly.data import LightlyDataset, SwaVCollateFunction\n",
    "from lightly.loss import SwaVLoss\n",
    "from lightly.loss.memory_bank import MemoryBankModule\n",
    "from lightly.models.modules import SwaVProjectionHead, SwaVPrototypes\n",
    "\n",
    "\n",
    "class SwaV(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = SwaVProjectionHead(2048, 2048, 128)\n",
    "        self.prototypes = SwaVPrototypes(128, 2048, 1)\n",
    "\n",
    "        self.start_queue_at_epoch = 35\n",
    "        self.queues = nn.ModuleList([MemoryBankModule(size=256) for _ in range(2)])\n",
    "\n",
    "    def forward(self, high_resolution, low_resolution, epoch):\n",
    "        self.prototypes.normalize()\n",
    "\n",
    "        high_resolution_features = [self._subforward(x) for x in high_resolution]\n",
    "        low_resolution_features = [self._subforward(x) for x in low_resolution]\n",
    "\n",
    "        high_resolution_prototypes = [\n",
    "            self.prototypes(x, epoch) for x in high_resolution_features\n",
    "        ]\n",
    "        low_resolution_prototypes = [\n",
    "            self.prototypes(x, epoch) for x in low_resolution_features\n",
    "        ]\n",
    "        queue_prototypes = self._get_queue_prototypes(high_resolution_features, epoch)\n",
    "\n",
    "        return high_resolution_prototypes, low_resolution_prototypes, queue_prototypes\n",
    "\n",
    "    def _subforward(self, input):\n",
    "        features = self.backbone(input).flatten(start_dim=1)\n",
    "        features = self.projection_head(features)\n",
    "        features = nn.functional.normalize(features, dim=1, p=2)\n",
    "        return features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_queue_prototypes(self, high_resolution_features, epoch):\n",
    "        if len(high_resolution_features) != len(self.queues):\n",
    "            raise ValueError(\n",
    "                f\"The number of queues ({len(self.queues)}) should be equal to the number of high \"\n",
    "                f\"resolution inputs ({len(high_resolution_features)}). Set `n_queues` accordingly.\"\n",
    "            )\n",
    "\n",
    "        # Get the queue features\n",
    "        queue_features = []\n",
    "        for i in range(len(self.queues)):\n",
    "            _, features = self.queues[i](high_resolution_features[i], update=True)\n",
    "            # Queue features are in (num_ftrs X queue_length) shape, while the high res\n",
    "            # features are in (batch_size X num_ftrs). Swap the axes for interoperability.\n",
    "            features = torch.permute(features, (1, 0))\n",
    "            queue_features.append(features)\n",
    "\n",
    "        # If loss calculation with queue prototypes starts at a later epoch,\n",
    "        # just queue the features and return None instead of queue prototypes.\n",
    "        if self.start_queue_at_epoch > 0 and epoch < self.start_queue_at_epoch:\n",
    "            return None\n",
    "\n",
    "        # Assign prototypes\n",
    "        queue_prototypes = [self.prototypes(x, epoch) for x in queue_features]\n",
    "        return queue_prototypes\n",
    "\n",
    "\n",
    "resnet = torchvision.models.resnet50()\n",
    "\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "model = SwaV(backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T08:36:29.244950Z",
     "end_time": "2023-06-22T08:36:29.792861Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def sigmoid_focal_loss(\n",
    "        inputs: torch.Tensor,\n",
    "        targets: torch.Tensor,\n",
    "        alpha: float = 0.25,\n",
    "        gamma: float = 2,\n",
    "        reduction: str = \"none\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "\n",
    "    Args:\n",
    "        inputs (Tensor): A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets (Tensor): A float tensor with the same shape as inputs. Stores the binary\n",
    "                classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "        alpha (float): Weighting factor in range (0,1) to balance\n",
    "                positive vs negative examples or -1 for ignore. Default: ``0.25``.\n",
    "        gamma (float): Exponent of the modulating factor (1 - p_t) to\n",
    "                balance easy vs hard examples. Default: ``2``.\n",
    "        reduction (string): ``'none'`` | ``'mean'`` | ``'sum'``\n",
    "                ``'none'``: No reduction will be applied to the output.\n",
    "                ``'mean'``: The output will be averaged.\n",
    "                ``'sum'``: The output will be summed. Default: ``'none'``.\n",
    "    Returns:\n",
    "        Loss tensor with the reduction option applied.\n",
    "    \"\"\"\n",
    "    # Original implementation from https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py\n",
    "\n",
    "    p = torch.sigmoid(inputs)\n",
    "\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    p_t = p * targets + (1 - p) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "\n",
    "    # Check reduction option and return loss accordingly\n",
    "    if reduction == \"none\":\n",
    "        pass\n",
    "    elif reduction == \"mean\":\n",
    "        loss = loss.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        loss = loss.sum()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid Value for arg 'reduction': '{reduction} \\n Supported reduction modes: 'none', 'mean', 'sum'\"\n",
    "        )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T08:36:29.796863Z",
     "end_time": "2023-06-22T08:36:30.359055Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SupervisedDownstream(pl.LightningModule):\n",
    "    def __init__(self, backbone, unfreeze_backbone_at_epoch=100):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.fc1 = nn.Linear(2048, 512)\n",
    "        self.fc2 = nn.Linear(512, 64)\n",
    "        self.fc3 = nn.Linear(64, 8)\n",
    "        self.fc4 = nn.Linear(8, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.alpha = 0.5\n",
    "        self.gamma = 8\n",
    "        self.unfreeze_backbone_at_epoch = unfreeze_backbone_at_epoch\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        if self.current_epoch < self.unfreeze_backbone_at_epoch:\n",
    "            self.backbone.eval()\n",
    "            x = self.backbone(x)\n",
    "            with torch.no_grad():\n",
    "                x = x.view(-1, 2048)\n",
    "        else:\n",
    "            x = self.backbone(x)\n",
    "            x = x.view(-1, 2048)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pred = self.fc4(x)\n",
    "        pred = self.softmax(pred)\n",
    "        label = F.one_hot(y, num_classes=2).squeeze()\n",
    "        loss = sigmoid_focal_loss(pred.float(), label.float(), alpha=self.alpha, gamma=self.gamma, reduction='mean')\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(-1, 2048)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pred = self.fc4(x)\n",
    "        pred = self.softmax(pred)\n",
    "        label = F.one_hot(y, num_classes=2).squeeze()\n",
    "        loss = sigmoid_focal_loss(pred.float(), label.float(), alpha=self.alpha, gamma=self.gamma, reduction='mean')\n",
    "        out = torch.argmax(pred, dim=1)\n",
    "        out = out.detach().cpu().numpy()\n",
    "        target = y.squeeze().detach().cpu().numpy()\n",
    "        precision, recall, fscore, support = sklearn.metrics.precision_recall_fscore_support(out, target,labels = [0,1],zero_division=0)\n",
    "        acc = sklearn.metrics.accuracy_score(out, target)\n",
    "        # print(acc)\n",
    "        # print(precision)\n",
    "        # print(recall)\n",
    "        # print(fscore)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        self.log(\"val_precision\", precision[1])\n",
    "        self.log(\"val_recall\", recall[1])\n",
    "        return pred, label\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        emb = self.backbone(x)\n",
    "        emb = emb.view(-1, 2048)\n",
    "        x = F.relu(self.fc1(emb))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pred = self.fc4(x)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        return pred, y, emb\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T08:36:30.359055Z",
     "end_time": "2023-06-22T08:36:30.891981Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    info = list(zip(*batch))\n",
    "    data = info[0]\n",
    "    label = info[1]\n",
    "    return torch.stack(data), torch.stack(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T08:36:30.891981Z",
     "end_time": "2023-06-22T08:36:31.442836Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['HUP047.npy',\n 'HUP084.npy',\n 'HUP096.npy',\n 'HUP109.npy',\n 'HUP121.npy',\n 'HUP129.npy',\n 'HUP131.npy',\n 'HUP137.npy',\n 'HUP147.npy',\n 'HUP156.npy',\n 'HUP159.npy',\n 'HUP182.npy',\n 'HUP199.npy',\n 'RNS026.npy',\n 'RNS029.npy']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('rns_test_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def get_data(file_names, split=0.7):\n",
    "    file_name_temp = file_names[0]\n",
    "    cache = np.load('rns_test_cache/' + file_name_temp, allow_pickle=True)\n",
    "    temp_file = cache.item().get('data')\n",
    "\n",
    "    train_data = np.empty((0, temp_file.shape[1], temp_file.shape[2]))\n",
    "    train_label = np.array([])\n",
    "    test_data = np.empty((0, temp_file.shape[1], temp_file.shape[2]))\n",
    "    test_label = np.array([])\n",
    "\n",
    "    for name in tqdm(file_names):\n",
    "        cache = np.load('rns_test_cache/' + name, allow_pickle=True)\n",
    "        data = cache.item().get('data')\n",
    "        label = cache.item().get('label')\n",
    "        split_n = int(data.shape[0] * (split))\n",
    "        train_data = np.vstack((train_data, data[:split_n]))\n",
    "        train_label = np.hstack((train_label, label[:split_n]))\n",
    "        test_data = np.vstack((test_data, data[split_n:]))\n",
    "        test_label = np.hstack((test_label, label[split_n:]))\n",
    "\n",
    "    return train_data, train_label, test_data, test_label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-22T08:36:31.441828Z",
     "end_time": "2023-06-22T08:36:31.988914Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31685, 249, 36)\n",
      "(31685,)\n",
      "(73950, 249, 36)\n",
      "(73950,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_list = os.listdir('rns_test_cache')\n",
    "\n",
    "train_data, train_label, test_data, test_label = get_data(data_list, split=0.3)\n",
    "# data, label,_,_ = get_data(data_list, split=1)\n",
    "# train_data, test_data, train_label, test_label = sklearn.model_selection.train_test_split(data, label, test_size=0.8, random_state=42)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-22T08:36:31.987913Z",
     "end_time": "2023-06-22T08:36:50.403105Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "21221.0"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-22T08:36:50.405109Z",
     "end_time": "2023-06-22T08:36:51.015472Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(\"rns_ckpt/checkpoint31.pth\")\n",
    "resnet = torchvision.models.resnet50()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "swav = SwaV(backbone)\n",
    "swav.load_state_dict(ckpt['model_state_dict'])\n",
    "model = SupervisedDownstream(backbone)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "checkpoint_callback = pl_callbacks.ModelCheckpoint(monitor='val_loss',\n",
    "                                                   filename='swav_pretrained-{epoch:02d}-{val_loss:.5f}',\n",
    "                                                   dirpath='rns_linear_checkpoints',every_n_epochs = 5)\n",
    "csv_logger = pl_loggers.CSVLogger(\"rns_linear_logs\", name=\"logger\")\n",
    "\n",
    "trainer = pl.Trainer(logger=csv_logger, max_epochs=80, callbacks=[checkpoint_callback], accelerator='gpu', devices=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-22T08:36:51.016472Z",
     "end_time": "2023-06-22T08:36:52.256309Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-06-22T08:36:52.258311Z",
     "end_time": "2023-06-22T08:37:05.818132Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:616: UserWarning: Checkpoint directory C:\\Users\\Patrick Xu\\Desktop\\RNS_Annotation_Project\\rns_linear_checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | backbone | Sequential | 23.5 M\n",
      "1 | fc1      | Linear     | 1.0 M \n",
      "2 | fc2      | Linear     | 32.8 K\n",
      "3 | fc3      | Linear     | 520   \n",
      "4 | fc4      | Linear     | 18    \n",
      "5 | softmax  | Softmax    | 0     \n",
      "----------------------------------------\n",
      "24.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.6 M    Total params\n",
      "98.362    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "(31685, 249, 36)\n",
      "(31685,)\n",
      "data loaded\n",
      "(73950, 249, 36)\n",
      "(73950,)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc7a4ac9c9d84305bdc2e44bf257b81b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d24518c8b2dc48aab044dcb34c6a4d40"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:653: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "train_dataset = RNS_Downstream(train_data, train_label, transform=True, astensor=True)\n",
    "test_dataset = RNS_Downstream(test_data, test_label, transform=False, astensor=True)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "(73950, 249, 36)\n",
      "(73950,)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = RNS_Downstream(test_data, test_label, transform=False, astensor=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-22T08:37:05.819133Z",
     "end_time": "2023-06-22T08:37:06.398066Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 19it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4960cd4dc6124a50bb844b1eb1b91461"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(model,val_dataloader,ckpt_path='rns_linear_checkpoints/swav_pretrained-epoch=05-val_loss=0.00179_linear_eval_all.ckpt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-22T08:37:06.398066Z",
     "end_time": "2023-06-22T08:38:48.916040Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "output_list = []\n",
    "target_list = []\n",
    "emb_list = []\n",
    "m = nn.Softmax(dim=1)\n",
    "for pred, y, emb in predictions:\n",
    "    output_list.append(pred)\n",
    "    target_list.append(y)\n",
    "    emb_list.append(emb)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-22T08:38:48.915038Z",
     "end_time": "2023-06-22T08:38:49.467793Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.1967,  0.0204],\n        [ 0.1967,  0.0204],\n        [ 0.1967,  0.0204],\n        [ 0.3138, -0.1657],\n        [ 0.7194, -0.8103],\n        [ 0.7422,  0.0987],\n        [ 0.7526,  0.3495],\n        [ 0.8382,  0.5140],\n        [ 0.9884, -1.2379],\n        [ 1.0389, -1.3182],\n        [ 1.0004, -1.2570],\n        [ 0.9462, -1.1708],\n        [ 1.1320, -1.4662],\n        [ 0.8774, -1.0615],\n        [ 0.6491, -0.6987],\n        [ 0.7734, -0.8962],\n        [ 0.9767, -1.2193],\n        [ 1.0257, -1.2973],\n        [ 1.1124, -1.4350],\n        [ 1.1101, -1.4314],\n        [ 1.1618, -1.5135],\n        [ 1.3328, -1.7853],\n        [ 1.2815, -1.7038],\n        [ 1.2903, -1.7177],\n        [ 1.3007, -1.7343],\n        [ 1.0876, -1.3955],\n        [ 0.8402, -1.0024],\n        [ 0.7077, -0.7918],\n        [ 0.6734, -0.7373],\n        [ 0.5488, -0.5391],\n        [ 0.4584, -0.3956],\n        [ 0.4799, -0.4297],\n        [ 0.4111, -0.3203],\n        [ 0.2787, -0.1098],\n        [ 0.5651, -0.5651],\n        [ 0.5445, -0.5323],\n        [ 0.8562, -1.0278],\n        [ 0.8604, -1.0344],\n        [ 1.0818, -1.3864],\n        [ 1.2035, -1.5798],\n        [ 1.2288, -1.6199],\n        [ 1.3642, -1.8352],\n        [ 1.2741, -1.6920],\n        [ 1.3591, -1.8271],\n        [ 1.4642, -1.9942],\n        [ 1.3797, -1.8598],\n        [ 1.4781, -2.0162],\n        [ 1.3186, -1.7628],\n        [ 1.3318, -1.7837],\n        [ 1.2734, -1.6909],\n        [ 1.1133, -1.4365],\n        [ 0.8962, -1.0913],\n        [ 0.6955, -0.7724],\n        [ 0.5870, -0.5999],\n        [ 0.3759, -0.2645],\n        [ 0.6311, -0.6700],\n        [ 0.4646, -0.4054],\n        [ 0.2056,  0.0062],\n        [ 0.3636, -0.2449],\n        [ 0.3217, -0.1782],\n        [ 0.4788, -0.4280],\n        [ 0.3363, -0.2014],\n        [ 0.4404, -0.3668],\n        [ 0.6003, -0.6211],\n        [ 0.5450, -0.5332],\n        [ 0.5410, -0.5269],\n        [ 0.5601, -0.5571],\n        [ 0.5166, -0.4881],\n        [ 0.4305, -0.3511],\n        [ 0.8279, -0.9828],\n        [ 0.7556, -0.8679],\n        [ 0.9033, -1.1026],\n        [ 0.9495, -1.1761],\n        [ 0.8728, -1.0542],\n        [ 0.8473, -1.0137],\n        [ 0.7455, -0.8519],\n        [ 0.5535, -0.5467],\n        [ 0.5216, -0.4960],\n        [ 0.3710, -0.2567],\n        [ 0.3296, -0.1908],\n        [ 0.3748, -0.2626],\n        [ 0.8339, -0.9924],\n        [ 1.0173, -1.2839],\n        [ 0.8983, -1.0947],\n        [ 0.8306, -0.9871],\n        [ 0.8680, -1.0465],\n        [ 0.9466, -1.1714],\n        [ 1.1435, -1.4844],\n        [ 1.1437, -1.4848],\n        [ 1.2233, -1.6112],\n        [ 0.1967,  0.0204],\n        [ 0.1967,  0.0204],\n        [ 0.1967,  0.0204],\n        [ 0.6901, -0.7638]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-22T08:38:49.467793Z",
     "end_time": "2023-06-22T08:38:50.021825Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "pred_raw = torch.vstack(output_list)\n",
    "target = torch.vstack(target_list)\n",
    "emb = torch.vstack(emb_list)\n",
    "out = torch.argmax(pred_raw, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-22T08:38:50.022826Z",
     "end_time": "2023-06-22T08:38:50.655726Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(21221)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(target)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-22T08:38:50.655726Z",
     "end_time": "2023-06-22T08:38:51.221071Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8544962812711291"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(torch.argmax(pred_raw, dim=1), target)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-22T08:38:51.222073Z",
     "end_time": "2023-06-22T08:38:51.788730Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_report = sklearn.metrics.classification_report(torch.argmax(pred_raw, dim=1), target, digits=6)\n",
    "\n",
    "print(f\"Classification Report : \\n{clf_report}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T06:30:54.005638Z",
     "end_time": "2023-04-19T06:30:54.635308Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-19T06:30:54.633307Z",
     "end_time": "2023-04-19T06:30:55.211158Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-19T06:30:55.211158Z",
     "end_time": "2023-04-19T06:30:55.746644Z"
    }
   },
   "outputs": [],
   "source": [
    "len(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-19T06:30:55.747645Z",
     "end_time": "2023-04-19T06:31:52.531071Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_comp_n = 30\n",
    "batch_size = 32\n",
    "\n",
    "pca = PCA(n_components=pca_comp_n, copy=True).fit(emb)\n",
    "p = pca.transform(emb)\n",
    "\n",
    "# ind = np.random.choice(len(emb), 10000)\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=75, random_state=142, init='pca')\n",
    "z = tsne.fit_transform(emb)\n",
    "interictal_inds = np.where(target == 0)[0]\n",
    "ictal_inds = np.where(target == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-19T06:31:52.532072Z",
     "end_time": "2023-04-19T06:31:53.167154Z"
    }
   },
   "outputs": [],
   "source": [
    "spc = z\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# plt.scatter(spc[interictal_inds,0],spc[interictal_inds,1],c='gold',label= 'interictal')\n",
    "plt.scatter(spc[ictal_inds, 0], spc[ictal_inds, 1], c='royalblue', label='ictal')\n",
    "plt.title('Swav Embedding t-SNE')\n",
    "plt.xlabel('comp 1')\n",
    "plt.ylabel(\"comp 2\")\n",
    "plt.legend()\n",
    "plt.xlim(-67, 74)\n",
    "plt.ylim(-67, 75)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-19T06:31:53.149137Z",
     "end_time": "2023-04-19T06:31:53.821331Z"
    }
   },
   "outputs": [],
   "source": [
    "# dt = np.vstack((z[:,0], z[:,1])).T\n",
    "interactive_plot.interactive_plot(z, ['RNS026', 'HUP159', 'HUP129', 'HUP096'], data_import, color_override=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-19T06:31:53.822331Z",
     "end_time": "2023-04-19T06:31:54.497782Z"
    }
   },
   "outputs": [],
   "source": [
    "interactive_plot.interactive_plot(z, ['HUP159'], data_import, color_override=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "RocCurveDisplay.from_predictions(\n",
    "    target,\n",
    "    output[:, 1],\n",
    "    color=\"darkorange\",\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"One-vs-Rest ROC curves:\\nVirginica vs (Setosa & Versicolor)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = torch.argmax(output, dim=1)\n",
    "output = output.detach().cpu().numpy()\n",
    "target = target.squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "clf_report = sklearn.metrics.classification_report(output, target, digits=6)\n",
    "\n",
    "print(f\"Classification Report : \\n{clf_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, label in tqdm(val_dataloader):\n",
    "    batch = batch.to(device)\n",
    "    label = label.to(device)\n",
    "    label = F.one_hot(label).squeeze()\n",
    "    outputs = model(batch)\n",
    "    print(batch)\n",
    "    loss = sigmoid_focal_loss(pred.float(), label.float(), alpha=0.5, gamma=8, reduction='mean')\n",
    "    print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# import torch\n",
    "# import torchvision\n",
    "# from torch import nn\n",
    "#\n",
    "# from lightly.data import DINOCollateFunction, LightlyDataset\n",
    "# from lightly.loss import DINOLoss\n",
    "# from lightly.models.modules import DINOProjectionHead\n",
    "# from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "# from lightly.utils.scheduler import cosine_schedule\n",
    "#\n",
    "#\n",
    "# class DINO(torch.nn.Module):\n",
    "#     def __init__(self, backbone, input_dim):\n",
    "#         super().__init__()\n",
    "#         self.student_backbone = backbone\n",
    "#         self.student_head = DINOProjectionHead(\n",
    "#             input_dim, 512, 64, 2048, freeze_last_layer=1\n",
    "#         )\n",
    "#         self.teacher_backbone = copy.deepcopy(backbone)\n",
    "#         self.teacher_head = DINOProjectionHead(input_dim, 512, 64, 2048)\n",
    "#         deactivate_requires_grad(self.teacher_backbone)\n",
    "#         deactivate_requires_grad(self.teacher_head)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         y = self.student_backbone(x).flatten(start_dim=1)\n",
    "#         z = self.student_head(y)\n",
    "#         return z\n",
    "#\n",
    "#     def forward_teacher(self, x):\n",
    "#         y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "#         z = self.teacher_head(y)\n",
    "#         return z\n",
    "#\n",
    "#\n",
    "# resnet = torchvision.models.resnet18()\n",
    "# backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "# input_dim = 512\n",
    "# # instead of a resnet you can also use a vision transformer backbone as in the\n",
    "# # original paper (you might have to reduce the batch size in this case):\n",
    "# # backbone = torch.hub.load('facebookresearch/dino:main', 'dino_vits16', pretrained=False)\n",
    "# # input_dim = backbone.embed_dim\n",
    "#\n",
    "# model = DINO(backbone, input_dim)\n",
    "#\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n",
    "#\n",
    "# # # we ignore object detection annotations by setting target_transform to return 0\n",
    "# # pascal_voc = torchvision.datasets.VOCDetection(\n",
    "# #     \"datasets/pascal_voc\", download=True, target_transform=lambda t: 0\n",
    "# # )\n",
    "# # dataset = LightlyDataset.from_torch_dataset(pascal_voc)\n",
    "# # # or create a dataset from a folder containing images or videos:\n",
    "# # # dataset = LightlyDataset(\"path/to/folder\")\n",
    "#\n",
    "# collate_fn = DINOCollateFunction(solarization_prob = 0, hf_prob = 0,vf_prob = 0,rr_prob=0,cj_prob=0,random_gray_scale=0)\n",
    "#\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     train_set,\n",
    "#     batch_size=64,\n",
    "#     collate_fn=collate_fn,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     num_workers=1,\n",
    "# )\n",
    "#\n",
    "# criterion = DINOLoss(\n",
    "#     output_dim=2048,\n",
    "#     warmup_teacher_temp_epochs=5,\n",
    "# )\n",
    "# # move loss to correct device because it also contains parameters\n",
    "# criterion = criterion.to(device)\n",
    "#\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#\n",
    "# epochs = 10\n",
    "#\n",
    "# print(\"Starting Training\")\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = 0\n",
    "#     momentum_val = cosine_schedule(epoch, epochs, 0.996, 1)\n",
    "#     for views, _, _ in tqdm(dataloader):\n",
    "#         update_momentum(model.student_backbone, model.teacher_backbone, m=momentum_val)\n",
    "#         update_momentum(model.student_head, model.teacher_head, m=momentum_val)\n",
    "#         views = [view.to(device) for view in views]\n",
    "#         global_views = views[:2]\n",
    "#         teacher_out = [model.forward_teacher(view) for view in global_views]\n",
    "#         student_out = [model.forward(view) for view in views]\n",
    "#         loss = criterion(teacher_out, student_out, epoch=epoch)\n",
    "#         total_loss += loss.detach()\n",
    "#         loss.backward()\n",
    "#         # We only cancel gradients of student head.\n",
    "#         model.student_head.cancel_last_layer_gradients(current_epoch=epoch)\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#\n",
    "#     avg_loss = total_loss / len(dataloader)\n",
    "#     print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((256, 512), interpolation=T.InterpolationMode.NEAREST),\n",
    "    T.RandomApply([T.ColorJitter()], p=0.5),\n",
    "    T.RandomApply([T.GaussianBlur(kernel_size=(3, 3))], p=0.5),\n",
    "    T.RandomInvert(p=0.2),\n",
    "    T.RandomPosterize(4, p=0.2),\n",
    "])\n",
    "\n",
    "data = ictal_data_X[0]\n",
    "\n",
    "channel_index = np.arange(data.shape[0])\n",
    "np.random.shuffle(channel_index)\n",
    "data = data[channel_index]\n",
    "data = torch.from_numpy(data).clone()\n",
    "data = data.repeat(3, 1, 1)\n",
    "data = augmentation(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[channel_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# print(\"Starting Training\")\n",
    "# for epoch in range(50):\n",
    "#     total_loss = 0\n",
    "#     i = 0\n",
    "#     for batch, label in tqdm(dataloader):\n",
    "#         batch = batch.to(device)\n",
    "#         # print(type(batch))\n",
    "#         label = label.to(device)\n",
    "#         label = F.one_hot(label).squeeze()\n",
    "#         outputs = model(batch)\n",
    "#         loss = sigmoid_focal_loss(outputs.float(),label.float(), alpha = 0.25, gamma = 7,reduction = 'mean')\n",
    "#         total_loss += loss.detach()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#\n",
    "#     avg_loss = total_loss / len(dataloader)\n",
    "#     torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': avg_loss,\n",
    "#             }, 'ckpt/checkpoint'+str(epoch)+'.pth')\n",
    "#\n",
    "#     print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
