{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-18T02:39:24.045037Z",
     "end_time": "2024-04-18T02:39:25.547065Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-18T02:39:25.548666Z",
     "end_time": "2024-04-18T02:39:41.470656Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../tools')\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "import random\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import pytorch_lightning.callbacks as pl_callbacks\n",
    "from models.rns_dataloader import get_data, get_data_by_episode\n",
    "\n",
    "import data_utility\n",
    "import annotation_utility\n",
    "import interactive_plot\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*Consider increasing the value of the `num_workers` argument*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*Set a lower value for log_every_n_steps if you want to see logs for the training epoch*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*exists and is not empty*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*Checkpoint directory {dirpath} exists and is not empty*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-18T02:39:41.472656Z",
     "end_time": "2024-04-18T02:39:42.077656Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": "42"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"../../../user_data/\"\n",
    "log_folder_root = '../../../user_data/logs/'\n",
    "ckpt_folder_root = '../../../user_data/checkpoints/'\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    # True ensures the algorithm selected by CUFA is deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # torch.set_deterministic(True)\n",
    "    # False ensures CUDA select the same algorithm each time the application is run\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import pytorch_lightning\n",
    "\n",
    "pytorch_lightning.utilities.seed.seed_everything(seed=random_seed, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-16T14:54:01.598653Z",
     "end_time": "2024-04-16T14:54:02.190652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['HUP047',\n 'HUP084',\n 'HUP096',\n 'HUP109',\n 'HUP121',\n 'HUP129',\n 'HUP131',\n 'HUP137',\n 'HUP147',\n 'HUP153',\n 'HUP156',\n 'HUP159',\n 'HUP182',\n 'HUP197',\n 'HUP199',\n 'HUP205',\n 'RNS026',\n 'RNS029']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_annotations = pd.read_csv(data_dir + 'full_updated_anns_annotTbl_cleaned.csv')\n",
    "ids = list(np.unique(raw_annotations[raw_annotations['descriptions'].notnull()]['HUP_ID']))\n",
    "# ids = list(np.unique(raw_annotations['HUP_ID']))\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T17:16:37.181682Z",
     "end_time": "2024-02-22T17:17:11.065316Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:33<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "data_import = data_utility.read_files(path=data_dir+'rns_data', patientIDs=ids,\n",
    "                                      verbose=True)  # Import data with annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T17:17:16.743087Z",
     "end_time": "2024-02-22T17:17:20.110823Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick Xu\\Desktop\\RNS_Annotation-Pipeline\\scripts\\RNS_LITT_ANNOTATION_PIPELINE\\rns_scripts\\../tools\\annotation_utility.py:137: RuntimeWarning: invalid value encountered in cast\n",
      "  temp_df['Episode_Index'] = episode_ind.astype(int)\n",
      "C:\\Users\\Patrick Xu\\Desktop\\RNS_Annotation-Pipeline\\scripts\\RNS_LITT_ANNOTATION_PIPELINE\\rns_scripts\\../tools\\annotation_utility.py:138: RuntimeWarning: invalid value encountered in cast\n",
      "  temp_df['Episode_Start_Index'] = episode_start_ind.astype(int)\n",
      "C:\\Users\\Patrick Xu\\Desktop\\RNS_Annotation-Pipeline\\scripts\\RNS_LITT_ANNOTATION_PIPELINE\\rns_scripts\\../tools\\annotation_utility.py:139: RuntimeWarning: invalid value encountered in cast\n",
      "  temp_df['Episode_End_Index'] = episode_end_ind.astype(int)\n",
      "C:\\Users\\Patrick Xu\\Desktop\\RNS_Annotation-Pipeline\\scripts\\RNS_LITT_ANNOTATION_PIPELINE\\rns_scripts\\../tools\\annotation_utility.py:137: RuntimeWarning: invalid value encountered in cast\n",
      "  temp_df['Episode_Index'] = episode_ind.astype(int)\n"
     ]
    }
   ],
   "source": [
    "annotations = annotation_utility.read_annotation(annotation_path=data_dir +'full_updated_anns_annotTbl_cleaned.csv',\n",
    "                                                 data=data_import, n_class=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T17:17:31.345872Z",
     "end_time": "2024-02-22T17:17:32.035703Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy.lib.recfunctions as rfn\n",
    "np.random.seed(seed=42)\n",
    "annot = annotations.annotations\n",
    "annot_nonseizure = annot[annot['Class_Code'] == 0]\n",
    "annot_seizure = annot[annot['Class_Code'] == 1]\n",
    "patient_list = list(np.unique(annot['Patient_ID']))\n",
    "# patient_list = ['RNS026', 'HUP159', 'HUP129', 'HUP096', 'HUP182']\n",
    "clip_dict = {}\n",
    "for p in patient_list:\n",
    "    seizure_start_index = np.array([])\n",
    "    seizure_end_index = np.array([])\n",
    "    nonseizure_start_index = np.array([])\n",
    "    nonseizure_end_index = np.array([])\n",
    "    global_episode_index_seizure = np.array([])\n",
    "    global_episode_index_nonseizure = np.array([])\n",
    "    start_index = annot_seizure[annot_seizure['Patient_ID'] == p]['Episode_Start_Index']\n",
    "    end_index = annot_seizure[annot_seizure['Patient_ID'] == p]['Episode_End_Index']\n",
    "    annot_start_list = annot_seizure[annot_seizure['Patient_ID'] == p]['Annotation_Start_Index']\n",
    "    annot_end_list = annot_seizure[annot_seizure['Patient_ID'] == p]['Annotation_End_Index']\n",
    "    episode_index = annot_seizure[annot_seizure['Patient_ID'] == p]['Episode_Index']\n",
    "\n",
    "    for i, slel in enumerate(zip(annot_start_list, annot_end_list, episode_index.index)):\n",
    "        sl_order = np.argsort(slel[0])\n",
    "        sl = np.array(slel[0])[sl_order]\n",
    "        el = np.array(slel[1])[sl_order]\n",
    "        ei = slel[2]\n",
    "\n",
    "        annot_array = np.vstack((sl, el))\n",
    "        seizure_start_index = np.hstack((seizure_start_index, annot_array[0, :]))\n",
    "        seizure_end_index = np.hstack((seizure_end_index, annot_array[1, :]))\n",
    "\n",
    "        nonseizure_start_index = np.hstack((nonseizure_start_index, start_index.iloc[i]))\n",
    "        nonseizure_end_index = np.hstack((nonseizure_end_index, annot_array[0, 0]))\n",
    "\n",
    "        nonseizure_start_index = np.hstack((nonseizure_start_index, annot_array[1, -1]))\n",
    "        nonseizure_end_index = np.hstack((nonseizure_end_index, end_index.iloc[i]))\n",
    "\n",
    "        if annot_array.shape[1] > 1:\n",
    "            nonseizure_start_index = np.hstack((nonseizure_start_index,annot_array[1, :-1]))\n",
    "            nonseizure_end_index = np.hstack((nonseizure_end_index,  annot_array[0, 1:]))\n",
    "\n",
    "\n",
    "        global_episode_index_seizure = np.hstack((global_episode_index_seizure,\n",
    "                                                  np.repeat(ei, len(seizure_start_index) -\n",
    "                                                            len(global_episode_index_seizure))))\n",
    "        global_episode_index_nonseizure = np.hstack((global_episode_index_nonseizure,\n",
    "                                                     np.repeat(ei, len(nonseizure_start_index) -\n",
    "                                                               len(global_episode_index_nonseizure))))\n",
    "\n",
    "    assert len(global_episode_index_nonseizure) == len(nonseizure_start_index)\n",
    "    assert len(global_episode_index_seizure) == len(seizure_start_index)\n",
    "\n",
    "    start_index = annot_nonseizure[annot_nonseizure['Patient_ID'] == p]['Episode_Start_Index']\n",
    "    end_index = annot_nonseizure[annot_nonseizure['Patient_ID'] == p]['Episode_End_Index']\n",
    "    episode_index = start_index.index\n",
    "\n",
    "    nonseizure_ind_arr = np.vstack(\n",
    "        [nonseizure_start_index,\n",
    "         nonseizure_end_index,\n",
    "         global_episode_index_nonseizure]).astype(int)\n",
    "\n",
    "    seizure_ind_arr = np.vstack(\n",
    "        [seizure_start_index,\n",
    "         seizure_end_index,\n",
    "         global_episode_index_seizure]).astype(int)\n",
    "\n",
    "    nonseizure_ind_arr_eps = np.vstack(\n",
    "        [start_index,\n",
    "         end_index,\n",
    "         episode_index]).astype(int)\n",
    "\n",
    "    nonseizure_clip_temp = np.hstack((nonseizure_ind_arr, nonseizure_ind_arr_eps))\n",
    "    nonseizure_clip_label = np.zeros(nonseizure_clip_temp.shape[1]).astype(int)\n",
    "    non_seizure_clip = np.vstack((nonseizure_clip_temp, nonseizure_clip_label))\n",
    "\n",
    "    seizure_clip_temp = np.vstack(\n",
    "            [seizure_start_index,\n",
    "             seizure_end_index,\n",
    "             global_episode_index_seizure]).astype(int)\n",
    "    seizure_clip_label = np.ones(seizure_clip_temp.shape[1]).astype(int)\n",
    "    seizure_clip = np.vstack((seizure_clip_temp, seizure_clip_label))\n",
    "\n",
    "    combined_clip = np.hstack((seizure_clip, non_seizure_clip))\n",
    "\n",
    "    valid = np.where((combined_clip[1] - combined_clip[0]) > 500)\n",
    "\n",
    "    combined_clip = combined_clip[:,valid].squeeze()\n",
    "\n",
    "    if combined_clip.shape[1]>0:\n",
    "        # shuffled_index = np.arange(combined_clip.shape[1])\n",
    "        # np.random.shuffle(shuffled_index)\n",
    "        # clip_dict[p] = combined_clip[:, shuffled_index]\n",
    "\n",
    "\n",
    "        structured_array = rfn.unstructured_to_structured(combined_clip.T.astype(int),\n",
    "                                      np.dtype(\n",
    "                                          [('start_index', 'int32'), ('end_index', 'int32'), ('episode_index', 'int32'),\n",
    "                                           ('label', 'int32')]))\n",
    "\n",
    "        clip_dict[p] = combined_clip.T[np.argsort(structured_array, order=['episode_index','start_index'])].T\n",
    "\n",
    "np.save(data_dir + 'rns_test_cache/clip_dict.npy', clip_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "import numpy.lib.recfunctions as rfn\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "annot = annotations.annotations\n",
    "annot_nonseizure = annot[annot['Class_Code'] == 0]\n",
    "annot_seizure = annot[annot['Class_Code'] == 1]\n",
    "patient_list = ['HUP047',\n",
    "       'HUP059',\n",
    "       'HUP084',\n",
    "       'HUP096',\n",
    "       'HUP101',\n",
    "       'HUP108',\n",
    "       'HUP109',\n",
    "       'HUP121',\n",
    "       'HUP127',\n",
    "       'HUP128',\n",
    "       'HUP129',\n",
    "       'HUP131',\n",
    "       'HUP136',\n",
    "       # 'HUP137',\n",
    "       'HUP143',\n",
    "       'HUP147',\n",
    "       # 'HUP153',\n",
    "       'HUP156',\n",
    "       'HUP159',\n",
    "       'HUP182',\n",
    "       'HUP192',\n",
    "       'HUP197',\n",
    "       'HUP199',\n",
    "       'HUP205',\n",
    "       'RNS021',\n",
    "       'RNS022',\n",
    "       'RNS026',\n",
    "       'RNS029']\n",
    "# patient_list = ['RNS026', 'HUP159', 'HUP129', 'HUP096', 'HUP182']\n",
    "clip_dict = {}\n",
    "# for p in patient_list:\n",
    "for p in patient_list:\n",
    "    # print(p)\n",
    "    seizure_start_index = np.array([])\n",
    "    seizure_end_index = np.array([])\n",
    "    nonseizure_start_index = np.array([])\n",
    "    nonseizure_end_index = np.array([])\n",
    "    global_episode_index_seizure = np.array([])\n",
    "    global_episode_index_nonseizure = np.array([])\n",
    "\n",
    "    annotation_list = []\n",
    "\n",
    "    start_index = annot[annot['Patient_ID'] == p]['Episode_Start_Index']\n",
    "    end_index = annot[annot['Patient_ID'] == p]['Episode_End_Index']\n",
    "    annot_start_list = annot[annot['Patient_ID'] == p]['Annotation_Start_Index']\n",
    "    annot_end_list = annot[annot['Patient_ID'] == p]['Annotation_End_Index']\n",
    "    j = 0\n",
    "    for i in range(len(start_index)):\n",
    "        initial_arr = np.zeros(end_index.iloc[i] - start_index.iloc[i])\n",
    "        if len(annot_start_list.iloc[i]) > 0:\n",
    "            sl_order = np.argsort(annot_start_list.iloc[i])\n",
    "            sl = np.array(annot_start_list.iloc[i])[sl_order]\n",
    "            el = np.array(annot_end_list.iloc[i])[sl_order]\n",
    "\n",
    "            for si, ei in zip(sl, el):\n",
    "                initial_arr[si - start_index.iloc[i]:ei - start_index.iloc[i]] = 1\n",
    "\n",
    "        annotation_list.append(initial_arr)\n",
    "\n",
    "    ind_arr = np.vstack(\n",
    "        [start_index,\n",
    "         end_index,\n",
    "         start_index.index]).astype(int)\n",
    "\n",
    "    valid = np.where((ind_arr[1] - ind_arr[0]) > 500)\n",
    "    combined_clip = ind_arr[:, valid].squeeze()\n",
    "    annotation_list = np.array(annotation_list, dtype=object)[valid]\n",
    "    combined_clip = np.vstack((combined_clip, annotation_list))\n",
    "\n",
    "    if combined_clip.shape[1]>0:\n",
    "\n",
    "        clip_dict[p] = combined_clip\n",
    "\n",
    "np.save(data_dir + 'rns_test_cache/clip_dict.npy', clip_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-27T12:35:20.397114Z",
     "end_time": "2024-02-27T12:35:21.492113Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[995590, 1329499, 2328305, 3584137, 4102460, 4177243, 4850221,\n        6379914, 7127405, 7352695, 8238876, 8734496, 9478014, 9658246,\n        9816002, 11474261, 12082549, 12510541, 17604018, 17773125,\n        20327009, 20372052, 20877529, 22755604, 23402984, 23785908,\n        25182544, 30470712, 31101419, 38707538, 39360873, 40262120,\n        40307172, 40374725, 47095918, 49371398, 54631255, 55531462,\n        59361792, 63012791, 72118005, 73019502, 77797587, 81787042,\n        87286122, 87376250, 90592854, 92801678],\n       [1018115, 1352025, 2350838, 3606654, 4120950, 4199768, 4872752,\n        6402432, 7149927, 7375219, 8261404, 8757023, 9500533, 9680771,\n        9838525, 11496790, 12105067, 12533065, 17626525, 17795653,\n        20349528, 20394566, 20900049, 22778124, 23425506, 23808438,\n        25205066, 30493229, 31123953, 38730076, 39383397, 40284629,\n        40329665, 40397254, 47118431, 49393927, 54653767, 55553974,\n        59384333, 63035308, 72140518, 73042043, 77820128, 81809551,\n        87308632, 87398763, 90615395, 92824185],\n       [1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920,\n        1921, 1922, 1923, 1925, 1926, 1927, 1928, 1929, 1930, 1932, 1933,\n        1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944,\n        1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955,\n        1956, 1957, 1958, 1959],\n       [array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 1., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.]),\n        array([0., 0., 0., ..., 0., 0., 0.])]], dtype=object)"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-27T12:01:33.995289Z",
     "end_time": "2024-02-27T12:01:34.743285Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "data": {
      "text/plain": "(96,)"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation_list.T.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-27T11:57:31.720044Z",
     "end_time": "2024-02-27T11:57:32.415046Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "1910                    []\n1911                    []\n1912             [2335547]\n1913                    []\n1914                    []\n1915                    []\n1916                    []\n1917                    []\n1918                    []\n1919    [7354774, 7360415]\n1920                    []\n1921             [8746040]\n1922                    []\n1923             [9665517]\n1925             [9823226]\n1926                    []\n1927                    []\n1928            [12517826]\n1929                    []\n1930            [17780466]\n1932            [20328635]\n1933                    []\n1934                    []\n1935                    []\n1936                    []\n1937                    []\n1938                    []\n1939                    []\n1940                    []\n1941                    []\n1942                    []\n1943            [40262773]\n1944            [40310001]\n1945                    []\n1946            [47098297]\n1947                    []\n1948            [54633602]\n1949            [55532587]\n1950                    []\n1951            [63015719]\n1952            [72121902]\n1953                    []\n1954                    []\n1955            [81794281]\n1956            [87289418]\n1957            [87379911]\n1958                    []\n1959            [92803159]\nName: Annotation_Start_Index, dtype: object"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot_start_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-26T13:23:10.577332Z",
     "end_time": "2024-02-26T13:23:11.034913Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-28T13:23:34.452710Z",
     "end_time": "2024-02-28T13:23:49.548925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8434,) (8434,) (8434,)\n",
      "(4302,) (4302,) (4302,)\n",
      "(8769,) (8769,) (8769,)\n",
      "(4223,) (4223,) (4223,)\n",
      "(6815,) (6815,) (6815,)\n",
      "(6865,) (6865,) (6865,)\n",
      "(5824,) (5824,) (5824,)\n",
      "(6389,) (6389,) (6389,)\n",
      "(8612,) (8612,) (8612,)\n",
      "(12906,) (12906,) (12906,)\n",
      "(3769,) (3769,) (3769,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick Xu\\Desktop\\RNS_Annotation-Pipeline\\scripts\\RNS_LITT_ANNOTATION_PIPELINE\\rns_scripts\\../tools\\segmentation.py:113: RuntimeWarning: invalid value encountered in cast\n",
      "  concatenate_ind_table = np.empty((clip_len_cumsum[-1], concat_n * 2 + 1)).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2313,) (2313,) (2313,)\n",
      "(4483,) (4483,) (4483,)\n",
      "(8561,) (8561,) (8561,)\n",
      "(8688,) (8688,) (8688,)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "window_len = 1\n",
    "stride = 1\n",
    "concat_n = 4\n",
    "for id in clip_dict.keys():\n",
    "    data_import[id].set_window_parameter(window_length=window_len, window_displacement=stride)\n",
    "    data_import[id].set_concatenation_parameter(concatenate_window_n=concat_n)\n",
    "    window_indices, _ = data_import[id].get_windowed_data(clip_dict[id][0], clip_dict[id][1])\n",
    "    import_indices = []\n",
    "    import_label = []\n",
    "    import_clip_indices = []\n",
    "    import_start_indicies = []\n",
    "    import_patient_ID = []\n",
    "    for i, ind in enumerate(window_indices):\n",
    "        indices = window_indices[i]+1-clip_dict[id][0][i]\n",
    "        offsets = np.arange(249)\n",
    "        full_indices = indices[:,0][:, np.newaxis] + offsets\n",
    "        slices_no_loop = clip_dict[id][3][i][full_indices]\n",
    "        mode_result = mode(slices_no_loop, axis=1)\n",
    "        mode_values = mode_result.mode\n",
    "\n",
    "        # print(mode_values)\n",
    "        import_label.append(mode_values)\n",
    "        import_indices.append(np.repeat(clip_dict[id][2][i], len(ind)))\n",
    "        import_clip_indices.append(np.arange(len(ind)))\n",
    "        import_start_indicies.append(np.repeat(clip_dict[id][0][i], len(ind)))\n",
    "        import_patient_ID.append(np.repeat(id, len(ind)))\n",
    "\n",
    "    import_label = np.hstack(import_label)\n",
    "    import_indices = np.hstack(import_indices)\n",
    "    import_clip_indices = np.hstack(import_clip_indices)\n",
    "    import_start_indicies = np.hstack(import_start_indicies)\n",
    "    import_patient_ID = np.hstack(import_patient_ID)\n",
    "\n",
    "    data_import[id].normalize_windowed_data()\n",
    "    _, concatenated_data = data_import[id].get_concatenated_data(data_import[id].windowed_data, arrange='channel_stack')\n",
    "\n",
    "    assert np.hstack(import_label).shape[0] == concatenated_data.shape[0]\n",
    "\n",
    "    np.save(data_dir+'rns_test_cache/' + id + '.npy', {'data': concatenated_data, 'label': import_label, 'patientID': import_patient_ID, 'indices': np.vstack([import_indices,import_clip_indices,import_start_indicies]).T})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-16T14:54:22.732730Z",
     "end_time": "2024-04-16T14:54:23.709731Z"
    }
   },
   "outputs": [],
   "source": [
    "from models.rns_dataloader import RNS_Downstream\n",
    "from models.SwaV import SwaV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-16T14:54:24.556733Z",
     "end_time": "2024-04-16T14:54:25.225733Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "from lightly.data import LightlyDataset, SwaVCollateFunction\n",
    "from lightly.loss import SwaVLoss\n",
    "from lightly.loss.memory_bank import MemoryBankModule\n",
    "from lightly.models.modules import SwaVProjectionHead, SwaVPrototypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-16T14:54:25.240731Z",
     "end_time": "2024-04-16T14:54:25.958730Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    info = list(zip(*batch))\n",
    "    data = info[0]\n",
    "    label = info[1]\n",
    "    return torch.stack(data), torch.stack(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-18T02:39:42.050658Z",
     "end_time": "2024-04-18T02:39:47.785656Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:05,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(992,)\n",
      "(992,)\n",
      "(992,)\n",
      "(270,)\n",
      "(270,)\n",
      "(270,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_list = os.listdir(data_dir+'rns_test_cache')[1:]\n",
    "\n",
    "# data_list = ['HUP182.npy',   'HUP129.npy',   'HUP109.npy', 'HUP156.npy', 'HUP096.npy', 'RNS026.npy',  'HUP159.npy']\n",
    "# data_list = ['RNS026.npy', 'HUP159.npy', 'HUP129.npy', 'HUP096.npy', 'HUP182.npy']\n",
    "train_data, train_label, test_data, test_label, train_index, test_index = get_data_by_episode(data_list, split=0.8)\n",
    "# data, label,_,_ = get_data(data_list, split=1)\n",
    "# train_data, test_data, train_label, test_label = sklearn.model_selection.train_test_split(data, label, test_size=0.8, random_state=42)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(train_index.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)\n",
    "print(test_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-18T02:39:47.784656Z",
     "end_time": "2024-04-18T02:39:48.312657Z"
    }
   },
   "outputs": [],
   "source": [
    "from models.SupervisedDownstream import SupervisedDownstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-16T14:54:30.427728Z",
     "end_time": "2024-04-16T14:54:31.687603Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SwaV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m swav \u001B[38;5;241m=\u001B[39m \u001B[43mSwaV\u001B[49m()\u001B[38;5;241m.\u001B[39mload_from_checkpoint(ckpt_folder_root \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrns_swav_50_12/rns_swav-epoch=82-swav_loss=2.58204.ckpt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m SupervisedDownstream(swav\u001B[38;5;241m.\u001B[39mbackbone, swav\u001B[38;5;241m.\u001B[39mprojection_head)\n\u001B[0;32m      4\u001B[0m device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'SwaV' is not defined"
     ]
    }
   ],
   "source": [
    "swav = SwaV().load_from_checkpoint(ckpt_folder_root + 'rns_swav_50_12/rns_swav-epoch=82-swav_loss=2.58204.ckpt')\n",
    "model = SupervisedDownstream(swav.backbone, swav.projection_head)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "checkpoint_callback = pl_callbacks.ModelCheckpoint(monitor='val_loss',\n",
    "                                                   filename='rns_swav_50_all_linear_eval-{epoch:02d}-{val_loss:.5f}', save_last=True, save_top_k=-1, dirpath=ckpt_folder_root + 'rns_swav_50_all_linear_eval')\n",
    "csv_logger = pl_loggers.CSVLogger(log_folder_root, name='rns_swav_50_all_linear_eval')\n",
    "\n",
    "trainer = pl.Trainer(logger=csv_logger, max_epochs=80, callbacks=[checkpoint_callback], accelerator='gpu', devices=1,precision=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T17:28:16.222150Z",
     "end_time": "2024-02-22T17:30:09.433023Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                       | Type               | Params\n",
      "------------------------------------------------------------------\n",
      "0 | backbone                   | Sequential         | 23.5 M\n",
      "1 | fc1                        | Linear             | 1.0 M \n",
      "2 | fc2                        | Linear             | 32.8 K\n",
      "3 | dp                         | Dropout1d          | 0     \n",
      "4 | fc3                        | Linear             | 520   \n",
      "5 | fc4                        | Linear             | 18    \n",
      "6 | softmax                    | Softmax            | 0     \n",
      "7 | unfreeze_backbone_at_epoch | SwaVProjectionHead | 4.5 M \n",
      "------------------------------------------------------------------\n",
      "29.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "29.1 M    Total params\n",
      "58.102    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05e5f9ed026840ed9c64b19d44241a89"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [14]\u001B[0m, in \u001B[0;36m<cell line: 21>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m train_dataloader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(\n\u001B[0;32m      6\u001B[0m     train_dataset,\n\u001B[0;32m      7\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     10\u001B[0m     drop_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     11\u001B[0m )\n\u001B[0;32m     13\u001B[0m val_dataloader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(\n\u001B[0;32m     14\u001B[0m     test_dataset,\n\u001B[0;32m     15\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     18\u001B[0m     drop_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     19\u001B[0m )\n\u001B[1;32m---> 21\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:696\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    677\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    678\u001B[0m \u001B[38;5;124;03mRuns the full optimization routine.\u001B[39;00m\n\u001B[0;32m    679\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    693\u001B[0m \u001B[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001B[39;00m\n\u001B[0;32m    694\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    695\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m model\n\u001B[1;32m--> 696\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    697\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[0;32m    698\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001B[0m, in \u001B[0;36mTrainer._call_and_handle_interrupt\u001B[1;34m(self, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[0;32m    648\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    649\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 650\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    651\u001B[0m \u001B[38;5;66;03m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001B[39;00m\n\u001B[0;32m    652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:735\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[0;32m    731\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m ckpt_path \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresume_from_checkpoint\n\u001B[0;32m    732\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__set_ckpt_path(\n\u001B[0;32m    733\u001B[0m     ckpt_path, model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    734\u001B[0m )\n\u001B[1;32m--> 735\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    737\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[0;32m    738\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1166\u001B[0m, in \u001B[0;36mTrainer._run\u001B[1;34m(self, model, ckpt_path)\u001B[0m\n\u001B[0;32m   1162\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39mrestore_training_state()\n\u001B[0;32m   1164\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39mresume_end()\n\u001B[1;32m-> 1166\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1168\u001B[0m log\u001B[38;5;241m.\u001B[39mdetail(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1169\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_teardown()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1252\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1250\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredicting:\n\u001B[0;32m   1251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_predict()\n\u001B[1;32m-> 1252\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1274\u001B[0m, in \u001B[0;36mTrainer._run_train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1271\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pre_training_routine()\n\u001B[0;32m   1273\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[1;32m-> 1274\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1276\u001B[0m \u001B[38;5;66;03m# enable train mode\u001B[39;00m\n\u001B[0;32m   1277\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1343\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1341\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[0;32m   1342\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m-> 1343\u001B[0m     \u001B[43mval_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1345\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1347\u001B[0m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001B[0m, in \u001B[0;36mLoop.run\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 200\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madvance(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[0;32m    202\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py:155\u001B[0m, in \u001B[0;36mEvaluationLoop.advance\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_dataloaders \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    154\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataloader_idx\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m dataloader_idx\n\u001B[1;32m--> 155\u001B[0m dl_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdl_max_batches\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;66;03m# store batch level output per dataloader\u001B[39;00m\n\u001B[0;32m    158\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs\u001B[38;5;241m.\u001B[39mappend(dl_outputs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:200\u001B[0m, in \u001B[0;36mLoop.run\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    199\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 200\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madvance(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    201\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[0;32m    202\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py:143\u001B[0m, in \u001B[0;36mEvaluationEpochLoop.advance\u001B[1;34m(self, data_fetcher, dl_max_batches, kwargs)\u001B[0m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_started()\n\u001B[0;32m    142\u001B[0m \u001B[38;5;66;03m# lightning module methods\u001B[39;00m\n\u001B[1;32m--> 143\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluation_step(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    144\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluation_step_end(output)\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py:240\u001B[0m, in \u001B[0;36mEvaluationEpochLoop._evaluation_step\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;124;03m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001B[39;00m\n\u001B[0;32m    230\u001B[0m \n\u001B[0;32m    231\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;124;03m    the outputs of the step\u001B[39;00m\n\u001B[0;32m    238\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    239\u001B[0m hook_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_step\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mtesting \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 240\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1704\u001B[0m, in \u001B[0;36mTrainer._call_strategy_hook\u001B[1;34m(self, hook_name, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1701\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m   1703\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1704\u001B[0m     output \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1706\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[0;32m   1707\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:370\u001B[0m, in \u001B[0;36mStrategy.validation_step\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    368\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39mval_step_context():\n\u001B[0;32m    369\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, ValidationStep)\n\u001B[1;32m--> 370\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mvalidation_step(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\RNS_Annotation-Pipeline\\scripts\\RNS_LITT_ANNOTATION_PIPELINE\\rns_scripts\\models\\SupervisedDownstream.py:46\u001B[0m, in \u001B[0;36mSupervisedDownstream.validation_step\u001B[1;34m(self, batch, batch_idx)\u001B[0m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidation_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, batch_idx):\n\u001B[1;32m---> 46\u001B[0m     x, y \u001B[38;5;241m=\u001B[39m batch\n\u001B[0;32m     47\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackbone(x)\n\u001B[0;32m     48\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2048\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from models.rns_dataloader import *\n",
    "train_dataset = RNS_Active_by_episode_LSTM(train_data, train_label, transform=True, astensor=True)\n",
    "test_dataset = RNS_Active_by_episode_LSTM(test_data, test_label, transform=False, astensor=True)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=256,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T23:59:18.267350Z",
     "end_time": "2024-01-12T23:59:18.833624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "(21556, 249, 36)\n",
      "(21556,)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = RNS_Downstream(test_data, test_label, transform=False, astensor=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=256,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T23:59:18.840622Z",
     "end_time": "2024-01-12T23:59:31.825643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 249, 36)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:12<00:00,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_list = os.listdir(data_dir+'rns_test_cache')[1:]\n",
    "# data_list = ['RNS026_seizure.npy', 'HUP159_seizure.npy', 'HUP129_seizure.npy', 'HUP096_seizure.npy','RNS026_nonseizure.npy', 'HUP159_nonseizure.npy', 'HUP129_nonseizure.npy', 'HUP096_nonseizure.npy']\n",
    "class RNS_Downstream(Dataset):\n",
    "    def __init__(self, file_names, transform=True, astensor=True):\n",
    "        self.file_names = file_names\n",
    "        self.transform = transform\n",
    "\n",
    "        file_name_temp = self.file_names[0]\n",
    "        cache = np.load(data_dir +'rns_test_cache/' + file_name_temp, allow_pickle=True)\n",
    "        temp_file = cache.item().get('data')\n",
    "\n",
    "        self.data = np.empty((0, temp_file.shape[1], temp_file.shape[2]))\n",
    "        self.label = np.array([])\n",
    "        print(self.data.shape)\n",
    "\n",
    "        for name in tqdm(self.file_names):\n",
    "            cache = np.load(data_dir +'rns_test_cache/' + name, allow_pickle=True)\n",
    "            data = cache.item().get('data')\n",
    "            label = cache.item().get('label')\n",
    "            self.data = np.vstack((self.data, data))\n",
    "            self.label = np.hstack((self.label, label))\n",
    "\n",
    "        print('data loaded')\n",
    "\n",
    "        self.label = self.label[np.newaxis].T\n",
    "\n",
    "        self.length = len(self.data)\n",
    "\n",
    "        if astensor:\n",
    "            self.augmentation = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "                T.RandomApply([T.ColorJitter()], p=0.5),\n",
    "                T.RandomApply([T.GaussianBlur(kernel_size=(3, 3))], p=0.5),\n",
    "                T.RandomInvert(p=0.2),\n",
    "                T.RandomPosterize(4, p=0.2),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "\n",
    "            self.totensor = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            self.augmentation = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "                T.RandomApply([T.ColorJitter()], p=0.5),\n",
    "                T.RandomApply([T.GaussianBlur(kernel_size=(3, 3))], p=0.5),\n",
    "                T.RandomInvert(p=0.2),\n",
    "                T.RandomPosterize(4, p=0.2),\n",
    "            ])\n",
    "\n",
    "            self.totensor = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "            ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "\n",
    "        if self.transform:\n",
    "            concat_len = data.shape[1] / 4\n",
    "            channel_index = np.arange(4)\n",
    "            np.random.shuffle(channel_index)\n",
    "            channel_index = channel_index * concat_len + (concat_len - 1) / 2\n",
    "            channel_index = np.repeat(channel_index, concat_len)\n",
    "            concate_len_1 = (concat_len - 1) / 2\n",
    "            a_repeat = np.arange(-concate_len_1, concate_len_1 + 1)[np.newaxis].T\n",
    "            base_repeat = np.repeat(a_repeat, 4, axis=1).T.flatten()\n",
    "            channel_index = channel_index + base_repeat\n",
    "            data = data[channel_index.astype(int)]\n",
    "            data = torch.from_numpy(data).clone()\n",
    "            data = data.repeat(3, 1, 1)\n",
    "            data = self.augmentation(data)\n",
    "\n",
    "        else:\n",
    "            concat_len = data.shape[1] / 4\n",
    "            channel_index = np.arange(4)\n",
    "            # np.random.shuffle(channel_index)\n",
    "            channel_index = channel_index * concat_len + (concat_len - 1) / 2\n",
    "            channel_index = np.repeat(channel_index, concat_len)\n",
    "            concate_len_1 = (concat_len - 1) / 2\n",
    "            a_repeat = np.arange(-concate_len_1, concate_len_1 + 1)[np.newaxis].T\n",
    "            base_repeat = np.repeat(a_repeat, 4, axis=1).T.flatten()\n",
    "            channel_index = channel_index + base_repeat\n",
    "            data = data[channel_index.astype(int)]\n",
    "            data = torch.from_numpy(data).clone()\n",
    "            data = data.repeat(3, 1, 1)\n",
    "            data = self.totensor(data)\n",
    "\n",
    "        return data, torch.from_numpy(label).to(dtype=torch.long), None\n",
    "\n",
    "dataset = RNS_Downstream(data_list, transform=False,astensor = True)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T17:30:13.753643Z",
     "end_time": "2024-02-22T17:30:43.076479Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "Predicting: 472it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6ed818874f3472c94c01b7baabe6bd4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(model,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T17:30:43.076479Z",
     "end_time": "2024-02-22T17:30:43.557673Z"
    }
   },
   "outputs": [],
   "source": [
    "output_list = []\n",
    "target_list = []\n",
    "emb_list = []\n",
    "m = nn.Softmax(dim=1)\n",
    "for pred, y, emb in predictions:\n",
    "    output_list.append(pred)\n",
    "    target_list.append(y)\n",
    "    emb_list.append(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T17:30:43.557673Z",
     "end_time": "2024-02-22T17:30:43.567674Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T17:30:43.567674Z",
     "end_time": "2024-02-22T17:30:44.035504Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_raw = torch.vstack(output_list)\n",
    "target = torch.vstack(target_list)\n",
    "emb = torch.vstack(emb_list)\n",
    "out = torch.argmax(pred_raw, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T17:26:58.021849Z",
     "end_time": "2024-02-22T17:26:58.636576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.9998, 0.9998, 0.9998,  ..., 0.9928, 0.9836, 0.9926])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(pred_raw.float())[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T17:27:00.393372Z",
     "end_time": "2024-02-22T17:27:01.004214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "21530"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(np.unique(test_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T15:45:00.399008Z",
     "start_time": "2023-11-07T15:44:59.426830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'annotations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [23]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     10\u001B[0m index_array \u001B[38;5;241m=\u001B[39m test_index[index_location[np\u001B[38;5;241m.\u001B[39margsort(test_index[index_location], order\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstart_index\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mslice_index\u001B[39m\u001B[38;5;124m'\u001B[39m])]]\n\u001B[0;32m     11\u001B[0m start_ind \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(index_array[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstart_index\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m---> 13\u001B[0m annot \u001B[38;5;241m=\u001B[39m \u001B[43mannotations\u001B[49m\u001B[38;5;241m.\u001B[39mannotations[annotations\u001B[38;5;241m.\u001B[39mannotations\u001B[38;5;241m.\u001B[39mindex \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39munique(index_array[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepisode_index\u001B[39m\u001B[38;5;124m'\u001B[39m])[\u001B[38;5;241m0\u001B[39m]]\n\u001B[0;32m     15\u001B[0m start_indexes \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     16\u001B[0m start_indexes\u001B[38;5;241m.\u001B[39mextend(annot\u001B[38;5;241m.\u001B[39mAnnotation_Start_Index\u001B[38;5;241m.\u001B[39mitem())\n",
      "\u001B[1;31mNameError\u001B[0m: name 'annotations' is not defined"
     ]
    }
   ],
   "source": [
    "index_list = []\n",
    "for i in range(len(list(np.unique(test_index['episode_index'])))):\n",
    "\n",
    "    print(i)\n",
    "\n",
    "    index_location = np.where(test_index['episode_index'] == list(np.unique(test_index['episode_index']))[i])[0]\n",
    "\n",
    "    # print(target[index_location[np.argsort(test_index[index_location], order=['start_index','slice_index'])]].squeeze(1))\n",
    "    # print(out[index_location[np.argsort(test_index[index_location], order=['start_index','slice_index'])]])\n",
    "    index_array = test_index[index_location[np.argsort(test_index[index_location], order=['start_index','slice_index'])]]\n",
    "    start_ind = np.unique(index_array['start_index'])\n",
    "\n",
    "    annot = annotations.annotations[annotations.annotations.index == np.unique(index_array['episode_index'])[0]]\n",
    "\n",
    "    start_indexes = []\n",
    "    start_indexes.extend(annot.Annotation_Start_Index.item())\n",
    "    start_indexes.extend(annot.Annotation_End_Index.item())\n",
    "    start_indexes.append(annot.Episode_Start_Index.item())\n",
    "    start_indexes.append(annot.Episode_End_Index.item())\n",
    "    # print(start_indexes)\n",
    "\n",
    "    in_list = []\n",
    "    for si in start_ind:\n",
    "        in_list.append(si in start_indexes)\n",
    "\n",
    "    assert True in in_list\n",
    "\n",
    "# [annot.Annotation_Start_Index.item(), annot.Episode_Start_Index, ]\n",
    "\n",
    "    # # print(annotations.annotations[annotations.annotations.index == slice_index])\n",
    "    # if class_code == 0:\n",
    "    #     try:\n",
    "    #         assert (((start_index == annot.Episode_Start_Index) | (end_index == annot.Episode_End_Index)).item())\n",
    "    #     except:\n",
    "    #         print(ind)\n",
    "    #         print(annot)\n",
    "    #\n",
    "    # elif class_code == 1:\n",
    "    #     assert start_index in annot.Annotation_Start_Index.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T17:30:44.038082Z",
     "end_time": "2024-02-22T17:30:44.519436Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.917981072555205"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(torch.argmax(pred_raw, dim=1), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-22T17:30:44.519436Z",
     "end_time": "2024-02-22T17:30:45.008266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0   0.931611  0.946978  0.939231     14428\n",
      "           1   0.888970  0.859287  0.873876      7128\n",
      "\n",
      "    accuracy                       0.917981     21556\n",
      "   macro avg   0.910290  0.903133  0.906554     21556\n",
      "weighted avg   0.917510  0.917981  0.917620     21556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_report = sklearn.metrics.classification_report(torch.argmax(pred_raw, dim=1), target, digits=6)\n",
    "\n",
    "print(f\"Classification Report : \\n{clf_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T15:48:58.029514Z",
     "start_time": "2023-11-07T15:48:57.277837Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T15:48:59.210888Z",
     "start_time": "2023-11-07T15:48:58.446626Z"
    }
   },
   "outputs": [],
   "source": [
    "len(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T14:15:19.163456Z",
     "start_time": "2023-10-27T14:13:19.396992Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_comp_n = 30\n",
    "batch_size = 32\n",
    "\n",
    "pca = PCA(n_components=pca_comp_n, copy=True).fit(emb)\n",
    "p = pca.transform(emb)\n",
    "\n",
    "# ind = np.random.choice(len(emb), 10000)\n",
    "#\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=75, random_state=142, init='pca')\n",
    "z = tsne.fit_transform(emb)\n",
    "interictal_inds = np.where(target == 0)[0]\n",
    "ictal_inds = np.where(target == 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T14:15:57.391035Z",
     "start_time": "2023-10-27T14:15:56.614349Z"
    }
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T14:16:00.862570Z",
     "start_time": "2023-10-27T14:16:00.226993Z"
    }
   },
   "outputs": [],
   "source": [
    "len(interictal_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T14:16:01.545191Z",
     "start_time": "2023-10-27T14:16:00.834545Z"
    }
   },
   "outputs": [],
   "source": [
    "len(ictal_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T14:16:04.283561Z",
     "start_time": "2023-10-27T14:16:03.634599Z"
    }
   },
   "outputs": [],
   "source": [
    "# spc = p\n",
    "#\n",
    "# fig = plt.figure(figsize=(10, 8))\n",
    "# ax = fig.add_subplot(projection='3d')\n",
    "# ax.scatter(spc[interictal_inds,0],spc[interictal_inds,1],spc[interictal_inds,2],c='gold',label= 'interictal')\n",
    "# ax.scatter(spc[ictal_inds, 0], spc[ictal_inds, 1],spc[ictal_inds, 2], c='royalblue', label='ictal')\n",
    "# # plt.title('Swav Embedding t-SNE')\n",
    "# ax.set_xlabel('comp 1')\n",
    "# ax.set_ylabel(\"comp 2\")\n",
    "# ax.set_zlabel(\"comp 2\")\n",
    "# plt.legend()\n",
    "# # plt.xlim(-67, 74)\n",
    "# # plt.ylim(-67, 75)\n",
    "# plt.grid()\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T14:16:18.657990Z",
     "start_time": "2023-10-27T14:16:17.960839Z"
    }
   },
   "outputs": [],
   "source": [
    "spc = p\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(spc[interictal_inds,0],spc[interictal_inds,1],c='gold',label= 'interictal')\n",
    "plt.scatter(spc[ictal_inds, 0], spc[ictal_inds, 1], c='royalblue', label='ictal',s=0.5)\n",
    "# plt.title('Swav Embedding t-SNE')\n",
    "plt.xlabel('comp 1')\n",
    "plt.ylabel(\"comp 2\")\n",
    "plt.legend()\n",
    "# plt.xlim(-67, 74)\n",
    "# plt.ylim(-67, 75)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T06:31:53.821331Z",
     "start_time": "2023-04-19T06:31:53.149137Z"
    }
   },
   "outputs": [],
   "source": [
    "# dt = np.vstack((z[:,0], z[:,1])).T\n",
    "interactive_plot.interactive_plot(z, ['RNS026', 'HUP159', 'HUP129', 'HUP096'], data_import, color_override=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T06:31:54.497782Z",
     "start_time": "2023-04-19T06:31:53.822331Z"
    }
   },
   "outputs": [],
   "source": [
    "interactive_plot.interactive_plot(z, ['HUP159'], data_import, color_override=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(m(pred_raw.float()), target, pos_label=1)\n",
    "sklearn.metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T14:31:38.593770Z",
     "start_time": "2023-10-27T14:31:37.922161Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "\n",
    "RocCurveDisplay.from_predictions(\n",
    "    target,\n",
    "    m(pred_raw.float())[:,1],\n",
    "    color=\"darkorange\",\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
    "plt.hlines(y = 1, xmin = 0, xmax = 1)\n",
    "plt.vlines(x = 0, ymin = 0, ymax = 1)\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"One-vs-Rest ROC curves:\\nVirginica vs (Setosa & Versicolor)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T14:31:39.880000Z",
     "start_time": "2023-10-27T14:31:39.093285Z"
    }
   },
   "outputs": [],
   "source": [
    "PrecisionRecallDisplay.from_predictions(\n",
    "    target,\n",
    "    m(pred_raw.float())[:,1],\n",
    "    color=\"darkorange\",\n",
    ")\n",
    "# plt.plot([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n",
    "\n",
    "plt.axis(\"square\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"One-vs-Rest ROC curves:\\nVirginica vs (Setosa & Versicolor)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T14:26:14.245661Z",
     "start_time": "2023-10-27T14:26:13.585060Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(target,\n",
    "    m(pred_raw.float())[:,1])\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = torch.argmax(output, dim=1)\n",
    "output = output.detach().cpu().numpy()\n",
    "target = target.squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "clf_report = sklearn.metrics.classification_report(output, target, digits=6)\n",
    "\n",
    "print(f\"Classification Report : \\n{clf_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, label in tqdm(val_dataloader):\n",
    "    batch = batch.to(device)\n",
    "    label = label.to(device)\n",
    "    label = F.one_hot(label).squeeze()\n",
    "    outputs = model(batch)\n",
    "    print(batch)\n",
    "    loss = sigmoid_focal_loss(pred.float(), label.float(), alpha=0.5, gamma=8, reduction='mean')\n",
    "    print(loss)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# import torch\n",
    "# import torchvision\n",
    "# from torch import nn\n",
    "#\n",
    "# from lightly.data import DINOCollateFunction, LightlyDataset\n",
    "# from lightly.loss import DINOLoss\n",
    "# from lightly.models.modules import DINOProjectionHead\n",
    "# from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
    "# from lightly.utils.scheduler import cosine_schedule\n",
    "#\n",
    "#\n",
    "# class DINO(torch.nn.Module):\n",
    "#     def __init__(self, backbone, input_dim):\n",
    "#         super().__init__()\n",
    "#         self.student_backbone = backbone\n",
    "#         self.student_head = DINOProjectionHead(\n",
    "#             input_dim, 512, 64, 2048, freeze_last_layer=1\n",
    "#         )\n",
    "#         self.teacher_backbone = copy.deepcopy(backbone)\n",
    "#         self.teacher_head = DINOProjectionHead(input_dim, 512, 64, 2048)\n",
    "#         deactivate_requires_grad(self.teacher_backbone)\n",
    "#         deactivate_requires_grad(self.teacher_head)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         y = self.student_backbone(x).flatten(start_dim=1)\n",
    "#         z = self.student_head(y)\n",
    "#         return z\n",
    "#\n",
    "#     def forward_teacher(self, x):\n",
    "#         y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "#         z = self.teacher_head(y)\n",
    "#         return z\n",
    "#\n",
    "#\n",
    "# resnet = torchvision.models.resnet18()\n",
    "# backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "# input_dim = 512\n",
    "# # instead of a resnet you can also use a vision transformer backbone as in the\n",
    "# # original paper (you might have to reduce the batch size in this case):\n",
    "# # backbone = torch.hub.load('facebookresearch/dino:main', 'dino_vits16', pretrained=False)\n",
    "# # input_dim = backbone.embed_dim\n",
    "#\n",
    "# model = DINO(backbone, input_dim)\n",
    "#\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n",
    "#\n",
    "# # # we ignore object detection annotations by setting target_transform to return 0\n",
    "# # pascal_voc = torchvision.datasets.VOCDetection(\n",
    "# #     \"datasets/pascal_voc\", download=True, target_transform=lambda t: 0\n",
    "# # )\n",
    "# # dataset = LightlyDataset.from_torch_dataset(pascal_voc)\n",
    "# # # or create a dataset from a folder containing images or videos:\n",
    "# # # dataset = LightlyDataset(\"path/to/folder\")\n",
    "#\n",
    "# collate_fn = DINOCollateFunction(solarization_prob = 0, hf_prob = 0,vf_prob = 0,rr_prob=0,cj_prob=0,random_gray_scale=0)\n",
    "#\n",
    "# dataloader = torch.utils.data.DataLoader(\n",
    "#     train_set,\n",
    "#     batch_size=64,\n",
    "#     collate_fn=collate_fn,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,\n",
    "#     num_workers=1,\n",
    "# )\n",
    "#\n",
    "# criterion = DINOLoss(\n",
    "#     output_dim=2048,\n",
    "#     warmup_teacher_temp_epochs=5,\n",
    "# )\n",
    "# # move loss to correct device because it also contains parameters\n",
    "# criterion = criterion.to(device)\n",
    "#\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#\n",
    "# epochs = 10\n",
    "#\n",
    "# print(\"Starting Training\")\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = 0\n",
    "#     momentum_val = cosine_schedule(epoch, epochs, 0.996, 1)\n",
    "#     for views, _, _ in tqdm(dataloader):\n",
    "#         update_momentum(model.student_backbone, model.teacher_backbone, m=momentum_val)\n",
    "#         update_momentum(model.student_head, model.teacher_head, m=momentum_val)\n",
    "#         views = [view.to(device) for view in views]\n",
    "#         global_views = views[:2]\n",
    "#         teacher_out = [model.forward_teacher(view) for view in global_views]\n",
    "#         student_out = [model.forward(view) for view in views]\n",
    "#         loss = criterion(teacher_out, student_out, epoch=epoch)\n",
    "#         total_loss += loss.detach()\n",
    "#         loss.backward()\n",
    "#         # We only cancel gradients of student head.\n",
    "#         model.student_head.cancel_last_layer_gradients(current_epoch=epoch)\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#\n",
    "#     avg_loss = total_loss / len(dataloader)\n",
    "#     print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((256, 512), interpolation=T.InterpolationMode.NEAREST),\n",
    "    T.RandomApply([T.ColorJitter()], p=0.5),\n",
    "    T.RandomApply([T.GaussianBlur(kernel_size=(3, 3))], p=0.5),\n",
    "    T.RandomInvert(p=0.2),\n",
    "    T.RandomPosterize(4, p=0.2),\n",
    "])\n",
    "\n",
    "data = ictal_data_X[0]\n",
    "\n",
    "channel_index = np.arange(data.shape[0])\n",
    "np.random.shuffle(channel_index)\n",
    "data = data[channel_index]\n",
    "data = torch.from_numpy(data).clone()\n",
    "data = data.repeat(3, 1, 1)\n",
    "data = augmentation(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[channel_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# print(\"Starting Training\")\n",
    "# for epoch in range(50):\n",
    "#     total_loss = 0\n",
    "#     i = 0\n",
    "#     for batch, label in tqdm(dataloader):\n",
    "#         batch = batch.to(device)\n",
    "#         # print(type(batch))\n",
    "#         label = label.to(device)\n",
    "#         label = F.one_hot(label).squeeze()\n",
    "#         outputs = model(batch)\n",
    "#         loss = sigmoid_focal_loss(outputs.float(),label.float(), alpha = 0.25, gamma = 7,reduction = 'mean')\n",
    "#         total_loss += loss.detach()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#\n",
    "#     avg_loss = total_loss / len(dataloader)\n",
    "#     torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'loss': avg_loss,\n",
    "#             }, 'ckpt/checkpoint'+str(epoch)+'.pth')\n",
    "#\n",
    "#     print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
