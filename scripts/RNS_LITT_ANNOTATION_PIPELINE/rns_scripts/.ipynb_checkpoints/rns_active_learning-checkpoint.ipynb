{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T02:59:02.571057Z",
     "start_time": "2023-04-20T02:59:00.696478Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T02:59:13.525421Z",
     "start_time": "2023-04-20T02:59:02.544032Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('tools')\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from dataset import get_dataset, get_handler, get_wa_handler\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import query_strategies\n",
    "import models\n",
    "from utils import print_log\n",
    "import kaggle_data_utility\n",
    "import dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import base_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import pytorch_lightning.callbacks as pl_callbacks\n",
    "import data_utility, annotation_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T02:59:14.110954Z",
     "start_time": "2023-04-20T02:59:13.528424Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "from lightly.data import LightlyDataset, SwaVCollateFunction\n",
    "from lightly.loss import SwaVLoss\n",
    "from lightly.loss.memory_bank import MemoryBankModule\n",
    "from lightly.models.modules import SwaVProjectionHead, SwaVPrototypes\n",
    "\n",
    "\n",
    "class SwaV(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = SwaVProjectionHead(2048, 2048, 128)\n",
    "        self.prototypes = SwaVPrototypes(128, 2048, 1)\n",
    "\n",
    "        self.start_queue_at_epoch = 35\n",
    "        self.queues = nn.ModuleList([MemoryBankModule(size=256) for _ in range(2)])\n",
    "\n",
    "    def forward(self, high_resolution, low_resolution, epoch):\n",
    "        self.prototypes.normalize()\n",
    "\n",
    "        high_resolution_features = [self._subforward(x) for x in high_resolution]\n",
    "        low_resolution_features = [self._subforward(x) for x in low_resolution]\n",
    "\n",
    "        high_resolution_prototypes = [\n",
    "            self.prototypes(x, epoch) for x in high_resolution_features\n",
    "        ]\n",
    "        low_resolution_prototypes = [\n",
    "            self.prototypes(x, epoch) for x in low_resolution_features\n",
    "        ]\n",
    "        queue_prototypes = self._get_queue_prototypes(high_resolution_features, epoch)\n",
    "\n",
    "        return high_resolution_prototypes, low_resolution_prototypes, queue_prototypes\n",
    "\n",
    "    def _subforward(self, input):\n",
    "        features = self.backbone(input).flatten(start_dim=1)\n",
    "        features = self.projection_head(features)\n",
    "        features = nn.functional.normalize(features, dim=1, p=2)\n",
    "        return features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_queue_prototypes(self, high_resolution_features, epoch):\n",
    "        if len(high_resolution_features) != len(self.queues):\n",
    "            raise ValueError(\n",
    "                f\"The number of queues ({len(self.queues)}) should be equal to the number of high \"\n",
    "                f\"resolution inputs ({len(high_resolution_features)}). Set `n_queues` accordingly.\"\n",
    "            )\n",
    "\n",
    "        # Get the queue features\n",
    "        queue_features = []\n",
    "        for i in range(len(self.queues)):\n",
    "            _, features = self.queues[i](high_resolution_features[i], update=True)\n",
    "            # Queue features are in (num_ftrs X queue_length) shape, while the high res\n",
    "            # features are in (batch_size X num_ftrs). Swap the axes for interoperability.\n",
    "            features = torch.permute(features, (1, 0))\n",
    "            queue_features.append(features)\n",
    "\n",
    "        # If loss calculation with queue prototypes starts at a later epoch,\n",
    "        # just queue the features and return None instead of queue prototypes.\n",
    "        if self.start_queue_at_epoch > 0 and epoch < self.start_queue_at_epoch:\n",
    "            return None\n",
    "\n",
    "        # Assign prototypes\n",
    "        queue_prototypes = [self.prototypes(x, epoch) for x in queue_features]\n",
    "        return queue_prototypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T02:59:14.479288Z",
     "start_time": "2023-04-20T02:59:14.116958Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "class RNS_Active(Dataset):\n",
    "    def __init__(self, data, label, transform=None, astensor=True):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transform = transform['transform']\n",
    "        print('data loaded')\n",
    "\n",
    "        self.label = self.label[np.newaxis].T\n",
    "\n",
    "        self.length = len(self.data)\n",
    "\n",
    "        print(data.shape)\n",
    "        print(label.shape)\n",
    "\n",
    "        if astensor:\n",
    "            self.augmentation = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "                T.RandomApply([T.ColorJitter()], p=0.5),\n",
    "                T.RandomApply([T.GaussianBlur(kernel_size=(3, 3))], p=0.5),\n",
    "                T.RandomInvert(p=0.2),\n",
    "                T.RandomPosterize(4, p=0.2),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "\n",
    "            self.totensor = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            self.augmentation = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "                T.RandomApply([T.ColorJitter()], p=0.5),\n",
    "                T.RandomApply([T.GaussianBlur(kernel_size=(3, 3))], p=0.5),\n",
    "                T.RandomInvert(p=0.2),\n",
    "                T.RandomPosterize(4, p=0.2),\n",
    "            ])\n",
    "\n",
    "            self.totensor = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        label = self.label[index]\n",
    "\n",
    "        if self.transform:\n",
    "            concat_len = data.shape[1] / 4\n",
    "            channel_index = np.arange(4)\n",
    "            np.random.shuffle(channel_index)\n",
    "            channel_index = channel_index * concat_len + (concat_len - 1) / 2\n",
    "            channel_index = np.repeat(channel_index, concat_len)\n",
    "            concate_len_1 = (concat_len - 1) / 2\n",
    "            a_repeat = np.arange(-concate_len_1, concate_len_1 + 1)[np.newaxis].T\n",
    "            base_repeat = np.repeat(a_repeat, 4, axis=1).T.flatten()\n",
    "            channel_index = channel_index + base_repeat\n",
    "            data = data[channel_index.astype(int)]\n",
    "            data = torch.from_numpy(data).clone()\n",
    "            data = data.repeat(3, 1, 1)\n",
    "            data = self.augmentation(data)\n",
    "\n",
    "        else:\n",
    "            concat_len = data.shape[1] / 4\n",
    "            channel_index = np.arange(4)\n",
    "            # np.random.shuffle(channel_index)\n",
    "            channel_index = channel_index * concat_len + (concat_len - 1) / 2\n",
    "            channel_index = np.repeat(channel_index, concat_len)\n",
    "            concate_len_1 = (concat_len - 1) / 2\n",
    "            a_repeat = np.arange(-concate_len_1, concate_len_1 + 1)[np.newaxis].T\n",
    "            base_repeat = np.repeat(a_repeat, 4, axis=1).T.flatten()\n",
    "            channel_index = channel_index + base_repeat\n",
    "            data = data[channel_index.astype(int)]\n",
    "            data = torch.from_numpy(data).clone()\n",
    "            data = data.repeat(3, 1, 1)\n",
    "            data = self.totensor(data)\n",
    "\n",
    "        return data, torch.from_numpy(label).to(dtype=torch.long), index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T02:59:14.843619Z",
     "start_time": "2023-04-20T02:59:14.487295Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_focal_loss(\n",
    "        inputs: torch.Tensor,\n",
    "        targets: torch.Tensor,\n",
    "        alpha: float = 0.25,\n",
    "        gamma: float = 2,\n",
    "        reduction: str = \"none\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n",
    "\n",
    "    Args:\n",
    "        inputs (Tensor): A float tensor of arbitrary shape.\n",
    "                The predictions for each example.\n",
    "        targets (Tensor): A float tensor with the same shape as inputs. Stores the binary\n",
    "                classification label for each element in inputs\n",
    "                (0 for the negative class and 1 for the positive class).\n",
    "        alpha (float): Weighting factor in range (0,1) to balance\n",
    "                positive vs negative examples or -1 for ignore. Default: ``0.25``.\n",
    "        gamma (float): Exponent of the modulating factor (1 - p_t) to\n",
    "                balance easy vs hard examples. Default: ``2``.\n",
    "        reduction (string): ``'none'`` | ``'mean'`` | ``'sum'``\n",
    "                ``'none'``: No reduction will be applied to the output.\n",
    "                ``'mean'``: The output will be averaged.\n",
    "                ``'sum'``: The output will be summed. Default: ``'none'``.\n",
    "    Returns:\n",
    "        Loss tensor with the reduction option applied.\n",
    "    \"\"\"\n",
    "    # Original implementation from https://github.com/facebookresearch/fvcore/blob/master/fvcore/nn/focal_loss.py\n",
    "\n",
    "    p = torch.sigmoid(inputs)\n",
    "\n",
    "    # print(inputs.size())\n",
    "    # print(targets.size())\n",
    "\n",
    "    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    p_t = p * targets + (1 - p) * (1 - targets)\n",
    "    loss = ce_loss * ((1 - p_t) ** gamma)\n",
    "\n",
    "    if alpha >= 0:\n",
    "        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n",
    "        loss = alpha_t * loss\n",
    "\n",
    "    # Check reduction option and return loss accordingly\n",
    "    if reduction == \"none\":\n",
    "        pass\n",
    "    elif reduction == \"mean\":\n",
    "        loss = loss.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        loss = loss.sum()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Invalid Value for arg 'reduction': '{reduction} \\n Supported reduction modes: 'none', 'mean', 'sum'\"\n",
    "        )\n",
    "    return loss\n",
    "import torch.nn.functional as F\n",
    "import sklearn\n",
    "\n",
    "class ActiveLearning(pl.LightningModule):\n",
    "    def __init__(self, backbone, unfreeze_backbone_at_epoch=100):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.fc1 = nn.Linear(2048, 512)\n",
    "        self.fc2 = nn.Linear(512, 64)\n",
    "        self.fc3 = nn.Linear(64, 8)\n",
    "        self.fc4 = nn.Linear(8, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.alpha = 0.5\n",
    "        self.gamma = 8\n",
    "        self.unfreeze_backbone_at_epoch = unfreeze_backbone_at_epoch\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        if self.current_epoch < self.unfreeze_backbone_at_epoch:\n",
    "            self.backbone.eval()\n",
    "            x = self.backbone(x)\n",
    "            with torch.no_grad():\n",
    "                x = x.view(-1, 2048)\n",
    "        else:\n",
    "            x = self.backbone(x)\n",
    "            x = x.view(-1, 2048)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pred = self.fc4(x)\n",
    "        pred = self.softmax(pred)\n",
    "        label = F.one_hot(y, num_classes=2).squeeze()\n",
    "        loss = sigmoid_focal_loss(pred.float(), label.float(), alpha=self.alpha, gamma=self.gamma, reduction='mean')\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(-1, 2048)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pred = self.fc4(x)\n",
    "        pred = self.softmax(pred)\n",
    "        label = F.one_hot(y, num_classes=2).squeeze()\n",
    "        loss = sigmoid_focal_loss(pred.float(), label.float(), alpha=self.alpha, gamma=self.gamma, reduction='mean')\n",
    "        out = torch.argmax(pred, dim=1)\n",
    "        out = out.detach().cpu().numpy()\n",
    "        target = y.squeeze().detach().cpu().numpy()\n",
    "        precision, recall, fscore, support = sklearn.metrics.precision_recall_fscore_support(out, target,labels = [0,1],zero_division=0)\n",
    "        acc = sklearn.metrics.accuracy_score(out, target)\n",
    "        # print(acc)\n",
    "        # print(precision)\n",
    "        # print(recall)\n",
    "        # print(fscore)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        self.log(\"val_precision\", precision[1])\n",
    "        self.log(\"val_recall\", recall[1])\n",
    "        return pred, label\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        emb = self.backbone(x)\n",
    "        emb = emb.view(-1, 2048)\n",
    "        x = F.relu(self.fc1(emb))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        pred = self.fc4(x)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        return pred, y\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-2)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T02:59:15.210954Z",
     "start_time": "2023-04-20T02:59:14.844620Z"
    }
   },
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    # True ensures the algorithm selected by CUFA is deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # torch.set_deterministic(True)\n",
    "    # False ensures CUDA select the same algorithm each time the application is run\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T02:59:42.046906Z",
     "start_time": "2023-04-20T02:59:15.211955Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:10<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 25)\n",
      "(2, 20)\n",
      "(2, 59)\n",
      "(2, 39)\n",
      "(2, 4)\n",
      "(2, 60)\n",
      "(2, 21)\n",
      "(2, 41)\n",
      "(2, 44)\n",
      "(2, 0)\n",
      "(2, 71)\n",
      "(2, 99)\n",
      "(2, 24)\n",
      "(2, 0)\n",
      "(2, 40)\n",
      "(2, 0)\n",
      "(2, 73)\n",
      "(2, 51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:12<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_annotations = pd.read_csv('full_updated_anns_annotTbl_cleaned.csv')\n",
    "ids = list(np.unique(raw_annotations[raw_annotations['descriptions'].notnull()]['HUP_ID']))\n",
    "data_import = data_utility.read_files(path='data/rns_data', path_data='rns_raw_cache', patientIDs=ids,\n",
    "                                      verbose=True)  # Import data with annotation\n",
    "annotations = annotation_utility.read_annotation(annotation_path='full_updated_anns_annotTbl_cleaned.csv',\n",
    "                                                 data=data_import, n_class=3)\n",
    "annot = annotations.annotations\n",
    "patient_list = list(np.unique(annot['Patient_ID']))\n",
    "# patient_list = ['RNS026', 'HUP159', 'HUP129', 'HUP096', 'HUP182']\n",
    "\n",
    "clip_dict = annotation_utility.combine_annot_index(annot,patient_list, 42)\n",
    "\n",
    "window_len = 1\n",
    "stride = 1\n",
    "concat_n = 4\n",
    "for id in tqdm(clip_dict.keys()):\n",
    "    data_import[id].set_window_parameter(window_length=window_len, window_displacement=stride)\n",
    "    data_import[id].set_concatenation_parameter(concatenate_window_n=concat_n)\n",
    "    window_indices, _ = data_import[id].get_windowed_data(clip_dict[id][0], clip_dict[id][1])\n",
    "    import_label = np.array([])\n",
    "    for i, ind in enumerate(window_indices):\n",
    "        import_label = np.hstack((import_label, np.repeat(clip_dict[id][2][i], len(ind))))\n",
    "    data_import[id].normalize_windowed_data()\n",
    "    _, concatenated_data = data_import[id].get_concatenated_data(data_import[id].windowed_data, arrange='channel_stack')\n",
    "    assert import_label.shape[0] == concatenated_data.shape[0]\n",
    "    np.save('rns_test_cache/' + id + '.npy', {'data': concatenated_data, 'label': import_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T02:59:42.442266Z",
     "start_time": "2023-04-20T02:59:42.049909Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(file_names, split=0.7):\n",
    "    file_name_temp = file_names[0]\n",
    "    cache = np.load('rns_test_cache/' + file_name_temp, allow_pickle=True)\n",
    "    temp_file = cache.item().get('data')\n",
    "\n",
    "    train_data = np.empty((0, temp_file.shape[1], temp_file.shape[2]))\n",
    "    train_label = np.array([])\n",
    "    test_data = np.empty((0, temp_file.shape[1], temp_file.shape[2]))\n",
    "    test_label = np.array([])\n",
    "\n",
    "    for name in tqdm(file_names):\n",
    "        cache = np.load('rns_test_cache/' + name, allow_pickle=True)\n",
    "        data = cache.item().get('data')\n",
    "        label = cache.item().get('label')\n",
    "        split_n = int(data.shape[0] * (split))\n",
    "        train_data = np.vstack((train_data, data[:split_n]))\n",
    "        train_label = np.hstack((train_label, label[:split_n]))\n",
    "        test_data = np.vstack((test_data, data[split_n:]))\n",
    "        test_label = np.hstack((test_label, label[split_n:]))\n",
    "\n",
    "    return train_data, train_label, test_data, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:00:00.201772Z",
     "start_time": "2023-04-20T02:59:42.443267Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84503, 249, 36)\n",
      "(84503,)\n",
      "(21132, 249, 36)\n",
      "(21132,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_list = os.listdir('rns_test_cache')\n",
    "\n",
    "X_train, y_train, X_test, y_test  = get_data(data_list, split=0.8)\n",
    "# data, label,_,_ = get_data(data_list, split=1)\n",
    "# train_data, test_data, train_label, test_label = sklearn.model_selection.train_test_split(data, label, test_size=0.8, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:00:00.601135Z",
     "start_time": "2023-04-20T03:00:00.200771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5320.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:00:00.999498Z",
     "start_time": "2023-04-20T03:00:00.602136Z"
    }
   },
   "outputs": [],
   "source": [
    "nStart = 1\n",
    "nEnd = 20\n",
    "nQuery = 2\n",
    "\n",
    "n_pool = len(y_train)\n",
    "n_test = len(y_test)\n",
    "\n",
    "save_file_name = 'rns_active_lc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:00:01.396859Z",
     "start_time": "2023-04-20T03:00:01.000499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845\n",
      "1690\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "NUM_INIT_LB = int(nStart * n_pool / 100)\n",
    "NUM_QUERY = int(nQuery * n_pool / 100) if nStart != 100 else 0\n",
    "NUM_ROUND = int((int(nEnd * n_pool / 100) - NUM_INIT_LB) / NUM_QUERY) if nStart != 100 else 0\n",
    "if NUM_QUERY != 0:\n",
    "    if (int(nEnd * n_pool / 100) - NUM_INIT_LB) % NUM_QUERY != 0:\n",
    "        NUM_ROUND += 1\n",
    "\n",
    "print(NUM_INIT_LB)\n",
    "print(NUM_QUERY)\n",
    "print(NUM_ROUND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:00:04.038262Z",
     "start_time": "2023-04-20T03:00:01.397860Z"
    }
   },
   "outputs": [],
   "source": [
    "idxs_lb = np.zeros(n_pool, dtype=bool)\n",
    "idxs_tmp = np.arange(n_pool)\n",
    "np.random.shuffle(idxs_tmp)\n",
    "idxs_lb[idxs_tmp[:NUM_INIT_LB]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:00:05.009254Z",
     "start_time": "2023-04-20T03:00:04.040263Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(\"rns_ckpt/checkpoint31.pth\")\n",
    "resnet = torchvision.models.resnet50()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "swav = SwaV(backbone)\n",
    "swav.load_state_dict(ckpt['model_state_dict'])\n",
    "model = ActiveLearning(swav.backbone)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "idxs_train = np.arange(n_pool)[idxs_lb]\n",
    "\n",
    "checkpoint_callback = pl_callbacks.ModelCheckpoint(monitor='val_loss', filename=save_file_name+'_round_0-{epoch:02d}-{val_loss:.5f}', dirpath=save_file_name + '_ckpt')\n",
    "csv_logger = pl_loggers.CSVLogger(save_file_name + '_log', name=\"logger_round_0\")\n",
    "trainer = pl.Trainer( logger=csv_logger, max_epochs=30, callbacks=[checkpoint_callback],accelerator='gpu', devices=1,log_every_n_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:00:06.200779Z",
     "start_time": "2023-04-20T03:00:05.009254Z"
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "modelstate = deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:20:14.842591Z",
     "start_time": "2023-04-20T03:00:06.202781Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | backbone | Sequential | 23.5 M\n",
      "1 | fc1      | Linear     | 1.0 M \n",
      "2 | fc2      | Linear     | 32.8 K\n",
      "3 | fc3      | Linear     | 520   \n",
      "4 | fc4      | Linear     | 18    \n",
      "5 | softmax  | Softmax    | 0     \n",
      "----------------------------------------\n",
      "24.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.6 M    Total params\n",
      "98.362    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "(845, 249, 36)\n",
      "(845,)\n",
      "data loaded\n",
      "(21132, 249, 36)\n",
      "(21132,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5ac8fcfc1b47c3baccc0a2391d6a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32471f5cb22d42ab9500a94f5185a1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab44231e4aa4f4e94337008afe80ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7e83bc3e934162842048849beab8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cbb197bb754ec6a60df4c99b0e73ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d132b4979e2f44f78fcb85d5e295c608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a9a8fac13143069e29d5c218a9da6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3bd4c5c4ba4a27accac1c9afa821ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877a282ea3b445638d3c0ea47845e95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406a51238a564ab1b18159e4b46df8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f12488ab21a467f8af78ca5f783cde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37713a2d40e429898794969af2cd7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83480b428d84359ac40068896d781df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a8a4a35af84965ab9614b7c7afb6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d3fec176ee4da4aefb3e9487ed6fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421a23dc32fd407bb7bed884eb5dcd37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f4d668626d4a1894bbb399585925d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d281d35897e742ea936d65a050326da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccdac49dcd543b484faa50a799fc4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89043719aed246659daf9fcbdcee224a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff1cde771f3494f9736fbb9a0ce9149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82a4591de494d5089f634f46471f29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26b22d2242c441eae8ddd76f00c69e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9386a6f17cdf402da4e2ec9af9e330c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d090d3d99e4aa9b8d7d559dd8b4b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5360cf1aebec4a28a68b01c65b039642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1d2e07510b46e5962805af3440321c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd115e2ea1342b59592f2bc8c8989be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da09a54a77446a8ba61179887a1d7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817d5f733f9b47c8bbe5de1d93732faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8947ddf04a8442fb0bc9bf97f9c849d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f3d56b185e47eda462c20d237896b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    info = list(zip(*batch))\n",
    "    data = info[0]\n",
    "    label = info[1]\n",
    "\n",
    "\n",
    "    return torch.stack(data), torch.stack(label)\n",
    "transforms_param = {'transform_tr': {'transform': True},\n",
    "                    'transform_te': {'transform': False},\n",
    "                    }\n",
    "\n",
    "train_data = RNS_Active(X_train[idxs_train],y_train[idxs_train],transform=transforms_param['transform_tr'])\n",
    "test_data = RNS_Active(X_test,y_test,transform=transforms_param['transform_te'])\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                        batch_size=128,\n",
    "                                        shuffle=True,\n",
    "                                        collate_fn=collate_fn,\n",
    "                                        drop_last=True, )\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:20:15.269980Z",
     "start_time": "2023-04-20T03:20:14.845593Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_query(idxs_lb, n):\n",
    "    inds = np.where(idxs_lb==0)[0]\n",
    "    return inds[np.random.permutation(len(inds))][:n]\n",
    "\n",
    "def entropy_query(X_train, y_train, trainer, model, idxs_lb, n):\n",
    "    idxs_unlabeled = np.arange(n_pool)[~idxs_lb]\n",
    "    untrained_data = RNS_Active(X_train[idxs_unlabeled], y_train[idxs_unlabeled], transform=transforms_param['transform_te'])\n",
    "    untrained_dataloader = torch.utils.data.DataLoader(untrained_data,\n",
    "                                                   batch_size=128,\n",
    "                                                   shuffle=True,\n",
    "                                                   collate_fn=collate_fn,\n",
    "                                                   drop_last=True, )\n",
    "    predictions = trainer.predict(model,untrained_dataloader)\n",
    "\n",
    "    probs = []\n",
    "    m = nn.Softmax(dim=1)\n",
    "    for pred, y in predictions:\n",
    "        out = m(pred)\n",
    "        probs.append(out)\n",
    "    probs = torch.vstack(probs)\n",
    "    # print(probs)\n",
    "    log_probs = torch.log(probs)\n",
    "    # print(log_probs)\n",
    "    U = (probs*log_probs).sum(1)\n",
    "    # print(U.sort())\n",
    "    # print(U.sort()[1][:n])\n",
    "    # print(y_train[idxs_unlabeled[U.sort()[1][:n]][::-1]])\n",
    "    return idxs_unlabeled[U.sort()[1][:n]]\n",
    "\n",
    "def lease_conf_query(X_train, y_train, trainer, model, idxs_lb, n):\n",
    "    idxs_unlabeled = np.arange(n_pool)[~idxs_lb]\n",
    "    untrained_data = RNS_Active(X_train[idxs_unlabeled], y_train[idxs_unlabeled], transform=transforms_param['transform_te'])\n",
    "    untrained_dataloader = torch.utils.data.DataLoader(untrained_data,\n",
    "                                                   batch_size=128,\n",
    "                                                   shuffle=True,\n",
    "                                                   collate_fn=collate_fn,\n",
    "                                                   drop_last=True, )\n",
    "    predictions = trainer.predict(model,untrained_dataloader)\n",
    "    output_list = []\n",
    "    m = nn.Softmax(dim=1)\n",
    "    for pred, y in predictions:\n",
    "        out = m(pred)\n",
    "        output_list.append(out)\n",
    "    probs = torch.vstack(output_list)\n",
    "    # print(probs)\n",
    "    U = probs.max(1)[0]\n",
    "    # print(U)\n",
    "    # print(y_train[idxs_unlabeled[U.sort()[1][:n]][::-1]])\n",
    "    return idxs_unlabeled[U.sort()[1][:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T09:47:23.827594Z",
     "start_time": "2023-04-20T03:20:15.269980Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUM_ROUND' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7934efd4917c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mrd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_ROUND\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Round {}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_ROUND\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mlabeled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_pool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midxs_lb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mNUM_QUERY\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnEnd\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_pool\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlabeled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mNUM_QUERY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnEnd\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_pool\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlabeled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NUM_ROUND' is not defined"
     ]
    }
   ],
   "source": [
    "for rd in range(1, NUM_ROUND + 1):\n",
    "    print('Round {}/{}'.format(rd, NUM_ROUND), flush=True)\n",
    "    labeled = len(np.arange(n_pool)[idxs_lb])\n",
    "    if NUM_QUERY > int(nEnd * n_pool / 100) - labeled:\n",
    "        NUM_QUERY = int(nEnd * n_pool / 100) - labeled\n",
    "\n",
    "    output = lease_conf_query(X_train, y_train, trainer, model, idxs_lb, NUM_QUERY)\n",
    "\n",
    "        # entropy_query(X_train, y_train, trainer, model, idxs_lb, NUM_QUERY)\n",
    "\n",
    "    idxs_lb_previous = deepcopy(idxs_lb)\n",
    "    # output = random_query(idxs_lb, NUM_QUERY)\n",
    "    q_idxs = output\n",
    "    idxs_lb_previous[q_idxs] = True\n",
    "    idxs_lb = idxs_lb_previous\n",
    "    print(len(np.arange(n_pool)[idxs_lb]))\n",
    "\n",
    "    idxs_train = np.arange(n_pool)[idxs_lb]\n",
    "    train_data = RNS_Active(X_train[idxs_train], y_train[idxs_train], transform=transforms_param['transform_tr'])\n",
    "    test_data = RNS_Active(X_test, y_test, transform=transforms_param['transform_te'])\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                                   batch_size=128,\n",
    "                                                   shuffle=True,\n",
    "                                                   collate_fn=collate_fn,\n",
    "                                                   drop_last=True, )\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=128,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    model.load_state_dict(modelstate)\n",
    "    checkpoint_callback = pl_callbacks.ModelCheckpoint(monitor='val_loss', filename=save_file_name+'_round_' + str(\n",
    "        rd) + '-{epoch:02d}-{val_loss:.5f}', dirpath=save_file_name + '_ckpt')\n",
    "    csv_logger = pl_loggers.CSVLogger(save_file_name + '_log', name=\"logger_round_\" + str(rd))\n",
    "    trainer = pl.Trainer(logger=csv_logger, max_epochs=30, callbacks=[checkpoint_callback], accelerator='gpu', devices=1,log_every_n_steps=5)\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
