{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T02:59:02.571057Z",
     "start_time": "2023-04-20T02:59:00.696478Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T02:59:13.525421Z",
     "start_time": "2023-04-20T02:59:02.544032Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('tools')\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import pytorch_lightning.callbacks as pl_callbacks\n",
    "import data_utility, annotation_utility\n",
    "from models.rns_dataloader import get_data, RNS_Downstream\n",
    "from active_learning_utility import get_strategy\n",
    "from active_learning_data import Data\n",
    "from active_learning_net import Net\n",
    "from copy import deepcopy\n",
    "from models.SwaV import SwaV\n",
    "from models.SupervisedDownstream import SupervisedDownstream\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*Consider increasing the value of the `num_workers` argument*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*Set a lower value for log_every_n_steps if you want to see logs for the training epoch*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    # True ensures the algorithm selected by CUFA is deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # torch.set_deterministic(True)\n",
    "    # False ensures CUDA select the same algorithm each time the application is run\n",
    "    torch.backends.cudnn.benchmark = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_dir = \"../../../user_data/\"\n",
    "log_folder_root = '../../../user_data/logs/'\n",
    "ckpt_folder_root = '../../../user_data/checkpoints/'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "strategy_name = 'RandomSampling'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nStart = 1\n",
    "nEnd = 20\n",
    "nQuery = 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "args_task = {'n_epoch': 80,\n",
    "             'transform_train': True,\n",
    "             'strategy_name': strategy_name,\n",
    "             'transform': False,\n",
    "             'loader_tr_args': {'batch_size': 128, 'num_workers': 0, 'collate_fn': collate_fn,\n",
    "                                'drop_last': True},\n",
    "             'loader_te_args': {'batch_size': 256, 'num_workers': 0, 'collate_fn': collate_fn,\n",
    "                                'drop_last': True}\n",
    "             }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_annotations = pd.read_csv(data_dir + 'full_updated_anns_annotTbl_cleaned.csv')\n",
    "ids = list(np.unique(raw_annotations[raw_annotations['descriptions'].notnull()]['HUP_ID']))\n",
    "# ids = list(np.unique(raw_annotations['HUP_ID']))\n",
    "\n",
    "data_import = data_utility.read_files(path=data_dir+'rns_data', path_data=data_dir+'rns_raw_cache', patientIDs=ids,\n",
    "                                      verbose=True)  # Import data with annotation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_pool = len(y_train)\n",
    "n_test = len(y_test)\n",
    "\n",
    "NUM_INIT_LB = int(nStart * n_pool / 100)\n",
    "NUM_QUERY = int(nQuery * n_pool / 100) if nStart != 100 else 0\n",
    "NUM_ROUND = int((int(nEnd * n_pool / 100) - NUM_INIT_LB) / NUM_QUERY) if nStart != 100 else 0\n",
    "if NUM_QUERY != 0:\n",
    "    if (int(nEnd * n_pool / 100) - NUM_INIT_LB) % NUM_QUERY != 0:\n",
    "        NUM_ROUND += 1\n",
    "\n",
    "print(NUM_INIT_LB)\n",
    "print(NUM_QUERY)\n",
    "print(NUM_ROUND)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_list = os.listdir('rns_test_cache')\n",
    "\n",
    "X_train, y_train, X_test, y_test  = get_data(data_list, split=0.8)\n",
    "# data, label,_,_ = get_data(data_list, split=1)\n",
    "# train_data, test_data, train_label, test_label = sklearn.model_selection.train_test_split(data, label, test_size=0.8, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = Data(X_train, y_train, X_test, y_test, RNS_Downstream, args_task)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "swav = SwaV().load_from_checkpoint(\n",
    "    ckpt_folder_root + 'kaggle_dog_swav_34/kaggle_dog_swav-epoch=116-swav_loss=2.73583.ckpt')\n",
    "model = SupervisedDownstream(swav.backbone)\n",
    "# initialize model and save the model state\n",
    "modelstate = deepcopy(model.state_dict())\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "net = Net(model, args_task, device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "strategy = get_strategy(strategy_name, dataset, net, None, args_task)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initial round of training, round 0\n",
    "dataset.initialize_labels(NUM_INIT_LB)\n",
    "strategy.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for rd in range(1, NUM_ROUND +1):\n",
    "    print('round ' + str(rd))\n",
    "    q_idxs = strategy.query(NUM_QUERY)\n",
    "    strategy.update(q_idxs)\n",
    "    strategy.net.round = rd\n",
    "    strategy.net.net.load_state_dict(modelstate)\n",
    "    strategy.train()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T02:59:14.479288Z",
     "start_time": "2023-04-20T02:59:14.116958Z"
    }
   },
   "outputs": [],
   "source": [
    "# import torchvision.transforms as T\n",
    "# from torch.utils.data import Dataset\n",
    "# class RNS_Active(Dataset):\n",
    "#     def __init__(self, data, label, transform=None, astensor=True):\n",
    "#         self.data = data\n",
    "#         self.label = label\n",
    "#         self.transform = transform['transform']\n",
    "#         print('data loaded')\n",
    "#\n",
    "#         self.label = self.label[np.newaxis].T\n",
    "#\n",
    "#         self.length = len(self.data)\n",
    "#\n",
    "#         print(data.shape)\n",
    "#         print(label.shape)\n",
    "#\n",
    "#         if astensor:\n",
    "#             self.augmentation = T.Compose([\n",
    "#                 T.ToPILImage(),\n",
    "#                 T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "#                 T.RandomApply([T.ColorJitter()], p=0.5),\n",
    "#                 T.RandomApply([T.GaussianBlur(kernel_size=(3, 3))], p=0.5),\n",
    "#                 T.RandomInvert(p=0.2),\n",
    "#                 T.RandomPosterize(4, p=0.2),\n",
    "#                 T.ToTensor()\n",
    "#             ])\n",
    "#\n",
    "#             self.totensor = T.Compose([\n",
    "#                 T.ToPILImage(),\n",
    "#                 T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "#                 T.ToTensor()\n",
    "#             ])\n",
    "#         else:\n",
    "#             self.augmentation = T.Compose([\n",
    "#                 T.ToPILImage(),\n",
    "#                 T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "#                 T.RandomApply([T.ColorJitter()], p=0.5),\n",
    "#                 T.RandomApply([T.GaussianBlur(kernel_size=(3, 3))], p=0.5),\n",
    "#                 T.RandomInvert(p=0.2),\n",
    "#                 T.RandomPosterize(4, p=0.2),\n",
    "#             ])\n",
    "#\n",
    "#             self.totensor = T.Compose([\n",
    "#                 T.ToPILImage(),\n",
    "#                 T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "#             ])\n",
    "#\n",
    "#     def __len__(self):\n",
    "#         return self.length\n",
    "#\n",
    "#     def __getitem__(self, index):\n",
    "#         data = self.data[index]\n",
    "#         label = self.label[index]\n",
    "#\n",
    "#         if self.transform:\n",
    "#             concat_len = data.shape[1] / 4\n",
    "#             channel_index = np.arange(4)\n",
    "#             np.random.shuffle(channel_index)\n",
    "#             channel_index = channel_index * concat_len + (concat_len - 1) / 2\n",
    "#             channel_index = np.repeat(channel_index, concat_len)\n",
    "#             concate_len_1 = (concat_len - 1) / 2\n",
    "#             a_repeat = np.arange(-concate_len_1, concate_len_1 + 1)[np.newaxis].T\n",
    "#             base_repeat = np.repeat(a_repeat, 4, axis=1).T.flatten()\n",
    "#             channel_index = channel_index + base_repeat\n",
    "#             data = data[channel_index.astype(int)]\n",
    "#             data = torch.from_numpy(data).clone()\n",
    "#             data = data.repeat(3, 1, 1)\n",
    "#             data = self.augmentation(data)\n",
    "#\n",
    "#         else:\n",
    "#             concat_len = data.shape[1] / 4\n",
    "#             channel_index = np.arange(4)\n",
    "#             # np.random.shuffle(channel_index)\n",
    "#             channel_index = channel_index * concat_len + (concat_len - 1) / 2\n",
    "#             channel_index = np.repeat(channel_index, concat_len)\n",
    "#             concate_len_1 = (concat_len - 1) / 2\n",
    "#             a_repeat = np.arange(-concate_len_1, concate_len_1 + 1)[np.newaxis].T\n",
    "#             base_repeat = np.repeat(a_repeat, 4, axis=1).T.flatten()\n",
    "#             channel_index = channel_index + base_repeat\n",
    "#             data = data[channel_index.astype(int)]\n",
    "#             data = torch.from_numpy(data).clone()\n",
    "#             data = data.repeat(3, 1, 1)\n",
    "#             data = self.totensor(data)\n",
    "#\n",
    "#         return data, torch.from_numpy(label).to(dtype=torch.long), index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T02:59:15.210954Z",
     "start_time": "2023-04-20T02:59:14.844620Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T02:59:42.046906Z",
     "start_time": "2023-04-20T02:59:15.211955Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:10<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 25)\n",
      "(2, 20)\n",
      "(2, 59)\n",
      "(2, 39)\n",
      "(2, 4)\n",
      "(2, 60)\n",
      "(2, 21)\n",
      "(2, 41)\n",
      "(2, 44)\n",
      "(2, 0)\n",
      "(2, 71)\n",
      "(2, 99)\n",
      "(2, 24)\n",
      "(2, 0)\n",
      "(2, 40)\n",
      "(2, 0)\n",
      "(2, 73)\n",
      "(2, 51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:12<00:00,  1.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# raw_annotations = pd.read_csv('full_updated_anns_annotTbl_cleaned.csv')\n",
    "# ids = list(np.unique(raw_annotations[raw_annotations['descriptions'].notnull()]['HUP_ID']))\n",
    "# data_import = data_utility.read_files(path='data/rns_data', path_data='rns_raw_cache', patientIDs=ids,\n",
    "#                                       verbose=True)  # Import data with annotation\n",
    "# annotations = annotation_utility.read_annotation(annotation_path='full_updated_anns_annotTbl_cleaned.csv',\n",
    "#                                                  data=data_import, n_class=3)\n",
    "# annot = annotations.annotations\n",
    "# patient_list = list(np.unique(annot['Patient_ID']))\n",
    "# # patient_list = ['RNS026', 'HUP159', 'HUP129', 'HUP096', 'HUP182']\n",
    "#\n",
    "# clip_dict = annotation_utility.combine_annot_index(annot,patient_list, 42)\n",
    "#\n",
    "# window_len = 1\n",
    "# stride = 1\n",
    "# concat_n = 4\n",
    "# for id in tqdm(clip_dict.keys()):\n",
    "#     data_import[id].set_window_parameter(window_length=window_len, window_displacement=stride)\n",
    "#     data_import[id].set_concatenation_parameter(concatenate_window_n=concat_n)\n",
    "#     window_indices, _ = data_import[id].get_windowed_data(clip_dict[id][0], clip_dict[id][1])\n",
    "#     import_label = np.array([])\n",
    "#     for i, ind in enumerate(window_indices):\n",
    "#         import_label = np.hstack((import_label, np.repeat(clip_dict[id][2][i], len(ind))))\n",
    "#     data_import[id].normalize_windowed_data()\n",
    "#     _, concatenated_data = data_import[id].get_concatenated_data(data_import[id].windowed_data, arrange='channel_stack')\n",
    "#     assert import_label.shape[0] == concatenated_data.shape[0]\n",
    "#     np.save('rns_test_cache/' + id + '.npy', {'data': concatenated_data, 'label': import_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:00:00.201772Z",
     "start_time": "2023-04-20T02:59:42.443267Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:17<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84503, 249, 36)\n",
      "(84503,)\n",
      "(21132, 249, 36)\n",
      "(21132,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:00:00.601135Z",
     "start_time": "2023-04-20T03:00:00.200771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5320.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:00:00.999498Z",
     "start_time": "2023-04-20T03:00:00.602136Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "save_file_name = 'rns_active_lc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:00:01.396859Z",
     "start_time": "2023-04-20T03:00:01.000499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "845\n",
      "1690\n",
      "10\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:00:04.038262Z",
     "start_time": "2023-04-20T03:00:01.397860Z"
    }
   },
   "outputs": [],
   "source": [
    "idxs_lb = np.zeros(n_pool, dtype=bool)\n",
    "idxs_tmp = np.arange(n_pool)\n",
    "np.random.shuffle(idxs_tmp)\n",
    "idxs_lb[idxs_tmp[:NUM_INIT_LB]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:00:05.009254Z",
     "start_time": "2023-04-20T03:00:04.040263Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(\"rns_ckpt/checkpoint31.pth\")\n",
    "resnet = torchvision.models.resnet50()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "swav = SwaV(backbone)\n",
    "swav.load_state_dict(ckpt['model_state_dict'])\n",
    "model = ActiveLearning(swav.backbone)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "idxs_train = np.arange(n_pool)[idxs_lb]\n",
    "\n",
    "checkpoint_callback = pl_callbacks.ModelCheckpoint(monitor='val_loss', filename=save_file_name+'_round_0-{epoch:02d}-{val_loss:.5f}', dirpath=save_file_name + '_ckpt')\n",
    "csv_logger = pl_loggers.CSVLogger(save_file_name + '_log', name=\"logger_round_0\")\n",
    "trainer = pl.Trainer( logger=csv_logger, max_epochs=30, callbacks=[checkpoint_callback],accelerator='gpu', devices=1,log_every_n_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:00:06.200779Z",
     "start_time": "2023-04-20T03:00:05.009254Z"
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "modelstate = deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:20:14.842591Z",
     "start_time": "2023-04-20T03:00:06.202781Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | backbone | Sequential | 23.5 M\n",
      "1 | fc1      | Linear     | 1.0 M \n",
      "2 | fc2      | Linear     | 32.8 K\n",
      "3 | fc3      | Linear     | 520   \n",
      "4 | fc4      | Linear     | 18    \n",
      "5 | softmax  | Softmax    | 0     \n",
      "----------------------------------------\n",
      "24.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.6 M    Total params\n",
      "98.362    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "(845, 249, 36)\n",
      "(845,)\n",
      "data loaded\n",
      "(21132, 249, 36)\n",
      "(21132,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5ac8fcfc1b47c3baccc0a2391d6a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32471f5cb22d42ab9500a94f5185a1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab44231e4aa4f4e94337008afe80ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7e83bc3e934162842048849beab8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cbb197bb754ec6a60df4c99b0e73ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d132b4979e2f44f78fcb85d5e295c608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a9a8fac13143069e29d5c218a9da6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3bd4c5c4ba4a27accac1c9afa821ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877a282ea3b445638d3c0ea47845e95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406a51238a564ab1b18159e4b46df8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f12488ab21a467f8af78ca5f783cde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37713a2d40e429898794969af2cd7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83480b428d84359ac40068896d781df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a8a4a35af84965ab9614b7c7afb6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d3fec176ee4da4aefb3e9487ed6fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421a23dc32fd407bb7bed884eb5dcd37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f4d668626d4a1894bbb399585925d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d281d35897e742ea936d65a050326da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccdac49dcd543b484faa50a799fc4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89043719aed246659daf9fcbdcee224a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff1cde771f3494f9736fbb9a0ce9149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82a4591de494d5089f634f46471f29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26b22d2242c441eae8ddd76f00c69e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9386a6f17cdf402da4e2ec9af9e330c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d090d3d99e4aa9b8d7d559dd8b4b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5360cf1aebec4a28a68b01c65b039642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1d2e07510b46e5962805af3440321c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd115e2ea1342b59592f2bc8c8989be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da09a54a77446a8ba61179887a1d7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817d5f733f9b47c8bbe5de1d93732faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8947ddf04a8442fb0bc9bf97f9c849d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f3d56b185e47eda462c20d237896b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    info = list(zip(*batch))\n",
    "    data = info[0]\n",
    "    label = info[1]\n",
    "\n",
    "\n",
    "    return torch.stack(data), torch.stack(label)\n",
    "transforms_param = {'transform_tr': {'transform': True},\n",
    "                    'transform_te': {'transform': False},\n",
    "                    }\n",
    "\n",
    "train_data = RNS_Active(X_train[idxs_train],y_train[idxs_train],transform=transforms_param['transform_tr'])\n",
    "test_data = RNS_Active(X_test,y_test,transform=transforms_param['transform_te'])\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                        batch_size=128,\n",
    "                                        shuffle=True,\n",
    "                                        collate_fn=collate_fn,\n",
    "                                        drop_last=True, )\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T03:20:15.269980Z",
     "start_time": "2023-04-20T03:20:14.845593Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_query(idxs_lb, n):\n",
    "    inds = np.where(idxs_lb==0)[0]\n",
    "    return inds[np.random.permutation(len(inds))][:n]\n",
    "\n",
    "def entropy_query(X_train, y_train, trainer, model, idxs_lb, n):\n",
    "    idxs_unlabeled = np.arange(n_pool)[~idxs_lb]\n",
    "    untrained_data = RNS_Active(X_train[idxs_unlabeled], y_train[idxs_unlabeled], transform=transforms_param['transform_te'])\n",
    "    untrained_dataloader = torch.utils.data.DataLoader(untrained_data,\n",
    "                                                   batch_size=128,\n",
    "                                                   shuffle=True,\n",
    "                                                   collate_fn=collate_fn,\n",
    "                                                   drop_last=True, )\n",
    "    predictions = trainer.predict(model,untrained_dataloader)\n",
    "\n",
    "    probs = []\n",
    "    m = nn.Softmax(dim=1)\n",
    "    for pred, y in predictions:\n",
    "        out = m(pred)\n",
    "        probs.append(out)\n",
    "    probs = torch.vstack(probs)\n",
    "    # print(probs)\n",
    "    log_probs = torch.log(probs)\n",
    "    # print(log_probs)\n",
    "    U = (probs*log_probs).sum(1)\n",
    "    # print(U.sort())\n",
    "    # print(U.sort()[1][:n])\n",
    "    # print(y_train[idxs_unlabeled[U.sort()[1][:n]][::-1]])\n",
    "    return idxs_unlabeled[U.sort()[1][:n]]\n",
    "\n",
    "def lease_conf_query(X_train, y_train, trainer, model, idxs_lb, n):\n",
    "    idxs_unlabeled = np.arange(n_pool)[~idxs_lb]\n",
    "    untrained_data = RNS_Active(X_train[idxs_unlabeled], y_train[idxs_unlabeled], transform=transforms_param['transform_te'])\n",
    "    untrained_dataloader = torch.utils.data.DataLoader(untrained_data,\n",
    "                                                   batch_size=128,\n",
    "                                                   shuffle=True,\n",
    "                                                   collate_fn=collate_fn,\n",
    "                                                   drop_last=True, )\n",
    "    predictions = trainer.predict(model,untrained_dataloader)\n",
    "    output_list = []\n",
    "    m = nn.Softmax(dim=1)\n",
    "    for pred, y in predictions:\n",
    "        out = m(pred)\n",
    "        output_list.append(out)\n",
    "    probs = torch.vstack(output_list)\n",
    "    # print(probs)\n",
    "    U = probs.max(1)[0]\n",
    "    # print(U)\n",
    "    # print(y_train[idxs_unlabeled[U.sort()[1][:n]][::-1]])\n",
    "    return idxs_unlabeled[U.sort()[1][:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-20T09:47:23.827594Z",
     "start_time": "2023-04-20T03:20:15.269980Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUM_ROUND' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-7934efd4917c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfor\u001B[0m \u001B[0mrd\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mNUM_ROUND\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Round {}/{}'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrd\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mNUM_ROUND\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mflush\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0mlabeled\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn_pool\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midxs_lb\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mNUM_QUERY\u001B[0m \u001B[1;33m>\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnEnd\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mn_pool\u001B[0m \u001B[1;33m/\u001B[0m \u001B[1;36m100\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mlabeled\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m         \u001B[0mNUM_QUERY\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnEnd\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mn_pool\u001B[0m \u001B[1;33m/\u001B[0m \u001B[1;36m100\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mlabeled\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'NUM_ROUND' is not defined"
     ]
    }
   ],
   "source": [
    "for rd in range(1, NUM_ROUND + 1):\n",
    "    print('Round {}/{}'.format(rd, NUM_ROUND), flush=True)\n",
    "    labeled = len(np.arange(n_pool)[idxs_lb])\n",
    "    if NUM_QUERY > int(nEnd * n_pool / 100) - labeled:\n",
    "        NUM_QUERY = int(nEnd * n_pool / 100) - labeled\n",
    "\n",
    "    output = lease_conf_query(X_train, y_train, trainer, model, idxs_lb, NUM_QUERY)\n",
    "\n",
    "        # entropy_query(X_train, y_train, trainer, model, idxs_lb, NUM_QUERY)\n",
    "\n",
    "    idxs_lb_previous = deepcopy(idxs_lb)\n",
    "    # output = random_query(idxs_lb, NUM_QUERY)\n",
    "    q_idxs = output\n",
    "    idxs_lb_previous[q_idxs] = True\n",
    "    idxs_lb = idxs_lb_previous\n",
    "    print(len(np.arange(n_pool)[idxs_lb]))\n",
    "\n",
    "    idxs_train = np.arange(n_pool)[idxs_lb]\n",
    "    train_data = RNS_Active(X_train[idxs_train], y_train[idxs_train], transform=transforms_param['transform_tr'])\n",
    "    test_data = RNS_Active(X_test, y_test, transform=transforms_param['transform_te'])\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                                   batch_size=128,\n",
    "                                                   shuffle=True,\n",
    "                                                   collate_fn=collate_fn,\n",
    "                                                   drop_last=True, )\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=128,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    model.load_state_dict(modelstate)\n",
    "    checkpoint_callback = pl_callbacks.ModelCheckpoint(monitor='val_loss', filename=save_file_name+'_round_' + str(\n",
    "        rd) + '-{epoch:02d}-{val_loss:.5f}', dirpath=save_file_name + '_ckpt')\n",
    "    csv_logger = pl_loggers.CSVLogger(save_file_name + '_log', name=\"logger_round_\" + str(rd))\n",
    "    trainer = pl.Trainer(logger=csv_logger, max_epochs=30, callbacks=[checkpoint_callback], accelerator='gpu', devices=1,log_every_n_steps=5)\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
