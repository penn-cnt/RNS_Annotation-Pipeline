{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T20:47:25.264796Z",
     "end_time": "2024-02-15T20:47:25.379123Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %matplotlib inline\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T20:47:25.380123Z",
     "end_time": "2024-02-15T20:47:25.491564Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../tools')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0MBB1IVas91",
    "ExecuteTime": {
     "start_time": "2024-02-15T20:47:25.491564Z",
     "end_time": "2024-02-15T20:47:29.939620Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import math\n",
    "from torchsummary import summary\n",
    "import math\n",
    "import lightning as L\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from model_config import MODEL_CONFIG\n",
    "from lightly.models.modules import SwaVPrototypes, SwaVProjectionHead\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch import callbacks as pl_callbacks\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "\n",
    "matplotlib.use(\"nbAgg\")\n",
    "\n",
    "import data_utility\n",
    "\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T20:47:29.937621Z",
     "end_time": "2024-02-15T20:47:30.274767Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "data_dir = \"../../../user_data/\"\n",
    "log_folder_root = '../../../user_data/logs/'\n",
    "ckpt_folder_root = '../../../user_data/checkpoints/'\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    # True ensures the algorithm selected by CUFA is deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # torch.set_deterministic(True)\n",
    "    # False ensures CUDA select the same algorithm each time the application is run\n",
    "    torch.backends.cudnn.benchmark = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T20:47:30.268743Z",
     "end_time": "2024-02-15T20:47:30.598271Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T20:47:30.599273Z",
     "end_time": "2024-02-15T20:47:30.930358Z"
    }
   },
   "outputs": [],
   "source": [
    "dir_list = os.listdir(data_dir+'rns_data')\n",
    "patientIDs = [s for s in dir_list for type_string in ['HUP', 'RNS'] if type_string in s.upper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T20:47:30.931358Z",
     "end_time": "2024-02-15T20:47:31.262374Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['HUP047.npy',\n 'HUP059.npy',\n 'HUP084.npy',\n 'HUP096.npy',\n 'HUP101.npy',\n 'HUP108.npy',\n 'HUP109.npy',\n 'HUP121.npy',\n 'HUP127.npy',\n 'HUP128.npy',\n 'HUP129.npy',\n 'HUP131.npy',\n 'HUP136.npy',\n 'HUP137.npy',\n 'HUP143.npy',\n 'HUP147.npy',\n 'HUP153.npy',\n 'HUP156.npy',\n 'HUP159.npy',\n 'HUP182.npy',\n 'HUP192.npy',\n 'HUP197.npy',\n 'HUP199.npy',\n 'HUP205.npy',\n 'RNS021.npy',\n 'RNS022.npy',\n 'RNS026.npy',\n 'RNS029.npy']"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir+'rns_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g05XnHgHas-D",
    "ExecuteTime": {
     "start_time": "2024-02-15T20:47:31.264374Z",
     "end_time": "2024-02-15T20:47:36.923744Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.88it/s]\n"
     ]
    }
   ],
   "source": [
    "data_import = data_utility.read_files(path = data_dir+'rns_data', path_data = data_dir+'rns_raw_cache',patientIDs=patientIDs[:10], annotation_only = False, verbose=True)\n",
    "ids = list(data_import.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T20:47:36.925744Z",
     "end_time": "2024-02-15T20:47:39.524057Z"
    }
   },
   "outputs": [],
   "source": [
    "window_len = 10\n",
    "stride = 10\n",
    "concat_n = 1\n",
    "\n",
    "id = ids[0]\n",
    "\n",
    "data_import[id].set_window_parameter(window_length=window_len, window_displacement=stride)\n",
    "data_import[id].normalize_data()\n",
    "_, sliced_data = data_import[id].get_windowed_data(data_import[id].catalog[\"Event Start idx\"],data_import[id].catalog[\"Event End idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class RNSDataset(Dataset):\n",
    "    def __init__(self, sliced_data, transform=False):\n",
    "        # load data\n",
    "        self.data = torch.tensor(sliced_data)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.tensor_transform = T.Compose([\n",
    "            T.RandomApply([T.ColorJitter(brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2)], p=0.8),\n",
    "            T.RandomApply([T.GaussianBlur(kernel_size=(5, 5))], p=0.6),\n",
    "        ])\n",
    "\n",
    "    def get_cropped_data(self, tensor, cropped_len):\n",
    "\n",
    "        resized_len = 2000\n",
    "        tensor = tensor.transpose(1,0)\n",
    "\n",
    "        start_indices = torch.randint(low=0, high=tensor.size()[-1] - cropped_len, size=(1,))\n",
    "        indices = (start_indices + torch.arange(cropped_len)).repeat(tensor.size(0), 1)\n",
    "        cropped_tensor = tensor.gather(dim=1, index=indices)\n",
    "\n",
    "        cropped_tensor = cropped_tensor.repeat_interleave(int(resized_len/cropped_len),dim= 1)\n",
    "\n",
    "        return cropped_tensor\n",
    "\n",
    "    def multicrop(self, tensor):\n",
    "        # get multiple crops of the data in time domain, but perseveres all the channels\n",
    "\n",
    "        size_list = [2000, 2000, 500, 500, 500, 500]\n",
    "        if self.transform:\n",
    "            data_list = [self.tensor_transform(self.get_cropped_data(tensor, sl).unsqueeze(0)).squeeze(0)\n",
    "                         for sl in size_list]\n",
    "        else:\n",
    "            data_list = [self.get_cropped_data(tensor, sl) for sl in size_list]\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_data = self.data[idx]\n",
    "\n",
    "        sample_data = self.multicrop(sample_data)\n",
    "\n",
    "        return sample_data, idx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T20:47:39.527058Z",
     "end_time": "2024-02-15T20:47:39.860152Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# next(iter(RNSDataset(sliced_data)))[0][2].size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T20:47:39.855149Z",
     "end_time": "2024-02-15T20:47:40.185371Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# tt = torch.tensor([[1,2,3,4,5,6],[1,2,3,4,5,6]])\n",
    "# tt.repeat_interleave(3,dim= 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T20:47:40.185371Z",
     "end_time": "2024-02-15T20:47:40.516753Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# tt.size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T20:47:40.517757Z",
     "end_time": "2024-02-15T20:47:40.853848Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# from models.rns_dataloader import RNS_Raw\n",
    "# # unlabeled_dataset = RNS_Raw(file_list, transform=True,astensor = False)\n",
    "# from lightly.data import SwaVCollateFunction\n",
    "#\n",
    "# dir_list = os.listdir(data_dir+'rns_cache')\n",
    "# # file_list = ['HUP084.npy','HUP131.npy','HUP096.npy']\n",
    "# # if self.current_epoch == 0:\n",
    "# #     file_list = ['HUP101.npy']\n",
    "# file_list = ['RNS026.npy', 'HUP159.npy', 'HUP129.npy', 'HUP096.npy']\n",
    "# unlabeled_dataset = RNS_Raw(file_list, transform=True, astensor=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T20:47:40.849849Z",
     "end_time": "2024-02-15T20:47:41.175970Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# next(iter(unlabeled_dataset))[0].size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T20:47:41.176969Z",
     "end_time": "2024-02-15T20:47:41.525083Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads=heads, dim_head=dim_head),\n",
    "                FeedForward(dim, mlp_dim)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "# %%\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self, dim1, dim2):\n",
    "        super(Transpose, self).__init__()\n",
    "        self.dim1 = dim1\n",
    "        self.dim2 = dim2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.transpose(self.dim1, self.dim2)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# %%\n",
    "class MultivarWav2Vec2(nn.Module):\n",
    "    \"\"\"\n",
    "    Multivariate Wav2Vec2 model. This model takes in multiple 1D waveforms as input and combines the representations.\n",
    "    The input waveforms are passed through individual 1D convolutional layers, and the representations are combined\n",
    "    by taking the mean across the channels. The combined representations are then passed through a Wav2Vec2 transformer\n",
    "    encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(MultivarWav2Vec2, self).__init__()\n",
    "\n",
    "        #  1D convolutions applied to each waveform, a series of 1D convolutions\n",
    "        #  followed by layer normalization and GELU activation\n",
    "        self.ft_enc = nn.ModuleList()\n",
    "        for i, _ in enumerate(config.ft_enc_dims):\n",
    "            if i == 0:\n",
    "                self.ft_enc.append(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=4,\n",
    "                        out_channels=config.ft_enc_dims[i],\n",
    "                        kernel_size=config.ft_enc_kernel_widths[i],\n",
    "                        stride=config.ft_enc_strides[i],\n",
    "                        padding=0,\n",
    "                        groups=config.channel_buffer_size,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.ft_enc.append(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=config.ft_enc_dims[i - 1],\n",
    "                        out_channels=config.ft_enc_dims[i],\n",
    "                        kernel_size=config.ft_enc_kernel_widths[i],\n",
    "                        stride=config.ft_enc_strides[i],\n",
    "                        padding=0,\n",
    "                        groups=config.channel_buffer_size,\n",
    "                    )\n",
    "                )\n",
    "            # transpose the output of the convolutional layer\n",
    "            self.ft_enc.append(Transpose(1, 2))\n",
    "            # layer normalization\n",
    "            self.ft_enc.append(nn.LayerNorm(config.ft_enc_dims[i]))\n",
    "            # GELU activation\n",
    "            self.ft_enc.append(nn.GELU())\n",
    "            # transpose the output of the convolutional layer\n",
    "            self.ft_enc.append(Transpose(1, 2))\n",
    "\n",
    "        # add a adaptive pool so different sized crops can be the same length afterward\n",
    "        self.ft_enc.append(nn.AdaptiveAvgPool1d(config.spatial_transformer_hidden))\n",
    "\n",
    "        # convert the list of modules to a sequential module\n",
    "        self.ft_enc = nn.Sequential(*self.ft_enc)\n",
    "\n",
    "        self.fc_layer = nn.Linear(\n",
    "            in_features=config.ft_enc_dims[-1] * config.channel_buffer_size,\n",
    "            out_features=1,\n",
    "        )\n",
    "\n",
    "        # positional encoding\n",
    "        self.temporal_pos_encoder = PositionalEncoding(config.temporal_transformer_hidden, config.dropout)\n",
    "\n",
    "        # add learnable class token for embeddings\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, config.temporal_transformer_hidden))\n",
    "\n",
    "        self.projection_head = SwaVProjectionHead(config.temporal_transformer_hidden, 256, config.prototype_dim)\n",
    "\n",
    "        self.prototypes = SwaVPrototypes(config.prototype_dim, n_prototypes=config.prototype_n,\n",
    "                                         n_steps_frozen_prototypes=0)\n",
    "\n",
    "        self.spatial_transformer = Transformer(config.spatial_transformer_hidden,\n",
    "                                               config.spatial_transformer_blocks,\n",
    "                                               config.spatial_transformer_heads,\n",
    "                                               config.spatial_transformer_inner_heads,\n",
    "                                               config.spatial_transformer_mlp_dim)\n",
    "\n",
    "        self.temporal_transformer = Transformer(config.temporal_transformer_hidden,\n",
    "                                                config.temporal_transformer_blocks,\n",
    "                                                config.temporal_transformer_heads,\n",
    "                                                config.temporal_transformer_inner_heads,\n",
    "                                                config.temporal_transformer_mlp_dim)\n",
    "        self.config = config\n",
    "\n",
    "        crop_transforms = []\n",
    "        crop_sizes=[224, 96]\n",
    "        crop_min_scales = [0.14, 0.05]\n",
    "        crop_max_scales = [1.0, 0.14]\n",
    "        crop_counts = [2, 6]\n",
    "        for i in range(len(crop_sizes)):\n",
    "            random_resized_crop = T.RandomResizedCrop(crop_sizes[i], scale=(crop_min_scales[i], crop_max_scales[i]))\n",
    "\n",
    "            crop_transforms.extend([T.Compose([random_resized_crop, ])] * crop_counts[i])\n",
    "        crop_transforms\n",
    "\n",
    "        self.crop_transforms = crop_transforms\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x is a list of waveforms, each of shape (batch_size, num_channels, sequence_length)\n",
    "\n",
    "        # computing eeg_channel wise convolution, reshape to put eeg_channels in batches so it can run faster\n",
    "        # x = torch.reshape(x, (-1, x.size()[-1])).unsqueeze(1)\n",
    "\n",
    "        # print(x.size())\n",
    "        x = self.ft_enc(x)\n",
    "\n",
    "        views = []\n",
    "        for tf in self.crop_transforms:\n",
    "            views.append(tf(x))\n",
    "\n",
    "        x = views[2]\n",
    "\n",
    "\n",
    "        # print(x.size())\n",
    "        # x = x.reshape((-1, self.config.spatial_transformer_hidden)).reshape(\n",
    "        #     (-1, self.config.temporal_transformer_hidden, self.config.spatial_transformer_hidden))\n",
    "        # print(x.size())\n",
    "        # print('=======================')\n",
    "\n",
    "\n",
    "        # print(x.size())\n",
    "\n",
    "        # pass to spatial encoder\n",
    "        # x = self.spatial_transformer(x)\n",
    "        #\n",
    "        # x = x.transpose(1, 2)\n",
    "        #\n",
    "        # # add class tokens\n",
    "        # cls_tokens = repeat(self.class_token, '1 1 d -> b 1 d', b=x.size(0))\n",
    "        # x = torch.cat((cls_tokens, x), dim=1)\n",
    "        # # add temporal positional encoding\n",
    "        # x = self.temporal_pos_encoder(x.transpose(0, 1)).transpose(0, 1)\n",
    "        # # pass to temporal encoder\n",
    "        # x = self.temporal_transformer(x)\n",
    "        # # recover class token\n",
    "        # x = x[:, 0]\n",
    "        # # x = x.flatten(1)\n",
    "        #\n",
    "        # # pass to swav projection head\n",
    "        # x = self.projection_head(x)\n",
    "        # x = nn.functional.normalize(x, dim=1, p=2)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T20:48:13.628322Z",
     "end_time": "2024-02-15T20:48:14.101589Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 16, 666]              64\n",
      "         Transpose-2              [-1, 666, 16]               0\n",
      "         LayerNorm-3              [-1, 666, 16]              32\n",
      "              GELU-4              [-1, 666, 16]               0\n",
      "         Transpose-5              [-1, 16, 666]               0\n",
      "            Conv1d-6              [-1, 64, 332]             832\n",
      "         Transpose-7              [-1, 332, 64]               0\n",
      "         LayerNorm-8              [-1, 332, 64]             128\n",
      "              GELU-9              [-1, 332, 64]               0\n",
      "        Transpose-10              [-1, 64, 332]               0\n",
      "           Conv1d-11             [-1, 128, 166]           4,224\n",
      "        Transpose-12             [-1, 166, 128]               0\n",
      "        LayerNorm-13             [-1, 166, 128]             256\n",
      "             GELU-14             [-1, 166, 128]               0\n",
      "        Transpose-15             [-1, 128, 166]               0\n",
      "           Conv1d-16              [-1, 256, 83]          16,640\n",
      "        Transpose-17              [-1, 83, 256]               0\n",
      "        LayerNorm-18              [-1, 83, 256]             512\n",
      "             GELU-19              [-1, 83, 256]               0\n",
      "        Transpose-20              [-1, 256, 83]               0\n",
      "AdaptiveAvgPool1d-21              [-1, 256, 72]               0\n",
      "================================================================\n",
      "Total params: 22,688\n",
      "Trainable params: 22,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 2.98\n",
      "Params size (MB): 0.09\n",
      "Estimated Total Size (MB): 3.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "model = MultivarWav2Vec2(MODEL_CONFIG)\n",
    "summary(model.cuda(), (4, 2000))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T20:48:14.407185Z",
     "end_time": "2024-02-15T20:48:14.995868Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import math\n",
    "\n",
    "import lightning as L\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from lightly.loss import SwaVLoss\n",
    "from lightly.models import utils\n",
    "from typing import Optional, Sequence, Tuple, Union\n",
    "from model_config import MODEL_CONFIG\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "class MemoryBankModule(Module):\n",
    "    \"\"\"Memory bank implementation\n",
    "\n",
    "    This is a parent class to all loss functions implemented by the lightly\n",
    "    Python package. This way, any loss can be used with a memory bank if\n",
    "    desired.\n",
    "\n",
    "    Attributes:\n",
    "        size:\n",
    "            Size of the memory bank as (num_features, dim) tuple. If num_features is 0\n",
    "            then the memory bank is disabled. Deprecated: If only a single integer is\n",
    "            passed, it is interpreted as the number of features and the feature\n",
    "            dimension is inferred from the first batch stored in the memory bank.\n",
    "            Leaving out the feature dimension might lead to errors in distributed\n",
    "            training.\n",
    "        gather_distributed:\n",
    "            If True then negatives from all gpus are gathered before the memory bank\n",
    "            is updated. This results in more frequent updates of the memory bank and\n",
    "            keeps the memory bank contents independent of the number of gpus. But it has\n",
    "            the drawback that synchronization between processes is required and\n",
    "            diversity of the memory bank content is reduced.\n",
    "        feature_dim_first:\n",
    "            If True, the memory bank returns features with shape (dim, num_features).\n",
    "            If False, the memory bank returns features with shape (num_features, dim).\n",
    "\n",
    "    Examples:\n",
    "        >>> class MyLossFunction(MemoryBankModule):\n",
    "        >>>\n",
    "        >>>     def __init__(self, memory_bank_size: Tuple[int, int] = (2 ** 16, 128)):\n",
    "        >>>         super().__init__(memory_bank_size)\n",
    "        >>>\n",
    "        >>>     def forward(self, output: Tensor, labels: Optional[Tensor] = None):\n",
    "        >>>         output, negatives = super().forward(output)\n",
    "        >>>\n",
    "        >>>         if negatives is not None:\n",
    "        >>>             # evaluate loss with negative samples\n",
    "        >>>         else:\n",
    "        >>>             # evaluate loss without negative samples\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            size: Union[int, Sequence[int]] = 65536,\n",
    "            gather_distributed: bool = False,\n",
    "            feature_dim_first: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        size_tuple = (size,) if isinstance(size, int) else tuple(size)\n",
    "\n",
    "        if any(x < 0 for x in size_tuple):\n",
    "            raise ValueError(\n",
    "                f\"Illegal memory bank size {size}, all entries must be non-negative.\"\n",
    "            )\n",
    "\n",
    "        self.size = size_tuple\n",
    "        self.gather_distributed = gather_distributed\n",
    "        self.feature_dim_first = feature_dim_first\n",
    "        self.bank: Tensor\n",
    "        self.register_buffer(\n",
    "            \"bank\",\n",
    "            tensor=torch.empty(size=size_tuple, dtype=torch.float),\n",
    "            persistent=False,\n",
    "        )\n",
    "        self.bank_ptr: Tensor\n",
    "        self.register_buffer(\n",
    "            \"bank_ptr\",\n",
    "            tensor=torch.empty(1, dtype=torch.long),\n",
    "            persistent=False,\n",
    "        )\n",
    "\n",
    "        if isinstance(size, int) and size > 0:\n",
    "            warnings.warn(\n",
    "                (\n",
    "                    f\"Memory bank size 'size={size}' does not specify feature \"\n",
    "                    \"dimension. It is recommended to set the feature dimension with \"\n",
    "                    \"'size=(n, dim)' when creating the memory bank. Distributed \"\n",
    "                    \"training might fail if the feature dimension is not set.\"\n",
    "                ),\n",
    "                UserWarning,\n",
    "            )\n",
    "        elif len(size_tuple) > 1:\n",
    "            self._init_memory_bank(size=size_tuple)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _init_memory_bank(self, size: Tuple[int, ...]) -> None:\n",
    "        \"\"\"Initialize the memory bank.\n",
    "\n",
    "        Args:\n",
    "            size:\n",
    "                Size of the memory bank as (num_features, dim) tuple.\n",
    "\n",
    "        \"\"\"\n",
    "        self.bank = torch.randn(size).type_as(self.bank)\n",
    "        self.bank = torch.nn.functional.normalize(self.bank, dim=-1)\n",
    "        self.bank_ptr = torch.zeros(1).type_as(self.bank_ptr)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, batch: Tensor) -> None:\n",
    "        \"\"\"Dequeue the oldest batch and add the latest one\n",
    "\n",
    "        Args:\n",
    "            batch:\n",
    "                The latest batch of keys to add to the memory bank.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.gather_distributed:\n",
    "            batch = utils.concat_all_gather(batch)\n",
    "\n",
    "        batch_size = batch.shape[0]\n",
    "        ptr = int(self.bank_ptr)\n",
    "        if ptr + batch_size >= self.size[0]:\n",
    "            self.bank[ptr:] = batch[: self.size[0] - ptr].detach()\n",
    "            self.bank_ptr.zero_()\n",
    "        else:\n",
    "            self.bank[ptr: ptr + batch_size] = batch.detach()\n",
    "            self.bank_ptr[0] = ptr + batch_size\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            output: Tensor,\n",
    "            labels: Optional[Tensor] = None,\n",
    "            update: bool = False,\n",
    "    ) -> Tuple[Tensor, Union[Tensor, None]]:\n",
    "        \"\"\"Query memory bank for additional negative samples\n",
    "\n",
    "        Args:\n",
    "            output:\n",
    "                The output of the model.\n",
    "            labels:\n",
    "                Should always be None, will be ignored.\n",
    "            update:\n",
    "                If True, the memory bank will be updated with the current output.\n",
    "\n",
    "        Returns:\n",
    "            The output if the memory bank is of size 0, otherwise the output\n",
    "            and the entries from the memory bank. Entries from the memory bank have\n",
    "            shape (dim, num_features) if feature_dim_first is True and\n",
    "            (num_features, dim) otherwise.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # no memory bank, return the output\n",
    "        if self.size[0] == 0:\n",
    "            return output, None\n",
    "\n",
    "        # Initialize the memory bank if it is not already done.\n",
    "        if self.bank.ndim == 1:\n",
    "            dim = output.shape[1:]\n",
    "            self._init_memory_bank(size=(*self.size, *dim))\n",
    "\n",
    "        # query and update memory bank\n",
    "        bank = self.bank.clone().detach()\n",
    "        if self.feature_dim_first:\n",
    "            # swap bank size and feature dimension for backwards compatibility\n",
    "            bank = bank.transpose(0, -1)\n",
    "\n",
    "        # only update memory bank if we later do backward pass (gradient)\n",
    "        if update:\n",
    "            self._dequeue_and_enqueue(output)\n",
    "\n",
    "        return output, bank"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T20:44:20.019604Z",
     "end_time": "2024-02-15T20:44:20.356148Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class WrapperWithQueue(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = SwaVLoss()\n",
    "\n",
    "        self.start_queue_at_epoch = 50\n",
    "        self.queues = nn.ModuleList([MemoryBankModule(size=(3084, 128)) for _ in range(2)])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # normalize prototype so it is on a sphere\n",
    "        self.model.prototypes.normalize()\n",
    "        signals, idx = batch\n",
    "\n",
    "        high_resolution, low_resolution = signals[:2], signals[2:]\n",
    "\n",
    "        high_resolution_features = [self.model(x.float().to(self.device)) for x in high_resolution]\n",
    "        low_resolution_features = [self.model(x.float().to(self.device)) for x in low_resolution]\n",
    "\n",
    "        high_resolution_prototypes = [self.model.prototypes(x, self.current_epoch) for x in high_resolution_features]\n",
    "        low_resolution_prototypes = [self.model.prototypes(x, self.current_epoch) for x in low_resolution_features]\n",
    "\n",
    "        queue_prototypes = self._get_queue_prototypes(high_resolution_features)\n",
    "\n",
    "        loss = self.criterion(high_resolution_prototypes, low_resolution_prototypes, queue_prototypes)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_queue_prototypes(self, high_resolution_features):\n",
    "        if len(high_resolution_features) != len(self.queues):\n",
    "            raise ValueError(\n",
    "                f\"The number of queues ({len(self.queues)}) should be equal to the number of high \"\n",
    "                f\"resolution inputs ({len(high_resolution_features)}). Set `n_queues` accordingly.\"\n",
    "            )\n",
    "\n",
    "        # Get the queue features\n",
    "        queue_features = []\n",
    "        for i in range(len(self.queues)):\n",
    "            _, features = self.queues[i](high_resolution_features[i], update=True)\n",
    "            # Queue features are in (num_ftrs X queue_length) shape, while the high res\n",
    "            # features are in (batch_size X num_ftrs). Swap the axes for interoperability.\n",
    "            features = torch.permute(features, (1, 0))\n",
    "            queue_features.append(features)\n",
    "\n",
    "        # If loss calculation with queue prototypes starts at a later epoch,\n",
    "        # just queue the features and return None instead of queue prototypes.\n",
    "        if (\n",
    "                self.start_queue_at_epoch > 0\n",
    "                and self.current_epoch < self.start_queue_at_epoch\n",
    "        ):\n",
    "            return None\n",
    "\n",
    "        # Assign prototypes\n",
    "        queue_prototypes = [\n",
    "            self.model.prototypes(x, self.current_epoch) for x in queue_features\n",
    "        ]\n",
    "        return queue_prototypes\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=MODEL_CONFIG.learning_rate)\n",
    "\n",
    "        return optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-15T20:44:20.359148Z",
     "end_time": "2024-02-15T20:44:20.700420Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T20:44:20.702422Z",
     "end_time": "2024-02-15T20:44:21.304868Z"
    }
   },
   "outputs": [],
   "source": [
    "unlabeled_dataset = RNSDataset(sliced_data)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    unlabeled_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T20:44:21.300876Z",
     "end_time": "2024-02-15T20:45:20.848147Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\fabric\\connector.py:565: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:630: Checkpoint directory C:\\Users\\Patrick Xu\\Desktop\\RNS_Annotation-Pipeline\\scripts\\RNS_LITT_ANNOTATION_PIPELINE\\rns_scripts\\checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | MultivarWav2Vec2 | 14.4 M\n",
      "1 | criterion | SwaVLoss         | 0     \n",
      "2 | queues    | ModuleList       | 0     \n",
      "-----------------------------------------------\n",
      "14.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.4 M    Total params\n",
      "57.427    Total estimated model params size (MB)\n",
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b23814229a74304bc23effe4d92519c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "model_nn = MultivarWav2Vec2(MODEL_CONFIG)\n",
    "model = WrapperWithQueue(model_nn)\n",
    "\n",
    "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "checkpoint_callback = pl_callbacks.ModelCheckpoint(monitor='train_loss',\n",
    "                                                   filename='model_epoch-{epoch:02d}-{train_loss:.5f}',\n",
    "                                                   save_top_k=-1,\n",
    "                                                   every_n_epochs=5,\n",
    "                                                   # enable_version_counter=True,\n",
    "                                                   dirpath= 'checkpoints')\n",
    "\n",
    "early_stopping = pl_callbacks.EarlyStopping(monitor=\"train_loss\",\n",
    "                                            mode=\"min\",\n",
    "                                            patience=15)\n",
    "csv_logger = pl_loggers.CSVLogger('logs',\n",
    "                                  name=\"log\")\n",
    "\n",
    "trainer = L.Trainer(log_every_n_steps=5,\n",
    "                    logger=csv_logger,\n",
    "                    max_epochs=500,\n",
    "                    callbacks=[checkpoint_callback,early_stopping],\n",
    "                    accelerator='gpu',\n",
    "                    devices=1,\n",
    "                    precision = 16)\n",
    "trainer.fit(model=model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-15T20:45:20.849146Z",
     "end_time": "2024-02-15T20:45:20.861344Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Train_SwAV_10_epochs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
