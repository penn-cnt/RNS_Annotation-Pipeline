{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-27T21:29:06.970362Z",
     "start_time": "2024-04-27T21:29:06.845364Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-27T21:29:06.971363Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "sys.path.append('../tools')\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import pytorch_lightning.callbacks as pl_callbacks\n",
    "import data_utility, annotation_utility\n",
    "from models.rns_dataloader import *\n",
    "from active_learning_utility import get_strategy\n",
    "from active_learning_data import Data\n",
    "from active_learning_net import Net\n",
    "import interrater_annotations.tools\n",
    "from copy import deepcopy\n",
    "from models.SwaV import SwaV\n",
    "from models.LSTMDownStream import SupervisedDownstream\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    # True ensures the algorithm selected by CUFA is deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # torch.set_deterministic(True)\n",
    "    # False ensures CUDA select the same algorithm each time the application is run\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import pytorch_lightning\n",
    "\n",
    "pytorch_lightning.utilities.seed.seed_everything(seed=random_seed, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../../user_data/\"\n",
    "log_folder_root = '../../../user_data/logs/'\n",
    "ckpt_folder_root = '../../../user_data/checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_annotations = pd.read_csv('interrater_annotations/full_updated_anns (1).csv')\n",
    "ids = list(raw_annotations['HUP_ID'].unique())\n",
    "ids[:-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from interrater_annotations.tools import data_utility\n",
    "data_import = data_utility.read_files(path=data_dir + 'rns_data', patientIDs=ids,\n",
    "                                      verbose=True)  # Import data with annotation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from interrater_annotations.tools import annotation_utility\n",
    "annotations = annotation_utility.read_annotation(annotation_path = 'interrater_annotations/all_updated_anns.csv', annotation_catalog_path='interrater_annotations/test_datasets.csv', data = data_import)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "annotations.annotation_dict['RNS_Test_Dataset_ErinConrad']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = os.listdir(data_dir+'rns_pred_cache')\n",
    "# print(data_list)\n",
    "# data_list = ['HUP047.npy', 'HUP084.npy', 'HUP096.npy', 'HUP109.npy', 'HUP121.npy', 'HUP129.npy', 'HUP131.npy',\n",
    "#              'HUP137.npy', 'HUP147.npy', 'HUP156.npy', 'HUP159.npy', 'HUP182.npy', 'HUP197.npy', 'HUP199.npy',\n",
    "#              'RNS026.npy', 'RNS029.npy']\n",
    "# data_list = os.listdir(data_dir+'rns_test_cache')[1:]\n",
    "data_list = ['HUP096.npy', 'HUP137.npy','HUP101.npy']\n",
    "# data_list = ['HUP182.npy',   'HUP129.npy',   'HUP109.npy', 'HUP156.npy', 'HUP096.npy', 'RNS026.npy',  'HUP159.npy']\n",
    "# data_list = ['RNS026.npy', 'HUP159.npy', 'HUP129.npy', 'HUP096.npy', 'HUP182.npy']\n",
    "train_data, train_label, test_data, test_label, train_index, test_index = get_data_by_episode(data_list, file_path = 'rns_pred_cache', split=1)\n",
    "# train_data, train_label, test_data, test_label, train_index, test_index = get_data_by_episode(data_list, split=0.8)\n",
    "# data, label,_,_ = get_data(data_list, split=1)\n",
    "# train_data, test_data, train_label, test_label = sklearn.model_selection.train_test_split(data, label, test_size=0.8, random_state=42)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " with open(log_folder_root + 'rns_active_selected/' + 'EntropySampling' + '/' + 'selected_indices.pkl', 'rb') as f:\n",
    "    # Load the content of the file into a Python object\n",
    "    selected_inds = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "selected_ind_list = []\n",
    "for items in selected_inds.items():\n",
    "    selected_ind_list.append(np.array(items[1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "selected_ind_list\n",
    "\n",
    "np.concatenate(selected_ind_list[:-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.concatenate(train_index)[np.concatenate(selected_ind_list[:-1])][np.concatenate(train_index)[np.concatenate(selected_ind_list[:-1])]['patient_index'] == b'HUP101']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.concatenate(train_index)[np.concatenate(selected_ind_list[:-1])]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_list = np.array([ti[0] for ti in train_index])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matching_array = [None]*38\n",
    "i = 0\n",
    "for index, row in annotations.annotation_dict['RNS_Test_Dataset_BrianLitt'].iterrows():\n",
    "\n",
    "    pt = row['Patient_ID'].encode('utf-8')\n",
    "    si = row['Episode_Start_Index']\n",
    "    filtered_1 = train_list[train_list['patient_index'] == pt]\n",
    "    filtered_2 = filtered_1[filtered_1['start_index'] == si]\n",
    "    matching_array[i] = np.where(train_list == filtered_2)[0]\n",
    "    i+= 1\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "annot = annotations.annotation_dict['RNS_Test_Dataset_ErinConrad']\n",
    "annot_nonseizure = annot[annot['Class_Code'] == 0]\n",
    "annot_seizure = annot[annot['Class_Code'] == 1]\n",
    "patient_list = ['HUP047',\n",
    "       'HUP059',\n",
    "       'HUP084',\n",
    "       'HUP096',\n",
    "       'HUP101',\n",
    "       'HUP108',\n",
    "       'HUP109',\n",
    "       'HUP121',\n",
    "       'HUP127',\n",
    "       'HUP128',\n",
    "       'HUP129',\n",
    "       'HUP131',\n",
    "       'HUP136',\n",
    "       'HUP137',\n",
    "       'HUP143',\n",
    "       'HUP147',\n",
    "       'HUP153',\n",
    "       'HUP156',\n",
    "       'HUP159',\n",
    "       'HUP182',\n",
    "       'HUP192',\n",
    "       'HUP197',\n",
    "       'HUP199',\n",
    "       'HUP205',\n",
    "       'RNS021',\n",
    "       'RNS022',\n",
    "       'RNS026',\n",
    "       'RNS029']\n",
    "\n",
    "# patient_list = [ 'HUP137',\n",
    "#        'HUP153',]\n",
    "\n",
    "# patient_list = ['RNS026', 'HUP159', 'HUP129', 'HUP096', 'HUP182']\n",
    "clip_dict = {}\n",
    "# for p in patient_list:\n",
    "for p in patient_list:\n",
    "    # print(p)\n",
    "    seizure_start_index = np.array([])\n",
    "    seizure_end_index = np.array([])\n",
    "    nonseizure_start_index = np.array([])\n",
    "    nonseizure_end_index = np.array([])\n",
    "    global_episode_index_seizure = np.array([])\n",
    "    global_episode_index_nonseizure = np.array([])\n",
    "\n",
    "    annotation_list = []\n",
    "\n",
    "    start_index = annot[annot['Patient_ID'] == p]['Episode_Start_Index']\n",
    "    end_index = annot[annot['Patient_ID'] == p]['Episode_End_Index']\n",
    "    annot_start_list = annot[annot['Patient_ID'] == p]['Annotation_Start_Index']\n",
    "    annot_end_list = annot[annot['Patient_ID'] == p]['Annotation_End_Index']\n",
    "    j = 0\n",
    "    for i in range(len(start_index)):\n",
    "        if end_index.iloc[i] - start_index.iloc[i] > 0:\n",
    "            initial_arr = np.zeros(end_index.iloc[i] - start_index.iloc[i])\n",
    "            if len(annot_start_list.iloc[i]) > 0:\n",
    "                sl_order = np.argsort(annot_start_list.iloc[i])\n",
    "                sl = np.array(annot_start_list.iloc[i])[sl_order]\n",
    "                el = np.array(annot_end_list.iloc[i])[sl_order]\n",
    "\n",
    "                for si, ei in zip(sl, el):\n",
    "                    initial_arr[si - start_index.iloc[i]:ei - start_index.iloc[i]] = 1\n",
    "        else:\n",
    "            # print(i)\n",
    "            initial_arr = np.zeros(1)\n",
    "\n",
    "        annotation_list.append(initial_arr)\n",
    "\n",
    "    ind_arr = np.vstack(\n",
    "        [start_index,\n",
    "         end_index,\n",
    "         start_index.index]).astype(int)\n",
    "\n",
    "    # print(annotation_list)\n",
    "\n",
    "    valid = np.where((ind_arr[1] - ind_arr[0]) > 500)\n",
    "    combined_clip = ind_arr[:, valid].squeeze()\n",
    "    annotation_list = np.array(annotation_list, dtype=object)[valid]\n",
    "    try:\n",
    "        combined_clip = np.vstack((combined_clip, annotation_list))\n",
    "    except:\n",
    "        print(annotation_list)\n",
    "        print(p)\n",
    "\n",
    "    if combined_clip.shape[1]>0:\n",
    "\n",
    "        clip_dict[p] = combined_clip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "window_len = 1\n",
    "stride = 1\n",
    "concat_n = 4\n",
    "for id in clip_dict.keys():\n",
    "    data_import[id].set_window_parameter(window_length=window_len, window_displacement=stride)\n",
    "    data_import[id].set_concatenation_parameter(concatenate_window_n=concat_n)\n",
    "    window_indices, _ = data_import[id].get_windowed_data(clip_dict[id][0], clip_dict[id][1])\n",
    "    import_indices = []\n",
    "    import_label = []\n",
    "    import_clip_indices = []\n",
    "    import_start_indicies = []\n",
    "    import_patient_ID = []\n",
    "    for i, ind in enumerate(window_indices):\n",
    "        indices = window_indices[i]+1-clip_dict[id][0][i]\n",
    "        offsets = np.arange(249)\n",
    "        full_indices = indices[:,0][:, np.newaxis] + offsets\n",
    "        slices_no_loop = clip_dict[id][3][i][full_indices]\n",
    "        mode_result = mode(slices_no_loop, axis=1)\n",
    "        mode_values = mode_result.mode\n",
    "\n",
    "        # print(mode_values)\n",
    "        import_label.append(mode_values)\n",
    "        import_indices.append(np.repeat(clip_dict[id][2][i], len(ind)))\n",
    "        import_clip_indices.append(np.arange(len(ind)))\n",
    "        import_start_indicies.append(np.repeat(clip_dict[id][0][i], len(ind)))\n",
    "        import_patient_ID.append(np.repeat(id, len(ind)))\n",
    "\n",
    "    import_label = np.hstack(import_label)\n",
    "    import_indices = np.hstack(import_indices)\n",
    "    import_clip_indices = np.hstack(import_clip_indices)\n",
    "    import_start_indicies = np.hstack(import_start_indicies)\n",
    "    import_patient_ID = np.hstack(import_patient_ID)\n",
    "\n",
    "    data_import[id].normalize_windowed_data()\n",
    "    _, concatenated_data = data_import[id].get_concatenated_data(data_import[id].windowed_data, arrange='channel_stack')\n",
    "\n",
    "    assert np.hstack(import_label).shape[0] == concatenated_data.shape[0]\n",
    "\n",
    "    np.save(data_dir+'rns_pred_cache/' + id + '.npy', {'data': concatenated_data, 'label': import_label, 'patientID': import_patient_ID, 'indices': np.vstack([import_indices,import_clip_indices,import_start_indicies]).T})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# selected_episode = np.array(matching_array[:25])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate(train_data[selected_episode])\n",
    "y_train = np.concatenate(train_label[selected_episode])\n",
    "# X_test = np.concatenate(test_data)\n",
    "# y_test = np.concatenate(test_label)\n",
    "index_train = np.concatenate(train_index[selected_episode])\n",
    "# index_test = np.concatenate(test_index)\n",
    "seq_len_train = np.array([y.shape[0] for y in train_label[selected_episode]])\n",
    "# seq_len_test = np.array([y.shape[0] for y in test_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = np.concatenate(train_data)\n",
    "y_train = np.concatenate(train_label)\n",
    "# X_test = np.concatenate(test_data)\n",
    "# y_test = np.concatenate(test_label)\n",
    "index_train = np.concatenate(train_index)\n",
    "# index_test = np.concatenate(test_index)\n",
    "seq_len_train = np.array([y.shape[0] for y in train_label])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "args_task = {'n_epoch': 40,\n",
    "             'transform_train': True,\n",
    "             'strategy_name': 'EntropySampling',\n",
    "             'transform': False,\n",
    "             'loader_tr_args': {'batch_size': 2, 'num_workers': 4, 'collate_fn': collate_fn,\n",
    "                                'drop_last': True, 'persistent_workers': True},\n",
    "             'loader_te_args': {'batch_size': 2, 'num_workers': 4, 'collate_fn': collate_fn,\n",
    "                                'drop_last': True, 'persistent_workers': True}\n",
    "             }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swav = SwaV().load_from_checkpoint(\n",
    "    ckpt_folder_root + 'rns_swav_50_12/rns_swav-epoch=82-swav_loss=2.58204.ckpt')\n",
    "model = SupervisedDownstream(swav.backbone)\n",
    "# initialize model and save the model state\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer( accelerator='gpu', devices=1,precision=16)\n",
    "from models.rns_dataloader import RNS_Active_by_episode_LSTM, collate_fn\n",
    "train_dataset = RNS_Active_by_episode_LSTM(train_data, train_label, transform=False, astensor=True)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "strategy_name = 'RandomSampling'\n",
    "with open(log_folder_root + 'rns_active_selected/' + strategy_name + '/' + 'selected_indices.pkl', 'rb') as f:\n",
    "    # Load the content of the file into a Python object\n",
    "        selected_inds = pickle.load(f)\n",
    "\n",
    "selected_inds[1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for rd in range(1, NUM_ROUND + 1):\n",
    "    print('round ' + str(rd))\n",
    "    log_file_name = log_folder_root + 'rns_active/active_logs_' + strategy_name + '/logger_round_' + str(\n",
    "        rd - 1) + '/version_0/metrics.csv'\n",
    "    logs = pd.read_csv(log_file_name)\n",
    "    max_ind = logs['val_acc'].argmax()\n",
    "    max_row = logs.iloc[max_ind]\n",
    "    ckpt_directory = ckpt_folder_root + 'rns_active/active_checkpoints_' + strategy_name\n",
    "    ckpt_files = os.listdir(ckpt_directory)\n",
    "    load_file_name = strategy_name + '_round_' + str(rd - 1) + '-step=' + str(int(max_row['step']+1))\n",
    "    print(load_file_name)\n",
    "\n",
    "    ind = next((i for i, s in enumerate(ckpt_files) if load_file_name in s), None)\n",
    "    print(ind, ckpt_files[ind])\n",
    "    strategy.net.net.load_from_checkpoint(ckpt_directory + '/' + ckpt_files[ind], backbone=swav.backbone)\n",
    "\n",
    "    q_idxs = strategy.query(NUM_QUERY * 90)\n",
    "\n",
    "    with open(log_folder_root + 'rns_active_selected/' + strategy_name + '/' + 'selected_indices.pkl', 'rb') as f:\n",
    "    # Load the content of the file into a Python object\n",
    "        selected_inds = pickle.load(f)\n",
    "    selected_inds[rd] = q_idxs\n",
    "    with open(log_folder_root + 'rns_active_selected/' + strategy_name + '/' + 'selected_indices.pkl', 'wb') as f:\n",
    "        pickle.dump(selected_inds, f)\n",
    "# Now you can use the dictionary object as usual\n",
    "    strategy.update(q_idxs)\n",
    "    strategy.net.round = rd\n",
    "    strategy.net.net.load_state_dict(modelstate)\n",
    "    torch.cuda.empty_cache()\n",
    "    strategy.train()\n",
    "    torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "strategy_name = 'MarginSamplingDropout'\n",
    "ckpt_directory = ckpt_folder_root + 'rns_active/active_checkpoints_' + strategy_name\n",
    "# ckpt_files = os.listdir(ckpt_directory)\n",
    "ckpt_files = sorted(Path(ckpt_directory).iterdir(), key=os.path.getmtime)[50:]\n",
    "high_f1 = 0\n",
    "high_class_f1 = 0\n",
    "file_name = []\n",
    "file_name_class_f1 = []\n",
    "for cf in tqdm(ckpt_files):\n",
    "    model = model.load_from_checkpoint(ckpt_folder_root + 'rns_active/active_checkpoints_' + strategy_name + '/' + cf.name, backbone=swav.backbone)\n",
    "    predictions = trainer.predict(model,train_dataloader)\n",
    "    output_list = []\n",
    "    target_list = []\n",
    "    emb_list = []\n",
    "    m = nn.Softmax(dim=1)\n",
    "    seq_len_list = []\n",
    "    for pred, y, emb, emb2, seq_len in predictions:\n",
    "        output_list.append(pred)\n",
    "        target_list.append(y)\n",
    "        emb_list.append(emb)\n",
    "        seq_len_list.append(seq_len)\n",
    "    pred_raw = torch.vstack(output_list)\n",
    "    target = torch.concat(target_list)\n",
    "    emb = torch.concat(emb_list)\n",
    "    out = torch.argmax(pred_raw, dim=1)\n",
    "    seq_len_arr = torch.tensor([item for sublist in seq_len_list for item in sublist])\n",
    "    pred_episode = combine_window_to_episode(torch.argmax(pred_raw, dim=1),seq_len_arr)\n",
    "    class_f1 = sklearn.metrics.f1_score([np.sign(tl.sum()) for tl in pred_episode], [np.sign(tl.sum()) for tl in train_label])\n",
    "    f1 = sklearn.metrics.f1_score(torch.argmax(pred_raw, dim=1), target)\n",
    "    if f1>high_f1:\n",
    "        high_f1 = f1\n",
    "        file_name.append(cf)\n",
    "        print('high_f1', f1,cf)\n",
    "    if class_f1>=high_class_f1:\n",
    "        high_class_f1 = class_f1\n",
    "        file_name_class_f1.append((class_f1, cf))\n",
    "        print('high_class_f1', class_f1,cf)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " file_name_class_f1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = model.load_from_checkpoint(ckpt_folder_root + 'rns_active/active_checkpoints_' + strategy_name + '/' + 'MarginSamplingDropout_round_5-step=750-train_loss=0.05351.ckpt', backbone=swav.backbone)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = trainer.predict(model,train_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "output_list = []\n",
    "target_list = []\n",
    "emb_list = []\n",
    "m = nn.Softmax(dim=1)\n",
    "seq_len_list = []\n",
    "for pred, y, emb, emb2, seq_len in predictions:\n",
    "    output_list.append(pred)\n",
    "    target_list.append(y)\n",
    "    emb_list.append(emb)\n",
    "    seq_len_list.append(seq_len)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_raw = torch.vstack(output_list)\n",
    "target = torch.concat(target_list)\n",
    "emb = torch.concat(emb_list)\n",
    "out = torch.argmax(pred_raw, dim=1)\n",
    "seq_len_arr = torch.tensor([item for sublist in seq_len_list for item in sublist])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sklearn\n",
    "clf_report = sklearn.metrics.classification_report(torch.argmax(pred_raw, dim=1), target, digits=6)\n",
    "\n",
    "print(f\"Classification Report : \\n{clf_report}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_episode = combine_window_to_episode(torch.argmax(pred_raw, dim=1),seq_len_arr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(target)\n",
    "plt.plot(torch.argmax(pred_raw, dim=1))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 31\n",
    "plt.figure()\n",
    "plt.plot(train_data[i][:,:,4].flatten()+2,color = 'k')\n",
    "plt.plot(train_data[i][:,:,13].flatten()+1,color = 'k')\n",
    "plt.plot(train_data[i][:,:,22].flatten(),color = 'k')\n",
    "plt.plot(train_data[i][:,:,31].flatten()-1,color = 'k')\n",
    "label_start, label_end, pred_start, pred_end = plot_high_light(train_label[i],pred_episode[i])\n",
    "if len(pred_start)>0:\n",
    "    for i in range(len(pred_start)):\n",
    "        plt.axvspan(pred_start[i]*249, pred_end[i]*249, color=\"blue\", alpha=0.3)\n",
    "if len(label_start)>0:\n",
    "    for i in range(len(label_start)):\n",
    "        plt.axvspan(label_start[i]*249, label_end[i]*249, color=\"yellow\", alpha=0.3)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_start"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_high_light(train_label, pred_episode):\n",
    "    label_start = np.where(np.diff(train_label) == 1)[0]\n",
    "    label_end = np.where(np.diff(train_label) == -1)[0]\n",
    "    pred_start = np.where(np.diff(pred_episode) == 1)[0]\n",
    "    pred_end = np.where(np.diff(pred_episode) == -1)[0]\n",
    "    label_start, label_end = check_consistent(label_start,label_end, len(train_label))\n",
    "    pred_start, pred_end = check_consistent(pred_start,pred_end, len(pred_episode))\n",
    "    # if len(label_start)>0:\n",
    "    #     plt.axvspan(label_start[0]*249, label_end[0]*249, color=\"yellow\", alpha=0.3)\n",
    "\n",
    "    return label_start, label_end, pred_start, pred_end\n",
    "\n",
    "\n",
    "def check_consistent(start, end, total_len):\n",
    "    if len(start) != len(end):\n",
    "        if len(start)>0:\n",
    "            end = [total_len]\n",
    "        elif len(end)>0:\n",
    "            start = [0]\n",
    "    return start, end\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 31\n",
    "plot_high_light(train_label[i],pred_episode[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data[i]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.tensor([item for sublist in seq_len_list for item in sublist])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def combine_window_to_episode(data, seq_len, index=None):\n",
    "    cum_sum_index = np.cumsum(seq_len)\n",
    "    cum_sum_index = np.insert(cum_sum_index, 0, 0)\n",
    "\n",
    "    assert len(data) == cum_sum_index[-1]\n",
    "\n",
    "    data_out = [None] * (len(cum_sum_index) - 1)\n",
    "\n",
    "    for i in range(1, len(cum_sum_index)):\n",
    "        start_index, end_index = cum_sum_index[i - 1], cum_sum_index[i]\n",
    "        episode_data = data[start_index:end_index]\n",
    "\n",
    "        if index is None:\n",
    "            out = episode_data\n",
    "        else:\n",
    "            episode_labeled = index[start_index:end_index]\n",
    "            out = episode_data[episode_labeled]\n",
    "\n",
    "        if len(out) > 0:\n",
    "            data_out[i - 1] = out\n",
    "\n",
    "    data_out = [segment for segment in data_out if segment is not None]\n",
    "\n",
    "    return np.array(data_out, dtype=object)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[np.sign(tl.sum()) for tl in train_label]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[np.sign(tl.sum()) for tl in pred_episode]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_report = sklearn.metrics.classification_report([np.sign(tl.sum()) for tl in pred_episode], [np.sign(tl.sum()) for tl in train_label], digits=6)\n",
    "\n",
    "print(f\"Classification Report : \\n{clf_report}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sklearn.metrics.f1_score([np.sign(tl.sum()) for tl in pred_episode], [np.sign(tl.sum()) for tl in train_label])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.where(np.diff(train_label[i]) == -1)[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "machine_annot = annotations.annotation_dict['RNS_Test_Dataset_ErinConrad'].copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_start_stop(pred_episode):\n",
    "    pred_start = np.where(np.diff(pred_episode) == 1)[0]\n",
    "    pred_end = np.where(np.diff(pred_episode) == -1)[0]\n",
    "    pred_start, pred_end = check_consistent(pred_start,pred_end, len(pred_episode))\n",
    "    pred_start *= 249\n",
    "    pred_end *= 249\n",
    "    return pred_start, pred_end\n",
    "\n",
    "\n",
    "def check_consistent(start, end, total_len):\n",
    "    if len(start) != len(end):\n",
    "        if len(start)>0:\n",
    "            end = np.array([total_len], dtype=np.int64)\n",
    "        elif len(end)>0:\n",
    "            start = np.array([0], dtype=np.int64)\n",
    "    return start, end"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def interpolate_time(index, start_index, end_index, start_timestamp, end_timestamp):\n",
    "    return int((index-start_index)/(end_index-start_index)*(end_timestamp-start_timestamp)+start_timestamp)\n",
    "\n",
    "def timestamp_to_utctime(ts):\n",
    "    \"\"\"\n",
    "    :param ts: int - datetime timestamp\n",
    "    :return: string - utc time\n",
    "    \"\"\"\n",
    "    return datetime.utcfromtimestamp(ts * 1e-6)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "machine_annot = machine_annot.reset_index(drop=True)\n",
    "machine_annot.Dataset = 'RNS_Test_Dataset_DeepLearning'\n",
    "machine_annot.Alias_ID = 'RNS_Example_DL'\n",
    "machine_annot = machine_annot.drop(['Type_Description', 'Annotation_Channel', 'Channel_Code', 'Binary_Channel_Code','Annotation_Catalog_Index'], axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "machine_annot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for index, row in machine_annot.iterrows():\n",
    "    start_ind, end_ind = get_start_stop(pred_episode[index])\n",
    "    machine_annot.at[index, 'Annotation_Start_Index'] = (row.Episode_Start_Index + start_ind).tolist()\n",
    "    machine_annot.at[index, 'Annotation_End_Index'] = (row.Episode_Start_Index + end_ind).tolist()\n",
    "    machine_annot.at[index, 'Class_Code'] = np.sign(pred_episode[index].sum()).item()\n",
    "    machine_annot.at[index, 'Annotation_Start_Timestamp'] = [\n",
    "        interpolate_time(st_ind, row.Episode_Start_Index, row.Episode_End_Index, row.Episode_Start_Timestamp,\n",
    "                         row.Episode_End_Timestamp) for st_ind in (row.Episode_Start_Index + start_ind).tolist()]\n",
    "    machine_annot.at[index, 'Annotation_End_Timestamp'] = [\n",
    "        interpolate_time(ed_ind, row.Episode_Start_Index, row.Episode_End_Index, row.Episode_Start_Timestamp,\n",
    "                         row.Episode_End_Timestamp) for ed_ind in (row.Episode_Start_Index + end_ind).tolist()]\n",
    "    machine_annot.at[index, 'Annotation_Start_UTC_Time'] = [\n",
    "        timestamp_to_utctime(interpolate_time(st_ind, row.Episode_Start_Index, row.Episode_End_Index, row.Episode_Start_Timestamp,\n",
    "                         row.Episode_End_Timestamp)) for st_ind in (row.Episode_Start_Index + start_ind).tolist()]\n",
    "    machine_annot.at[index, 'Annotation_End_UTC_Time'] = [\n",
    "        timestamp_to_utctime(interpolate_time(ed_ind, row.Episode_Start_Index, row.Episode_End_Index, row.Episode_Start_Timestamp,\n",
    "                         row.Episode_End_Timestamp)) for ed_ind in (row.Episode_Start_Index + end_ind).tolist()]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "1442176484602143-interpolate_time(11192047, 11181530, 11204050 ,1442176442532000, 1442176532616000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "annotations.annotation_dict['RNS_Test_Dataset_ErinConrad']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ind-start_ind/end_ind - start_ind = ind_ts-st_ts/endst-st_ts"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "machine_annot.to_csv('machineprediction.csv', index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
