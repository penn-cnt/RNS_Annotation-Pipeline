{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T14:11:37.719968Z",
     "end_time": "2024-02-21T14:11:37.807966Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# %matplotlib inline\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T14:11:37.795966Z",
     "end_time": "2024-02-21T14:11:37.878760Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../tools')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import math\n",
    "from torchsummary import summary\n",
    "import math\n",
    "import lightning as L\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from model_config import MODEL_CONFIG\n",
    "from lightly.models.modules import SwaVPrototypes, SwaVProjectionHead\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch import callbacks as pl_callbacks\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "\n",
    "matplotlib.use(\"nbAgg\")\n",
    "\n",
    "import data_utility\n",
    "\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T14:11:37.875761Z",
     "end_time": "2024-02-21T14:11:45.212532Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T14:11:45.212532Z",
     "end_time": "2024-02-21T14:11:45.562210Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "data_dir = \"../../../user_data/\"\n",
    "log_folder_root = '../../../user_data/logs/'\n",
    "ckpt_folder_root = '../../../user_data/checkpoints/'\n",
    "torch.set_float32_matmul_precision('medium')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T14:11:45.562210Z",
     "end_time": "2024-02-21T14:11:45.910904Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T14:11:45.910904Z",
     "end_time": "2024-02-21T14:11:46.270040Z"
    }
   },
   "outputs": [],
   "source": [
    "dir_list = os.listdir(data_dir + 'rns_data')\n",
    "patientIDs = [s for s in dir_list for type_string in ['HUP', 'RNS'] if type_string in s.upper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g05XnHgHas-D",
    "ExecuteTime": {
     "start_time": "2024-02-21T14:11:46.270040Z",
     "end_time": "2024-02-21T14:12:34.098444Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:47<00:00,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "data_import = data_utility.read_files(path=data_dir + 'rns_data', annotation_only=False, verbose=True)\n",
    "ids = list(data_import.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T14:12:34.098444Z",
     "end_time": "2024-02-21T14:13:28.551663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUP047\n",
      "HUP059\n",
      "HUP084\n",
      "HUP096\n",
      "HUP101\n",
      "bla\n",
      "HUP108\n",
      "bla\n",
      "HUP109\n",
      "bla\n",
      "HUP121\n",
      "HUP127\n",
      "HUP128\n",
      "bla\n",
      "HUP129\n",
      "bla\n",
      "HUP131\n",
      "HUP136\n",
      "bla\n",
      "HUP143\n",
      "HUP147\n",
      "bla\n",
      "bla\n",
      "HUP153\n",
      "HUP159\n",
      "HUP182\n",
      "bla\n",
      "HUP192\n",
      "bla\n",
      "HUP197\n",
      "HUP199\n",
      "bla\n",
      "HUP205\n",
      "RNS021\n",
      "bla\n",
      "bla\n",
      "RNS022\n",
      "RNS026\n",
      "bla\n",
      "RNS029\n",
      "bla\n",
      "bla\n",
      "bla\n"
     ]
    }
   ],
   "source": [
    "window_len = 9\n",
    "stride = 9\n",
    "concat_n = 1\n",
    "\n",
    "ids = ['HUP047',\n",
    "       'HUP059',\n",
    "       'HUP084',\n",
    "       'HUP096',\n",
    "       'HUP101',\n",
    "       'HUP108',\n",
    "       'HUP109',\n",
    "       'HUP121',\n",
    "       'HUP127',\n",
    "       'HUP128',\n",
    "       'HUP129',\n",
    "       'HUP131',\n",
    "       'HUP136',\n",
    "       # 'HUP137',\n",
    "       'HUP143',\n",
    "       'HUP147',\n",
    "       'HUP153',\n",
    "       # 'HUP156',\n",
    "       'HUP159',\n",
    "       'HUP182',\n",
    "       'HUP192',\n",
    "       'HUP197',\n",
    "       'HUP199',\n",
    "       'HUP205',\n",
    "       'RNS021',\n",
    "       'RNS022',\n",
    "       'RNS026',\n",
    "       'RNS029']\n",
    "\n",
    "data_list = []\n",
    "for id in ids:\n",
    "    print(id)\n",
    "    data_import[id].set_window_parameter(window_length=window_len, window_displacement=stride)\n",
    "    data_import[id].normalize_data()\n",
    "    _, sliced_data = data_import[id].get_windowed_data(data_import[id].catalog[\"Event Start idx\"],\n",
    "                                                       data_import[id].catalog[\"Event End idx\"])\n",
    "    data_list.append(sliced_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# next(iter(RNSDataset(sliced_data)))[0][1].size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T14:13:28.551663Z",
     "end_time": "2024-02-21T14:13:28.906371Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# next(iter(unlabeled_dataset))[0].size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T14:13:28.906371Z",
     "end_time": "2024-02-21T14:13:29.251363Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import os\n",
    "import random\n",
    "from lightly.loss import SwaVLoss\n",
    "from lightly.loss.memory_bank import MemoryBankModule\n",
    "from lightly.models.modules import SwaVProjectionHead, SwaVPrototypes\n",
    "from lightly.transforms.swav_transform import SwaVTransform\n",
    "from lightly.data import SwaVCollateFunction\n",
    "from model_config import MODEL_CONFIG\n",
    "\n",
    "\n",
    "class Transpose(nn.Module):\n",
    "    def __init__(self, dim1, dim2):\n",
    "        super(Transpose, self).__init__()\n",
    "        self.dim1 = dim1\n",
    "        self.dim2 = dim2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.transpose(self.dim1, self.dim2)\n",
    "\n",
    "\n",
    "class SwaV(L.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        resnet = torchvision.models.resnet50()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.projection_head = SwaVProjectionHead(2048, 2048, 128)\n",
    "        self.prototypes = SwaVPrototypes(128, 20)\n",
    "        self.start_queue_at_epoch = 3\n",
    "        self.queues = nn.ModuleList([MemoryBankModule(size=1024) for _ in range(2)])\n",
    "        self.criterion = SwaVLoss(sinkhorn_epsilon=0.05)\n",
    "\n",
    "        self.ft_enc = nn.ModuleList()\n",
    "        for i, _ in enumerate(config.ft_enc_dims):\n",
    "            if i == 0:\n",
    "                self.ft_enc.append(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=4,\n",
    "                        out_channels=config.ft_enc_dims[i],\n",
    "                        kernel_size=config.ft_enc_kernel_widths[i],\n",
    "                        stride=config.ft_enc_strides[i],\n",
    "                        padding=0,\n",
    "                        groups=config.channel_buffer_size,\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                self.ft_enc.append(\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=config.ft_enc_dims[i - 1],\n",
    "                        out_channels=config.ft_enc_dims[i],\n",
    "                        kernel_size=config.ft_enc_kernel_widths[i],\n",
    "                        stride=config.ft_enc_strides[i],\n",
    "                        padding=0,\n",
    "                        groups=config.channel_buffer_size,\n",
    "                    )\n",
    "                )\n",
    "            # transpose the output of the convolutional layer\n",
    "            self.ft_enc.append(Transpose(1, 2))\n",
    "            # layer normalization\n",
    "            self.ft_enc.append(nn.LayerNorm(config.ft_enc_dims[i]))\n",
    "            # GELU activation\n",
    "            self.ft_enc.append(nn.GELU())\n",
    "            # transpose the output of the convolutional layer\n",
    "            self.ft_enc.append(Transpose(1, 2))\n",
    "\n",
    "        # add a adaptive pool so different sized crops can be the same length afterward\n",
    "        self.ft_enc.append(nn.AdaptiveAvgPool1d(config.spatial_transformer_hidden))\n",
    "\n",
    "        # convert the list of modules to a sequential module\n",
    "        self.ft_enc = nn.Sequential(*self.ft_enc)\n",
    "\n",
    "        crop_transforms = []\n",
    "        crop_sizes = [224, 96]\n",
    "        crop_min_scales = [0.14, 0.05]\n",
    "        crop_max_scales = [1.0, 0.14]\n",
    "        crop_counts = [2, 6]\n",
    "        for i in range(len(crop_sizes)):\n",
    "            random_resized_crop = T.RandomResizedCrop(crop_sizes[i], scale=(crop_min_scales[i], crop_max_scales[i]))\n",
    "\n",
    "            crop_transforms.extend([T.Compose([random_resized_crop])] * crop_counts[i])\n",
    "\n",
    "        self.crop_transforms = crop_transforms\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[0].float()\n",
    "\n",
    "        x = self.ft_enc(x)\n",
    "\n",
    "        views = []\n",
    "        for tf in self.crop_transforms:\n",
    "            views.append(tf(x).unsqueeze(1).repeat(1, 3, 1, 1))\n",
    "\n",
    "        high_resolution, low_resolution = views[:2], views[2:]\n",
    "        self.prototypes.normalize()\n",
    "\n",
    "        high_resolution_features = [self._subforward(x) for x in high_resolution]\n",
    "        low_resolution_features = [self._subforward(x) for x in low_resolution]\n",
    "\n",
    "        high_resolution_prototypes = [\n",
    "            self.prototypes(x, self.current_epoch) for x in high_resolution_features\n",
    "        ]\n",
    "        low_resolution_prototypes = [\n",
    "            self.prototypes(x, self.current_epoch) for x in low_resolution_features\n",
    "        ]\n",
    "        queue_prototypes = self._get_queue_prototypes(high_resolution_features)\n",
    "        loss = self.criterion(\n",
    "            high_resolution_prototypes, low_resolution_prototypes, queue_prototypes\n",
    "        )\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optim\n",
    "\n",
    "    def _subforward(self, input):\n",
    "        features = self.backbone(input).flatten(start_dim=1)\n",
    "        features = self.projection_head(features)\n",
    "        features = nn.functional.normalize(features, dim=1, p=2)\n",
    "        return features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_queue_prototypes(self, high_resolution_features):\n",
    "        if len(high_resolution_features) != len(self.queues):\n",
    "            raise ValueError(\n",
    "                f\"The number of queues ({len(self.queues)}) should be equal to the number of high \"\n",
    "                f\"resolution inputs ({len(high_resolution_features)}). Set `n_queues` accordingly.\"\n",
    "            )\n",
    "\n",
    "        # Get the queue features\n",
    "        queue_features = []\n",
    "        for i in range(len(self.queues)):\n",
    "            _, features = self.queues[i](high_resolution_features[i], update=True)\n",
    "            # Queue features are in (num_ftrs X queue_length) shape, while the high res\n",
    "            # features are in (batch_size X num_ftrs). Swap the axes for interoperability.\n",
    "            features = torch.permute(features, (1, 0))\n",
    "            queue_features.append(features)\n",
    "\n",
    "        # If loss calculation with queue prototypes starts at a later epoch,\n",
    "        # just queue the features and return None instead of queue prototypes.\n",
    "        if (\n",
    "                self.start_queue_at_epoch > 0\n",
    "                and self.current_epoch < self.start_queue_at_epoch\n",
    "        ):\n",
    "            return None\n",
    "\n",
    "        # Assign prototypes\n",
    "        queue_prototypes = [\n",
    "            self.prototypes(x, self.current_epoch) for x in queue_features\n",
    "        ]\n",
    "        return queue_prototypes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T15:13:57.674679Z",
     "end_time": "2024-02-21T15:13:58.115685Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "del data_import"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T14:13:29.598019Z",
     "end_time": "2024-02-21T14:13:33.955012Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from models.rns_dataloader import RNSDataset\n",
    "\n",
    "unlabeled_dataset = RNSDataset(data_list, transform=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T14:13:33.955012Z",
     "end_time": "2024-02-21T14:14:20.582722Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# next(iter(unlabeled_dataset))[0].size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T14:14:20.602723Z",
     "end_time": "2024-02-21T14:14:21.053275Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T14:14:21.057275Z",
     "end_time": "2024-02-21T14:14:21.411557Z"
    }
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    unlabeled_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=10,\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# np.vstack(data_list).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T14:14:21.411557Z",
     "end_time": "2024-02-21T14:14:21.765156Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T15:14:00.690260Z",
     "end_time": "2024-02-21T15:14:01.452401Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = SwaV(MODEL_CONFIG)\n",
    "\n",
    "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "checkpoint_callback = pl_callbacks.ModelCheckpoint(monitor='train_loss',\n",
    "                                                   filename='model_epoch-{epoch:02d}-{train_loss:.5f}',\n",
    "                                                   save_top_k=-1,\n",
    "                                                   every_n_epochs=1,\n",
    "                                                   # enable_version_counter=True,\n",
    "                                                   dirpath=ckpt_folder_root + 'rns_swav_p20')\n",
    "\n",
    "early_stopping = pl_callbacks.EarlyStopping(monitor=\"train_loss\",\n",
    "                                            mode=\"min\",\n",
    "                                            patience=15)\n",
    "csv_logger = pl_loggers.CSVLogger(log_folder_root + 'rns_swav_p20',\n",
    "                                  name=\"log\")\n",
    "\n",
    "trainer = L.Trainer(log_every_n_steps=100,\n",
    "                    logger=csv_logger,\n",
    "                    max_epochs=500,\n",
    "                    callbacks=[checkpoint_callback, early_stopping],\n",
    "                    accelerator='gpu',\n",
    "                    devices=1,\n",
    "                    precision=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at ../../../user_data/checkpoints/rns_swav_p20/model_epoch-epoch=00-train_loss=1.43917.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type               | Params\n",
      "-------------------------------------------------------\n",
      "0 | backbone        | Sequential         | 23.5 M\n",
      "1 | projection_head | SwaVProjectionHead | 4.5 M \n",
      "2 | prototypes      | SwaVPrototypes     | 2.6 K \n",
      "3 | queues          | ModuleList         | 0     \n",
      "4 | criterion       | SwaVLoss           | 0     \n",
      "5 | ft_enc          | Sequential         | 22.7 K\n",
      "-------------------------------------------------------\n",
      "28.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.0 M    Total params\n",
      "111.976   Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at ../../../user_data/checkpoints/rns_swav_p20/model_epoch-epoch=00-train_loss=1.43917.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: |          | 0/? [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b13fc367c10437fa07f9846af750062"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model=model, train_dataloaders=dataloader,ckpt_path=ckpt_folder_root + 'rns_swav_p20/model_epoch-epoch=00-train_loss=1.43917.ckpt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T15:14:03.944677Z",
     "end_time": "2024-02-22T13:48:00.678650Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 16, 1124]              64\n",
      "         Transpose-2             [-1, 1124, 16]               0\n",
      "         LayerNorm-3             [-1, 1124, 16]              32\n",
      "              GELU-4             [-1, 1124, 16]               0\n",
      "         Transpose-5             [-1, 16, 1124]               0\n",
      "            Conv1d-6              [-1, 64, 561]             832\n",
      "         Transpose-7              [-1, 561, 64]               0\n",
      "         LayerNorm-8              [-1, 561, 64]             128\n",
      "              GELU-9              [-1, 561, 64]               0\n",
      "        Transpose-10              [-1, 64, 561]               0\n",
      "           Conv1d-11             [-1, 128, 280]           4,224\n",
      "        Transpose-12             [-1, 280, 128]               0\n",
      "        LayerNorm-13             [-1, 280, 128]             256\n",
      "             GELU-14             [-1, 280, 128]               0\n",
      "        Transpose-15             [-1, 128, 280]               0\n",
      "           Conv1d-16             [-1, 256, 279]          16,640\n",
      "        Transpose-17             [-1, 279, 256]               0\n",
      "        LayerNorm-18             [-1, 279, 256]             512\n",
      "             GELU-19             [-1, 279, 256]               0\n",
      "        Transpose-20             [-1, 256, 279]               0\n",
      "AdaptiveAvgPool1d-21             [-1, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 22,688\n",
      "Trainable params: 22,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 6.65\n",
      "Params size (MB): 0.09\n",
      "Estimated Total Size (MB): 6.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model.ft_enc.cuda(), (4, 2249))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T15:09:30.896674Z",
     "end_time": "2024-02-21T15:09:31.134746Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-21T15:09:31.134746Z",
     "end_time": "2024-02-21T15:09:31.415774Z"
    }
   },
   "outputs": [],
   "source": [
    "test_a = torch.arange(65536).reshape(256, 256).repeat(10, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "crop_transforms = []\n",
    "crop_sizes = [224, 96]\n",
    "crop_min_scales = [0.14, 0.05]\n",
    "crop_max_scales = [1.0, 0.14]\n",
    "crop_counts = [2, 6]\n",
    "for i in range(len(crop_sizes)):\n",
    "    random_resized_crop = T.RandomResizedCrop(crop_sizes[i], scale=(crop_min_scales[i], crop_max_scales[i]))\n",
    "\n",
    "    crop_transforms.extend([T.Compose([random_resized_crop])] * crop_counts[i])\n",
    "\n",
    "views = []\n",
    "for tf in crop_transforms:\n",
    "    views.append(tf(test_a))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T15:09:31.415774Z",
     "end_time": "2024-02-21T15:09:31.974082Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from models.rns_dataloader import RNSDataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T15:09:31.974082Z",
     "end_time": "2024-02-21T15:09:32.359872Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T15:09:32.355871Z",
     "end_time": "2024-02-21T15:09:32.387870Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 1000])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1000).repeat(4,1).size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T15:09:32.371871Z",
     "end_time": "2024-02-21T15:09:32.733752Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_length):\n",
    "        self.out_length = out_length\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        slice_length = np.random.choice(np.arange(5,9)*250)\n",
    "        start_slice = np.random.randint(low=0, high=self.out_length - slice_length)\n",
    "        padded_tensor = torch.zeros_like(sample)\n",
    "        sample = T.functional.crop(sample, 0, start_slice, 4, slice_length)\n",
    "\n",
    "\n",
    "        if np.random.choice([True, False]):\n",
    "            padded_tensor[:, padded_tensor.size()[-1]-sample.size()[-1]:] = sample\n",
    "        else:\n",
    "            padded_tensor[:, :sample.size()[-1]] = sample\n",
    "\n",
    "\n",
    "        return padded_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T15:09:32.733752Z",
     "end_time": "2024-02-21T15:09:33.071826Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[   0,    0,    0,  ..., 1857, 1858, 1859],\n        [   0,    0,    0,  ..., 1857, 1858, 1859],\n        [   0,    0,    0,  ..., 1857, 1858, 1859],\n        [   0,    0,    0,  ..., 1857, 1858, 1859]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = torch.arange(2249).repeat(4,1)\n",
    "RandomCrop(2249)(sample)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T15:09:33.071826Z",
     "end_time": "2024-02-21T15:09:33.418803Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-21T15:09:33.414802Z",
     "end_time": "2024-02-21T15:09:33.446801Z"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Train_SwAV_10_epochs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
