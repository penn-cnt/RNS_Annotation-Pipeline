{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-03T08:37:26.350103Z",
     "end_time": "2023-10-03T08:37:27.234379Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-03T08:37:27.236381Z",
     "end_time": "2023-10-03T08:37:31.410866Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../tools')\n",
    "\n",
    "import multicrop_dataset\n",
    "import architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0MBB1IVas91",
    "ExecuteTime": {
     "start_time": "2023-10-03T08:37:31.411867Z",
     "end_time": "2023-10-03T08:37:34.729865Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import pytorch_lightning.callbacks as pl_callbacks\n",
    "\n",
    "matplotlib.use(\"nbAgg\")\n",
    "\n",
    "import data_utility\n",
    "\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-03T08:37:34.730865Z",
     "end_time": "2023-10-03T08:37:35.255759Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "data_dir = \"../../../user_data/\"\n",
    "log_folder_root = '../../../user_data/logs/'\n",
    "ckpt_folder_root = '../../../user_data/checkpoints/'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-03T08:37:35.257761Z",
     "end_time": "2023-10-03T08:37:35.781112Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-03T08:37:35.781112Z",
     "end_time": "2023-10-03T08:37:36.309073Z"
    }
   },
   "outputs": [],
   "source": [
    "dir_list = os.listdir(data_dir+'rns_data')\n",
    "patientIDs = [s for s in dir_list for type_string in ['HUP', 'RNS'] if type_string in s.upper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-03T08:37:36.309073Z",
     "end_time": "2023-10-03T08:37:36.834988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['HUP047',\n 'HUP059',\n 'HUP084',\n 'HUP096',\n 'HUP101',\n 'HUP108',\n 'HUP109',\n 'HUP121',\n 'HUP127',\n 'HUP128',\n 'HUP129',\n 'HUP131',\n 'HUP136',\n 'HUP137',\n 'HUP143',\n 'HUP147',\n 'HUP153',\n 'HUP156',\n 'HUP159',\n 'HUP182',\n 'HUP192',\n 'HUP197',\n 'HUP199',\n 'HUP205',\n 'RNS021',\n 'RNS022',\n 'RNS026',\n 'RNS029']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patientIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g05XnHgHas-D",
    "ExecuteTime": {
     "start_time": "2023-10-03T08:37:36.833987Z",
     "end_time": "2023-10-03T08:37:37.362122Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_import = data_utility.read_files(path = data_dir+'rns_data', path_data = data_dir+'rns_raw_cache',patientIDs=patientIDs[10:], annotation_only = False, verbose=True)\n",
    "# ids = list(data_import.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-03T08:37:37.362122Z",
     "end_time": "2023-10-03T08:37:37.887867Z"
    }
   },
   "outputs": [],
   "source": [
    "# window_len = 1\n",
    "# stride = 1\n",
    "# concat_n = 2\n",
    "# for id in tqdm(ids):\n",
    "#     data_import[id].set_window_parameter(window_length=window_len, window_displacement=stride)\n",
    "#     data_import[id].set_concatenation_parameter(concatenate_window_n=concat_n)\n",
    "#     data_import[id].get_windowed_data(data_import[id].catalog[\"Event Start idx\"],data_import[id].catalog[\"Event End idx\"])\n",
    "#     data_import[id].normalize_windowed_data()\n",
    "#     _, concatenated_data = data_import[id].get_concatenated_data(data_import[id].windowed_data, arrange='channel_stack')\n",
    "#     np.save(data_dir+'rns_cache/'+ id +'.npy',concatenated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-03T08:37:37.887867Z",
     "end_time": "2023-10-03T08:37:38.412464Z"
    }
   },
   "outputs": [],
   "source": [
    "file_list = ['HUP159.npy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-03T08:37:38.569999Z",
     "end_time": "2023-10-03T08:37:39.112377Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNS_Raw(Dataset):\n",
    "    def __init__(self, file_names, transform=True, astensor = True):\n",
    "        self.file_names = file_names\n",
    "        self.transform = transform\n",
    "        \n",
    "        file_name_temp = self.file_names[0]\n",
    "        with open(data_dir+'rns_cache/'+ file_name_temp, 'rb') as f:\n",
    "            temp_file = np.load(f)\n",
    "        \n",
    "        \n",
    "        self.data = np.empty((0,temp_file.shape[1],temp_file.shape[2]))    \n",
    "        print(self.data.shape)\n",
    "        \n",
    "        for name in tqdm(self.file_names):\n",
    "            with open(data_dir+'rns_cache/'+ name, 'rb') as f:\n",
    "                cache = np.load(f)\n",
    "            self.data = np.vstack((self.data,cache))\n",
    "\n",
    "        print('data loaded')\n",
    "\n",
    "        self.length = len(self.data)\n",
    "\n",
    "        if astensor:\n",
    "            self.augmentation = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "                T.RandomApply([T.ColorJitter()], p=0.5),\n",
    "                T.RandomApply([T.GaussianBlur(kernel_size=(3, 3))], p=0.5),\n",
    "                T.RandomInvert(p=0.2),\n",
    "                T.RandomPosterize(4, p=0.2),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "\n",
    "            self.totensor = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "                T.ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            self.augmentation = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "                T.RandomApply([T.ColorJitter()], p=0.5),\n",
    "                T.RandomApply([T.GaussianBlur(kernel_size=(3, 3))], p=0.5),\n",
    "                T.RandomInvert(p=0.2),\n",
    "                T.RandomPosterize(4, p=0.2),\n",
    "            ])\n",
    "\n",
    "            self.totensor = T.Compose([\n",
    "                T.ToPILImage(),\n",
    "                T.Resize((256, 256), interpolation=T.InterpolationMode.NEAREST),\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "\n",
    "        if self.transform:\n",
    "            concat_len = data.shape[1]/4\n",
    "            channel_index = np.arange(4)\n",
    "            np.random.shuffle(channel_index)\n",
    "            channel_index = channel_index*concat_len+(concat_len-1)/2\n",
    "            channel_index = np.repeat(channel_index, concat_len)\n",
    "            concate_len_1 = (concat_len-1)/2\n",
    "            a_repeat = np.arange(-concate_len_1,concate_len_1+1)[np.newaxis].T\n",
    "            base_repeat = np.repeat(a_repeat, 4,axis=1).T.flatten()\n",
    "            channel_index = channel_index+base_repeat\n",
    "            data = data[channel_index.astype(int)]\n",
    "            data = torch.from_numpy(data).clone()\n",
    "            data = data.repeat(3, 1, 1)\n",
    "            data = self.augmentation(data)\n",
    "\n",
    "        else:\n",
    "            data = torch.from_numpy(data).clone()\n",
    "            data = data.repeat(3, 1, 1)\n",
    "            data = self.totensor(data)\n",
    "\n",
    "        return data, [], None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-03T08:37:39.112377Z",
     "end_time": "2023-10-03T08:37:40.691855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 249, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "unlabeled_dataset = RNS_Raw(file_list, transform=True,astensor = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-03T08:37:40.694858Z",
     "end_time": "2023-10-03T08:37:41.830047Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from models.SwaV import SwaV\n",
    "from lightly.data import SwaVCollateFunction\n",
    "#\n",
    "model = SwaV()\n",
    "\n",
    "# model.load_from_checkpoint()\n",
    "\n",
    "collate_fn = SwaVCollateFunction(gaussian_blur = 0, hf_prob = 0,vf_prob = 0,rr_prob=0,cj_prob=0,random_gray_scale=0, normalize={'mean':[0, 0, 0], 'std':[1, 1, 1]})\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    unlabeled_dataset,\n",
    "    batch_size=256,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "checkpoint_callback = pl_callbacks.ModelCheckpoint(monitor='swav_loss',filename='rns_swav_159-{epoch:02d}-{swav_loss:.5f}', save_last=True, save_top_k=-1, dirpath=ckpt_folder_root + 'rns_swav_34')\n",
    "csv_logger = pl_loggers.CSVLogger(log_folder_root, name=\"rns_swav_34\")\n",
    "\n",
    "trainer = pl.Trainer(logger=csv_logger, max_epochs=120, callbacks=[checkpoint_callback],accelerator='gpu', devices=1,precision=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:616: UserWarning: Checkpoint directory C:\\Users\\Patrick Xu\\Desktop\\RNS_Annotation-Pipeline\\user_data\\checkpoints\\rns_swav_34 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Restoring states from the checkpoint path at ../../../user_data/checkpoints/rns_swav_34/rns_swav_159-epoch=79-swav_loss=3.52894.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type               | Params\n",
      "-------------------------------------------------------\n",
      "0 | backbone        | Sequential         | 21.3 M\n",
      "1 | projection_head | SwaVProjectionHead | 328 K \n",
      "2 | prototypes      | SwaVPrototypes     | 66.0 K\n",
      "3 | queues          | ModuleList         | 0     \n",
      "4 | criterion       | SwaVLoss           | 0     \n",
      "-------------------------------------------------------\n",
      "21.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.7 M    Total params\n",
      "43.359    Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint file at ../../../user_data/checkpoints/rns_swav_34/rns_swav_159-epoch=79-swav_loss=3.52894.ckpt\n",
      "C:\\Users\\Patrick Xu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 159it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5cc4155742dd47c7aa3de1a7d2532047"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model=model, train_dataloaders=dataloader,ckpt_path=ckpt_folder_root+'rns_swav_34/rns_swav_159-epoch=79-swav_loss=3.52894.ckpt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-03T00:05:27.988865Z",
     "end_time": "2023-10-03T08:36:27.500921Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Train_SwAV_10_epochs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
