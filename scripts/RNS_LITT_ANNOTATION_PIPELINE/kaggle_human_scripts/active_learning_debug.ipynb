{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('tools')\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from dataset import get_dataset, get_handler, get_wa_handler\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import query_strategies\n",
    "import models\n",
    "from utils import print_log\n",
    "import kaggle_data_utility\n",
    "import dataset\n",
    "import base_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.loggers as pl_loggers\n",
    "import pytorch_lightning.callbacks as pl_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lightly.data import LightlyDataset, SwaVCollateFunction\n",
    "from lightly.loss import SwaVLoss\n",
    "from lightly.loss.memory_bank import MemoryBankModule\n",
    "from lightly.models.modules import SwaVProjectionHead, SwaVPrototypes\n",
    "\n",
    "\n",
    "class SwaV(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = SwaVProjectionHead(512, 512, 128)\n",
    "        self.prototypes = SwaVPrototypes(128, 512, 5)\n",
    "\n",
    "        self.start_queue_at_epoch = 30\n",
    "        self.queues = nn.ModuleList([MemoryBankModule(size=512) for _ in range(2)])\n",
    "\n",
    "    def forward(self, high_resolution, low_resolution, epoch):\n",
    "        self.prototypes.normalize()\n",
    "\n",
    "        high_resolution_features = [self._subforward(x) for x in high_resolution]\n",
    "        low_resolution_features = [self._subforward(x) for x in low_resolution]\n",
    "\n",
    "        high_resolution_prototypes = [\n",
    "            self.prototypes(x, epoch) for x in high_resolution_features\n",
    "        ]\n",
    "        low_resolution_prototypes = [\n",
    "            self.prototypes(x, epoch) for x in low_resolution_features\n",
    "        ]\n",
    "        queue_prototypes = self._get_queue_prototypes(high_resolution_features, epoch)\n",
    "\n",
    "        return high_resolution_prototypes, low_resolution_prototypes, queue_prototypes\n",
    "\n",
    "    def _subforward(self, input):\n",
    "        features = self.backbone(input).flatten(start_dim=1)\n",
    "        features = self.projection_head(features)\n",
    "        features = nn.functional.normalize(features, dim=1, p=2)\n",
    "        return features\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_queue_prototypes(self, high_resolution_features, epoch):\n",
    "        if len(high_resolution_features) != len(self.queues):\n",
    "            raise ValueError(\n",
    "                f\"The number of queues ({len(self.queues)}) should be equal to the number of high \"\n",
    "                f\"resolution inputs ({len(high_resolution_features)}). Set `n_queues` accordingly.\"\n",
    "            )\n",
    "\n",
    "        # Get the queue features\n",
    "        queue_features = []\n",
    "        for i in range(len(self.queues)):\n",
    "            _, features = self.queues[i](high_resolution_features[i], update=True)\n",
    "            # Queue features are in (num_ftrs X queue_length) shape, while the high res\n",
    "            # features are in (batch_size X num_ftrs). Swap the axes for interoperability.\n",
    "            features = torch.permute(features, (1, 0))\n",
    "            queue_features.append(features)\n",
    "\n",
    "        # If loss calculation with queue prototypes starts at a later epoch,\n",
    "        # just queue the features and return None instead of queue prototypes.\n",
    "        if self.start_queue_at_epoch > 0 and epoch < self.start_queue_at_epoch:\n",
    "            return None\n",
    "\n",
    "        # Assign prototypes\n",
    "        queue_prototypes = [self.prototypes(x, epoch) for x in queue_features]\n",
    "        return queue_prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "class ActiveDataHandler(Dataset):\n",
    "    def __init__(self, X, Y, transform=None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.transform = transform['transform']\n",
    "        self.mean = transform['mean']\n",
    "        self.sd = transform['std']\n",
    "\n",
    "        self.augmentation = T.Compose([\n",
    "            T.Normalize([self.mean, self.mean, self.mean], [self.sd, self.sd, self.sd]),\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((256, 512), interpolation=T.InterpolationMode.NEAREST),\n",
    "            T.RandomApply([T.ColorJitter()], p=0.5),\n",
    "            T.RandomApply([T.GaussianBlur(kernel_size=(3, 3))], p=0.5),\n",
    "            T.RandomInvert(p=0.2),\n",
    "            T.RandomPosterize(4, p=0.2),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "        self.totensor = T.Compose([\n",
    "            T.Normalize([self.mean, self.mean, self.mean], [self.sd, self.sd, self.sd]),\n",
    "            T.ToPILImage(),\n",
    "            T.Resize((256, 512), interpolation=T.InterpolationMode.NEAREST),\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index]\n",
    "        y = self.Y[index]\n",
    "\n",
    "        if self.transform:\n",
    "            channel_index = np.arange(x.shape[0])\n",
    "            np.random.shuffle(channel_index)\n",
    "            x = x[channel_index]\n",
    "            x = torch.from_numpy(x).clone()\n",
    "            x = x.repeat(3, 1, 1)\n",
    "            x = self.augmentation(x)\n",
    "\n",
    "        else:\n",
    "            x = torch.from_numpy(x).clone()\n",
    "            x = x.repeat(3, 1, 1)\n",
    "            x = self.totensor(x)\n",
    "\n",
    "        return x, torch.from_numpy(y).to(dtype=torch.long), index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "(0s)\n",
      "X (178, 16, 400) y (178,) latencies (178,)\n",
      "Loading data\n",
      "(0s)\n",
      "X (172, 16, 400) y (172,) latencies (172,)\n",
      "Loading data\n",
      "(2s)\n",
      "X (480, 16, 400) y (480,) latencies (480,)\n",
      "Loading data\n",
      "(1s)\n",
      "X (257, 16, 400) y (257,) latencies (257,)\n",
      "Loading data\n",
      "(1s)\n",
      "X (418, 16, 400) y (418,)\n",
      "Loading data\n",
      "(5s)\n",
      "X (1148, 16, 400) y (1148,)\n",
      "Loading data\n",
      "(22s)\n",
      "X (4760, 16, 400) y (4760,)\n",
      "Loading data\n",
      "(12s)\n",
      "X (2790, 16, 400) y (2790,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/competition_data/clips\"\n",
    "\n",
    "targets = [\n",
    "    'Dog_1',\n",
    "    'Dog_2',\n",
    "    'Dog_3',\n",
    "    'Dog_4',\n",
    "]\n",
    "\n",
    "# targets = [\n",
    "#     'Patient_1',\n",
    "#     'Patient_2',\n",
    "#     'Patient_3',\n",
    "#     'Patient_4',\n",
    "#     'Patient_5',\n",
    "#     'Patient_6',\n",
    "#     'Patient_7',\n",
    "#     'Patient_8'\n",
    "# ]\n",
    "\n",
    "ictal_data_list = [kaggle_data_utility.parse_input_data(data_dir, targets[i], 'ictal', None) for i in\n",
    "                   range(len(targets))]\n",
    "interictal_data_list = [kaggle_data_utility.parse_input_data(data_dir, targets[i], 'interictal', None) for i in\n",
    "                        range(len(targets))]\n",
    "ictal_data_X = np.empty((0, 16, 400))\n",
    "interictal_data_X = np.empty((0, 16, 400))\n",
    "for data in ictal_data_list:\n",
    "    ictal_data_X = np.vstack((ictal_data_X, data['X']))\n",
    "for data in interictal_data_list:\n",
    "    interictal_data_X = np.vstack((interictal_data_X, data['X']))\n",
    "ictal_data_y = np.ones(len(ictal_data_X))[:, np.newaxis]\n",
    "interictal_data_y = np.zeros(len(interictal_data_X))[:, np.newaxis]\n",
    "data = np.vstack((ictal_data_X, interictal_data_X))\n",
    "label = np.vstack((ictal_data_y, interictal_data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10203, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nStart = 3\n",
    "nEnd = 30\n",
    "nQuery = 2\n",
    "\n",
    "n_pool = len(y_train)\n",
    "n_test = len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n",
      "102\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "NUM_INIT_LB = int(nStart * n_pool / 100)\n",
    "NUM_QUERY = int(nQuery * n_pool / 100) if nStart != 100 else 0\n",
    "NUM_ROUND = int((int(nEnd * n_pool / 100) - NUM_INIT_LB) / NUM_QUERY) if nStart != 100 else 0\n",
    "if NUM_QUERY != 0:\n",
    "    if (int(nEnd * n_pool / 100) - NUM_INIT_LB) % NUM_QUERY != 0:\n",
    "        NUM_ROUND += 1\n",
    "\n",
    "print(NUM_INIT_LB)\n",
    "print(NUM_QUERY)\n",
    "print(NUM_ROUND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_lb = np.zeros(n_pool, dtype=bool)\n",
    "idxs_tmp = np.arange(n_pool)\n",
    "np.random.shuffle(idxs_tmp)\n",
    "idxs_lb[idxs_tmp[:NUM_INIT_LB]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(\"ckpt/checkpoint99.pth\")\n",
    "resnet = torchvision.models.resnet34()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "swav = SwaV(backbone)\n",
    "swav.load_state_dict(ckpt['model_state_dict'])\n",
    "model = base_model.ActiveLearning(swav.backbone)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "idxs_train = np.arange(n_pool)[idxs_lb]\n",
    "\n",
    "checkpoint_callback = pl_callbacks.ModelCheckpoint(monitor='val_acc', filename='active_ent_round_0-{epoch:02d}-{val_loss:.5f}', dirpath='active_checkpoints_ent')\n",
    "csv_logger = pl_loggers.CSVLogger(\"active_logs_ent\", name=\"logger_round_0\")\n",
    "trainer = pl.Trainer( logger=csv_logger, max_epochs=50, callbacks=[checkpoint_callback],accelerator='gpu', devices=1,log_every_n_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "modelstate = deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ActiveDataHandler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(data), torch\u001b[38;5;241m.\u001b[39mstack(label)\n\u001b[0;32m      8\u001b[0m transforms_param \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform_tr\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9.875602358282888e-18\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m63.05380081813939\u001b[39m},\n\u001b[0;32m      9\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform_te\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9.875602358282888e-18\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m63.05380081813939\u001b[39m},\n\u001b[0;32m     10\u001b[0m                     }\n\u001b[1;32m---> 12\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mActiveDataHandler\u001b[49m(X_train[idxs_train],y_train[idxs_train],transform\u001b[38;5;241m=\u001b[39mtransforms_param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform_tr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     13\u001b[0m test_data \u001b[38;5;241m=\u001b[39m ActiveDataHandler(X_test,y_test,transform\u001b[38;5;241m=\u001b[39mtransforms_param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform_te\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     14\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_data,\n\u001b[0;32m     15\u001b[0m                                         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m     16\u001b[0m                                         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     17\u001b[0m                                         collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn,\n\u001b[0;32m     18\u001b[0m                                         drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ActiveDataHandler' is not defined"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    info = list(zip(*batch))\n",
    "    data = info[0]\n",
    "    label = info[1]\n",
    "\n",
    "\n",
    "    return torch.stack(data), torch.stack(label)\n",
    "transforms_param = {'transform_tr': {'transform': True , 'mean':-9.875602358282888e-18, 'std': 63.05380081813939},\n",
    "                    'transform_te': {'transform': False , 'mean':-9.875602358282888e-18, 'std': 63.05380081813939},\n",
    "                    }\n",
    "\n",
    "train_data = ActiveDataHandler(X_train[idxs_train],y_train[idxs_train],transform=transforms_param['transform_tr'])\n",
    "test_data = ActiveDataHandler(X_test,y_test,transform=transforms_param['transform_te'])\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                        batch_size=128,\n",
    "                                        shuffle=True,\n",
    "                                        collate_fn=collate_fn,\n",
    "                                        drop_last=True, )\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=128,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_query(idxs_lb, n):\n",
    "    inds = np.where(idxs_lb==0)[0]\n",
    "    return inds[np.random.permutation(len(inds))][:n]\n",
    "\n",
    "def entropy_query(X_train, y_train, trainer, model, idxs_lb, n):\n",
    "    idxs_unlabeled = np.arange(n_pool)[~idxs_lb]\n",
    "    untrained_data = ActiveDataHandler(X_train[idxs_unlabeled], y_train[idxs_unlabeled], transform=transforms_param['transform_te'])\n",
    "    untrained_dataloader = torch.utils.data.DataLoader(untrained_data,\n",
    "                                                   batch_size=128,\n",
    "                                                   shuffle=True,\n",
    "                                                   collate_fn=collate_fn,\n",
    "                                                   drop_last=True, )\n",
    "    predictions = trainer.predict(model,untrained_dataloader)\n",
    "\n",
    "    probs = []\n",
    "    m = nn.Softmax(dim=1)\n",
    "    for pred, y in predictions:\n",
    "        out = m(pred)\n",
    "        probs.append(out)\n",
    "    probs = torch.vstack(probs)\n",
    "    # print(probs)\n",
    "    log_probs = torch.log(probs)\n",
    "    # print(log_probs)\n",
    "    U = (probs*log_probs).sum(1)\n",
    "    # print(U.sort())\n",
    "    # print(U.sort()[1][:n])\n",
    "    # print(y_train[idxs_unlabeled[U.sort()[1][:n]][::-1]])\n",
    "    return idxs_unlabeled[U.sort()[1][:n]]\n",
    "\n",
    "def lease_conf_query(X_train, y_train, trainer, model, idxs_lb, n):\n",
    "    idxs_unlabeled = np.arange(n_pool)[~idxs_lb]\n",
    "    untrained_data = ActiveDataHandler(X_train[idxs_unlabeled], y_train[idxs_unlabeled], transform=transforms_param['transform_te'])\n",
    "    untrained_dataloader = torch.utils.data.DataLoader(untrained_data,\n",
    "                                                   batch_size=128,\n",
    "                                                   shuffle=True,\n",
    "                                                   collate_fn=collate_fn,\n",
    "                                                   drop_last=True, )\n",
    "    predictions = trainer.predict(model,untrained_dataloader)\n",
    "    output_list = []\n",
    "    m = nn.Softmax(dim=1)\n",
    "    for pred, y in predictions:\n",
    "        out = m(pred)\n",
    "        output_list.append(out)\n",
    "    probs = torch.vstack(output_list)\n",
    "    # print(probs)\n",
    "    U = probs.max(1)[0]\n",
    "    # print(U)\n",
    "    # print(y_train[idxs_unlabeled[U.sort()[1][:n]][::-1]])\n",
    "    return idxs_unlabeled[U.sort()[1][:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUM_ROUND' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8c8243887b19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mrd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_ROUND\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Round {}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_ROUND\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mlabeled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_pool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midxs_lb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mNUM_QUERY\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnEnd\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_pool\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlabeled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mNUM_QUERY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnEnd\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_pool\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlabeled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NUM_ROUND' is not defined"
     ]
    }
   ],
   "source": [
    "for rd in range(1, NUM_ROUND + 1):\n",
    "    print('Round {}/{}'.format(rd, NUM_ROUND), flush=True)\n",
    "    labeled = len(np.arange(n_pool)[idxs_lb])\n",
    "    if NUM_QUERY > int(nEnd * n_pool / 100) - labeled:\n",
    "        NUM_QUERY = int(nEnd * n_pool / 100) - labeled\n",
    "\n",
    "    output = entropy_query(X_train, y_train, trainer, model, idxs_lb, NUM_QUERY)\n",
    "\n",
    "    idxs_lb_previous = deepcopy(idxs_lb)\n",
    "    # output = random_query(idxs_lb, NUM_QUERY)\n",
    "    q_idxs = output\n",
    "    idxs_lb_previous[q_idxs] = True\n",
    "    idxs_lb = idxs_lb_previous\n",
    "    print(len(np.arange(n_pool)[idxs_lb]))\n",
    "\n",
    "    idxs_train = np.arange(n_pool)[idxs_lb]\n",
    "    train_data = ActiveDataHandler(X_train[idxs_train], y_train[idxs_train], transform=transforms_param['transform_tr'])\n",
    "    test_data = ActiveDataHandler(X_test, y_test, transform=transforms_param['transform_te'])\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                                   batch_size=128,\n",
    "                                                   shuffle=True,\n",
    "                                                   collate_fn=collate_fn,\n",
    "                                                   drop_last=True, )\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=128,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    model.load_state_dict(modelstate)\n",
    "    checkpoint_callback = pl_callbacks.ModelCheckpoint(monitor='val_acc', filename='active_ent_round_' + str(\n",
    "        rd) + '-{epoch:02d}-{val_loss:.5f}', dirpath='active_checkpoints_ent')\n",
    "    csv_logger = pl_loggers.CSVLogger(\"active_logs_ent\", name=\"logger_round_\" + str(rd))\n",
    "    trainer = pl.Trainer(logger=csv_logger, max_epochs=50, callbacks=[checkpoint_callback], accelerator='gpu', devices=1,log_every_n_steps=10)\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
